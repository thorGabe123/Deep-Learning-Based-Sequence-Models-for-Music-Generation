{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "from processing import *\n",
    "from models import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [F:\\GitHub\\dataset\\midi_dataset\\Beethoven - Symphony no. 5.mid] [F:\\GitHub\\dataset\\midi_dataset\\Beethoven - Symphony no. 6 - 1st movement.mid] [F:\\GitHub\\dataset\\midi_dataset\\Beethoven - Symphony no. 7 - 2nd movement.mid] [F:\\GitHub\\dataset\\midi_dataset\\Beethoven - Symphony no. 9 - 2nd movement.mid] [F:\\GitHub\\dataset\\midi_dataset\\Beethoven - Symphony no. 9 - 4th movement.mid] [F:\\GitHub\\dataset\\midi_dataset\\Dukas - Sorcerer's Apprentice.mid] [F:\\GitHub\\dataset\\midi_dataset\\Hans_Zimmer_-_Pirates_Of_The_Caribbean_-_He's_A_Pirate.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Eine Kleine Nachtmusik.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Lacrimoza.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Marige of Figaro.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Queen of the Night.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Symphony no. 40.mid] [F:\\GitHub\\dataset\\midi_dataset\\Mozart - Symphony no. 41 - 3rd movement.mid] [F:\\GitHub\\dataset\\midi_dataset\\Paul Dukas - Sorcerer's Apprentice.mid]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Draco\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocess_midi_files('F:\\\\GitHub\\\\dataset\\\\midi_dataset', 'F:\\\\GitHub\\\\dataset\\\\np_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60000], Average Loss: 7.1537\n",
      "Epoch [1/60000], Validation Loss: 6.4528\n",
      "Epoch [2/60000], Average Loss: 6.4471\n",
      "Epoch [2/60000], Validation Loss: 6.2560\n",
      "Epoch [3/60000], Average Loss: 6.2313\n",
      "Epoch [3/60000], Validation Loss: 6.1556\n",
      "Epoch [4/60000], Average Loss: 6.2250\n",
      "Epoch [4/60000], Validation Loss: 5.7878\n",
      "Epoch [5/60000], Average Loss: 5.8599\n",
      "Epoch [5/60000], Validation Loss: 5.8376\n",
      "Epoch [6/60000], Average Loss: 5.7123\n",
      "Epoch [6/60000], Validation Loss: 5.4175\n",
      "Epoch [7/60000], Average Loss: 5.5670\n",
      "Epoch [7/60000], Validation Loss: 5.3017\n",
      "Epoch [8/60000], Average Loss: 5.4035\n",
      "Epoch [8/60000], Validation Loss: 5.3871\n",
      "Epoch [9/60000], Average Loss: 5.4429\n",
      "Epoch [9/60000], Validation Loss: 5.1438\n",
      "Epoch [10/60000], Average Loss: 5.2411\n",
      "Epoch [10/60000], Validation Loss: 4.9493\n",
      "Epoch [11/60000], Average Loss: 5.0884\n",
      "Epoch [11/60000], Validation Loss: 4.8948\n",
      "Epoch [12/60000], Average Loss: 5.1974\n",
      "Epoch [12/60000], Validation Loss: 5.3222\n",
      "Epoch [13/60000], Average Loss: 5.0503\n",
      "Epoch [13/60000], Validation Loss: 4.6180\n",
      "Epoch [14/60000], Average Loss: 5.1836\n",
      "Epoch [14/60000], Validation Loss: 4.9730\n",
      "Epoch [15/60000], Average Loss: 5.0257\n",
      "Epoch [15/60000], Validation Loss: 4.9625\n",
      "Epoch [16/60000], Average Loss: 5.0381\n",
      "Epoch [16/60000], Validation Loss: 4.5559\n",
      "Epoch [17/60000], Average Loss: 5.1710\n",
      "Epoch [17/60000], Validation Loss: 4.9887\n",
      "Epoch [18/60000], Average Loss: 4.8417\n",
      "Epoch [18/60000], Validation Loss: 4.7663\n",
      "Epoch [19/60000], Average Loss: 4.9689\n",
      "Epoch [19/60000], Validation Loss: 4.9333\n",
      "Epoch [20/60000], Average Loss: 4.9135\n",
      "Epoch [20/60000], Validation Loss: 4.3832\n",
      "Epoch [21/60000], Average Loss: 4.8690\n",
      "Epoch [21/60000], Validation Loss: 4.7337\n",
      "Epoch [22/60000], Average Loss: 5.0702\n",
      "Epoch [22/60000], Validation Loss: 4.6140\n",
      "Epoch [23/60000], Average Loss: 4.6621\n",
      "Epoch [23/60000], Validation Loss: 4.6288\n",
      "Epoch [24/60000], Average Loss: 4.8462\n",
      "Epoch [24/60000], Validation Loss: 4.6696\n",
      "Epoch [25/60000], Average Loss: 4.7385\n",
      "Epoch [25/60000], Validation Loss: 4.9970\n",
      "Epoch [26/60000], Average Loss: 4.7901\n",
      "Epoch [26/60000], Validation Loss: 5.3244\n",
      "Epoch [27/60000], Average Loss: 4.6604\n",
      "Epoch [27/60000], Validation Loss: 5.0871\n",
      "Epoch [28/60000], Average Loss: 4.7832\n",
      "Epoch [28/60000], Validation Loss: 5.1767\n",
      "Epoch [29/60000], Average Loss: 4.5831\n",
      "Epoch [29/60000], Validation Loss: 5.1995\n",
      "Epoch [30/60000], Average Loss: 4.5399\n",
      "Epoch [30/60000], Validation Loss: 4.6819\n",
      "Epoch [31/60000], Average Loss: 4.5468\n",
      "Epoch [31/60000], Validation Loss: 4.4606\n",
      "Epoch [32/60000], Average Loss: 4.2932\n",
      "Epoch [32/60000], Validation Loss: 5.2662\n",
      "Epoch [33/60000], Average Loss: 4.2825\n",
      "Epoch [33/60000], Validation Loss: 4.5762\n",
      "Epoch [34/60000], Average Loss: 4.6550\n",
      "Epoch [34/60000], Validation Loss: 4.7938\n",
      "Epoch [35/60000], Average Loss: 5.0750\n",
      "Epoch [35/60000], Validation Loss: 4.6056\n",
      "Epoch [36/60000], Average Loss: 4.4829\n",
      "Epoch [36/60000], Validation Loss: 4.1933\n",
      "Epoch [37/60000], Average Loss: 4.4499\n",
      "Epoch [37/60000], Validation Loss: 4.6760\n",
      "Epoch [38/60000], Average Loss: 4.3021\n",
      "Epoch [38/60000], Validation Loss: 4.5895\n",
      "Epoch [39/60000], Average Loss: 4.6007\n",
      "Epoch [39/60000], Validation Loss: 4.0760\n",
      "Epoch [40/60000], Average Loss: 4.5203\n",
      "Epoch [40/60000], Validation Loss: 5.1304\n",
      "Epoch [41/60000], Average Loss: 4.7758\n",
      "Epoch [41/60000], Validation Loss: 4.9723\n",
      "Epoch [42/60000], Average Loss: 4.7265\n",
      "Epoch [42/60000], Validation Loss: 4.5225\n",
      "Epoch [43/60000], Average Loss: 4.4558\n",
      "Epoch [43/60000], Validation Loss: 4.6903\n",
      "Epoch [44/60000], Average Loss: 4.6396\n",
      "Epoch [44/60000], Validation Loss: 4.4791\n",
      "Epoch [45/60000], Average Loss: 4.5281\n",
      "Epoch [45/60000], Validation Loss: 5.5096\n",
      "Epoch [46/60000], Average Loss: 4.3356\n",
      "Epoch [46/60000], Validation Loss: 4.3863\n",
      "Epoch [47/60000], Average Loss: 4.7991\n",
      "Epoch [47/60000], Validation Loss: 5.2288\n",
      "Epoch [48/60000], Average Loss: 4.6286\n",
      "Epoch [48/60000], Validation Loss: 4.3391\n",
      "Epoch [49/60000], Average Loss: 4.4897\n",
      "Epoch [49/60000], Validation Loss: 5.3974\n",
      "Epoch [50/60000], Average Loss: 4.4759\n",
      "Epoch [50/60000], Validation Loss: 5.1876\n",
      "Epoch [51/60000], Average Loss: 4.7977\n",
      "Epoch [51/60000], Validation Loss: 5.4639\n",
      "Epoch [52/60000], Average Loss: 4.4224\n",
      "Epoch [52/60000], Validation Loss: 4.9338\n",
      "Epoch [53/60000], Average Loss: 4.4432\n",
      "Epoch [53/60000], Validation Loss: 5.2608\n",
      "Epoch [54/60000], Average Loss: 4.4950\n",
      "Epoch [54/60000], Validation Loss: 4.8404\n",
      "Epoch [55/60000], Average Loss: 4.4796\n",
      "Epoch [55/60000], Validation Loss: 4.2200\n",
      "Epoch [56/60000], Average Loss: 4.6548\n",
      "Epoch [56/60000], Validation Loss: 4.4578\n",
      "Epoch [57/60000], Average Loss: 4.4025\n",
      "Epoch [57/60000], Validation Loss: 5.3902\n",
      "Epoch [58/60000], Average Loss: 4.3867\n",
      "Epoch [58/60000], Validation Loss: 5.3824\n",
      "Epoch [59/60000], Average Loss: 4.8016\n",
      "Epoch [59/60000], Validation Loss: 5.0641\n",
      "Epoch [60/60000], Average Loss: 4.3233\n",
      "Epoch [60/60000], Validation Loss: 4.5731\n",
      "Epoch [61/60000], Average Loss: 4.6051\n",
      "Epoch [61/60000], Validation Loss: 6.2684\n",
      "Epoch [62/60000], Average Loss: 4.5979\n",
      "Epoch [62/60000], Validation Loss: 4.5360\n",
      "Epoch [63/60000], Average Loss: 4.5186\n",
      "Epoch [63/60000], Validation Loss: 4.0529\n",
      "Epoch [64/60000], Average Loss: 4.4297\n",
      "Epoch [64/60000], Validation Loss: 4.2404\n",
      "Epoch [65/60000], Average Loss: 4.5756\n",
      "Epoch [65/60000], Validation Loss: 5.1263\n",
      "Epoch [66/60000], Average Loss: 4.5551\n",
      "Epoch [66/60000], Validation Loss: 3.8652\n",
      "Epoch [67/60000], Average Loss: 4.4318\n",
      "Epoch [67/60000], Validation Loss: 4.1999\n",
      "Epoch [68/60000], Average Loss: 4.2124\n",
      "Epoch [68/60000], Validation Loss: 5.7175\n",
      "Epoch [69/60000], Average Loss: 4.4121\n",
      "Epoch [69/60000], Validation Loss: 4.4622\n",
      "Epoch [70/60000], Average Loss: 4.4834\n",
      "Epoch [70/60000], Validation Loss: 4.4178\n",
      "Epoch [71/60000], Average Loss: 4.6949\n",
      "Epoch [71/60000], Validation Loss: 5.0773\n",
      "Epoch [72/60000], Average Loss: 4.7226\n",
      "Epoch [72/60000], Validation Loss: 4.6688\n",
      "Epoch [73/60000], Average Loss: 4.8118\n",
      "Epoch [73/60000], Validation Loss: 4.8066\n",
      "Epoch [74/60000], Average Loss: 4.5171\n",
      "Epoch [74/60000], Validation Loss: 4.6118\n",
      "Epoch [75/60000], Average Loss: 4.3028\n",
      "Epoch [75/60000], Validation Loss: 5.2244\n",
      "Epoch [76/60000], Average Loss: 4.4486\n",
      "Epoch [76/60000], Validation Loss: 4.6475\n",
      "Epoch [77/60000], Average Loss: 4.6751\n",
      "Epoch [77/60000], Validation Loss: 4.0931\n",
      "Epoch [78/60000], Average Loss: 4.3160\n",
      "Epoch [78/60000], Validation Loss: 5.4476\n",
      "Epoch [79/60000], Average Loss: 4.2253\n",
      "Epoch [79/60000], Validation Loss: 4.8344\n",
      "Epoch [80/60000], Average Loss: 4.7009\n",
      "Epoch [80/60000], Validation Loss: 4.1514\n",
      "Epoch [81/60000], Average Loss: 4.5648\n",
      "Epoch [81/60000], Validation Loss: 5.2515\n",
      "Epoch [82/60000], Average Loss: 4.8952\n",
      "Epoch [82/60000], Validation Loss: 4.2055\n",
      "Epoch [83/60000], Average Loss: 4.3833\n",
      "Epoch [83/60000], Validation Loss: 4.7156\n",
      "Epoch [84/60000], Average Loss: 4.4175\n",
      "Epoch [84/60000], Validation Loss: 4.7035\n",
      "Epoch [85/60000], Average Loss: 4.5681\n",
      "Epoch [85/60000], Validation Loss: 5.0154\n",
      "Epoch [86/60000], Average Loss: 4.4467\n",
      "Epoch [86/60000], Validation Loss: 4.8664\n",
      "Epoch [87/60000], Average Loss: 4.4892\n",
      "Epoch [87/60000], Validation Loss: 5.5602\n",
      "Epoch [88/60000], Average Loss: 4.7592\n",
      "Epoch [88/60000], Validation Loss: 4.2497\n",
      "Epoch [89/60000], Average Loss: 4.6465\n",
      "Epoch [89/60000], Validation Loss: 5.3764\n",
      "Epoch [90/60000], Average Loss: 4.1999\n",
      "Epoch [90/60000], Validation Loss: 5.2826\n",
      "Epoch [91/60000], Average Loss: 4.6759\n",
      "Epoch [91/60000], Validation Loss: 5.3378\n",
      "Epoch [92/60000], Average Loss: 4.6615\n",
      "Epoch [92/60000], Validation Loss: 4.2101\n",
      "Epoch [93/60000], Average Loss: 4.4823\n",
      "Epoch [93/60000], Validation Loss: 4.5145\n",
      "Epoch [94/60000], Average Loss: 4.5942\n",
      "Epoch [94/60000], Validation Loss: 5.3337\n",
      "Epoch [95/60000], Average Loss: 4.3386\n",
      "Epoch [95/60000], Validation Loss: 4.9376\n",
      "Epoch [96/60000], Average Loss: 4.3639\n",
      "Epoch [96/60000], Validation Loss: 4.4744\n",
      "Epoch [97/60000], Average Loss: 4.6759\n",
      "Epoch [97/60000], Validation Loss: 4.8914\n",
      "Epoch [98/60000], Average Loss: 4.4729\n",
      "Epoch [98/60000], Validation Loss: 5.0174\n",
      "Epoch [99/60000], Average Loss: 4.5946\n",
      "Epoch [99/60000], Validation Loss: 5.0889\n",
      "Epoch [100/60000], Average Loss: 4.6182\n",
      "Epoch [100/60000], Validation Loss: 4.7562\n",
      "Epoch [101/60000], Average Loss: 4.5597\n",
      "Epoch [101/60000], Validation Loss: 4.9911\n",
      "Epoch [102/60000], Average Loss: 4.2347\n",
      "Epoch [102/60000], Validation Loss: 4.7503\n",
      "Epoch [103/60000], Average Loss: 4.3579\n",
      "Epoch [103/60000], Validation Loss: 4.2872\n",
      "Epoch [104/60000], Average Loss: 4.5375\n",
      "Epoch [104/60000], Validation Loss: 5.2000\n",
      "Epoch [105/60000], Average Loss: 4.2908\n",
      "Epoch [105/60000], Validation Loss: 5.0200\n",
      "Epoch [106/60000], Average Loss: 4.3734\n",
      "Epoch [106/60000], Validation Loss: 5.5643\n",
      "Epoch [107/60000], Average Loss: 4.4715\n",
      "Epoch [107/60000], Validation Loss: 4.5852\n",
      "Epoch [108/60000], Average Loss: 4.4911\n",
      "Epoch [108/60000], Validation Loss: 5.5877\n",
      "Epoch [109/60000], Average Loss: 4.4876\n",
      "Epoch [109/60000], Validation Loss: 4.5411\n",
      "Epoch [110/60000], Average Loss: 4.5514\n",
      "Epoch [110/60000], Validation Loss: 4.7947\n",
      "Epoch [111/60000], Average Loss: 4.5978\n",
      "Epoch [111/60000], Validation Loss: 5.8772\n",
      "Epoch [112/60000], Average Loss: 4.3219\n",
      "Epoch [112/60000], Validation Loss: 4.5429\n",
      "Epoch [113/60000], Average Loss: 4.5330\n",
      "Epoch [113/60000], Validation Loss: 4.4052\n",
      "Epoch [114/60000], Average Loss: 4.4245\n",
      "Epoch [114/60000], Validation Loss: 4.4916\n",
      "Epoch [115/60000], Average Loss: 4.6652\n",
      "Epoch [115/60000], Validation Loss: 5.1360\n",
      "Epoch [116/60000], Average Loss: 4.6330\n",
      "Epoch [116/60000], Validation Loss: 4.0632\n",
      "Epoch [117/60000], Average Loss: 4.1828\n",
      "Epoch [117/60000], Validation Loss: 4.8832\n",
      "Epoch [118/60000], Average Loss: 4.4351\n",
      "Epoch [118/60000], Validation Loss: 4.4379\n",
      "Epoch [119/60000], Average Loss: 4.3073\n",
      "Epoch [119/60000], Validation Loss: 5.2559\n",
      "Epoch [120/60000], Average Loss: 4.3335\n",
      "Epoch [120/60000], Validation Loss: 4.7026\n",
      "Epoch [121/60000], Average Loss: 4.4799\n",
      "Epoch [121/60000], Validation Loss: 5.2713\n",
      "Epoch [122/60000], Average Loss: 4.5149\n",
      "Epoch [122/60000], Validation Loss: 4.5448\n",
      "Epoch [123/60000], Average Loss: 4.5211\n",
      "Epoch [123/60000], Validation Loss: 3.9783\n",
      "Epoch [124/60000], Average Loss: 4.3157\n",
      "Epoch [124/60000], Validation Loss: 4.1868\n",
      "Epoch [125/60000], Average Loss: 4.6691\n",
      "Epoch [125/60000], Validation Loss: 4.6885\n",
      "Epoch [126/60000], Average Loss: 4.4802\n",
      "Epoch [126/60000], Validation Loss: 4.6377\n",
      "Epoch [127/60000], Average Loss: 4.4392\n",
      "Epoch [127/60000], Validation Loss: 4.8186\n",
      "Epoch [128/60000], Average Loss: 4.5326\n",
      "Epoch [128/60000], Validation Loss: 4.2320\n",
      "Epoch [129/60000], Average Loss: 4.6635\n",
      "Epoch [129/60000], Validation Loss: 4.6243\n",
      "Epoch [130/60000], Average Loss: 4.6356\n",
      "Epoch [130/60000], Validation Loss: 5.1339\n",
      "Epoch [131/60000], Average Loss: 4.4509\n",
      "Epoch [131/60000], Validation Loss: 4.2757\n",
      "Epoch [132/60000], Average Loss: 4.6729\n",
      "Epoch [132/60000], Validation Loss: 5.5949\n",
      "Epoch [133/60000], Average Loss: 4.6512\n",
      "Epoch [133/60000], Validation Loss: 4.0434\n",
      "Epoch [134/60000], Average Loss: 4.5327\n",
      "Epoch [134/60000], Validation Loss: 4.9784\n",
      "Epoch [135/60000], Average Loss: 4.4835\n",
      "Epoch [135/60000], Validation Loss: 4.0453\n",
      "Epoch [136/60000], Average Loss: 4.6674\n",
      "Epoch [136/60000], Validation Loss: 5.2846\n",
      "Epoch [137/60000], Average Loss: 4.4020\n",
      "Epoch [137/60000], Validation Loss: 4.8040\n",
      "Epoch [138/60000], Average Loss: 4.3962\n",
      "Epoch [138/60000], Validation Loss: 4.9844\n",
      "Epoch [139/60000], Average Loss: 4.4361\n",
      "Epoch [139/60000], Validation Loss: 4.3699\n",
      "Epoch [140/60000], Average Loss: 4.6228\n",
      "Epoch [140/60000], Validation Loss: 4.6328\n",
      "Epoch [141/60000], Average Loss: 4.4417\n",
      "Epoch [141/60000], Validation Loss: 4.8004\n",
      "Epoch [142/60000], Average Loss: 4.6077\n",
      "Epoch [142/60000], Validation Loss: 4.8620\n",
      "Epoch [143/60000], Average Loss: 4.6857\n",
      "Epoch [143/60000], Validation Loss: 4.6840\n",
      "Epoch [144/60000], Average Loss: 4.4688\n",
      "Epoch [144/60000], Validation Loss: 4.4831\n",
      "Epoch [145/60000], Average Loss: 4.3370\n",
      "Epoch [145/60000], Validation Loss: 5.3046\n",
      "Epoch [146/60000], Average Loss: 4.4101\n",
      "Epoch [146/60000], Validation Loss: 5.2975\n",
      "Epoch [147/60000], Average Loss: 4.4912\n",
      "Epoch [147/60000], Validation Loss: 4.4981\n",
      "Epoch [148/60000], Average Loss: 4.5373\n",
      "Epoch [148/60000], Validation Loss: 4.1939\n",
      "Epoch [149/60000], Average Loss: 4.4021\n",
      "Epoch [149/60000], Validation Loss: 4.1883\n",
      "Epoch [150/60000], Average Loss: 4.3276\n",
      "Epoch [150/60000], Validation Loss: 4.0783\n",
      "Epoch [151/60000], Average Loss: 4.3964\n",
      "Epoch [151/60000], Validation Loss: 4.2863\n",
      "Epoch [152/60000], Average Loss: 4.2207\n",
      "Epoch [152/60000], Validation Loss: 4.8828\n",
      "Epoch [153/60000], Average Loss: 4.7929\n",
      "Epoch [153/60000], Validation Loss: 4.2572\n",
      "Epoch [154/60000], Average Loss: 4.4341\n",
      "Epoch [154/60000], Validation Loss: 5.1247\n",
      "Epoch [155/60000], Average Loss: 4.4881\n",
      "Epoch [155/60000], Validation Loss: 4.4583\n",
      "Epoch [156/60000], Average Loss: 4.6253\n",
      "Epoch [156/60000], Validation Loss: 4.5256\n",
      "Epoch [157/60000], Average Loss: 4.4916\n",
      "Epoch [157/60000], Validation Loss: 4.6248\n",
      "Epoch [158/60000], Average Loss: 4.2473\n",
      "Epoch [158/60000], Validation Loss: 4.7908\n",
      "Epoch [159/60000], Average Loss: 4.2076\n",
      "Epoch [159/60000], Validation Loss: 4.9761\n",
      "Epoch [160/60000], Average Loss: 4.1799\n",
      "Epoch [160/60000], Validation Loss: 5.1682\n",
      "Epoch [161/60000], Average Loss: 4.7169\n",
      "Epoch [161/60000], Validation Loss: 4.4598\n",
      "Epoch [162/60000], Average Loss: 4.6013\n",
      "Epoch [162/60000], Validation Loss: 4.3817\n",
      "Epoch [163/60000], Average Loss: 4.6074\n",
      "Epoch [163/60000], Validation Loss: 5.8605\n",
      "Epoch [164/60000], Average Loss: 4.5291\n",
      "Epoch [164/60000], Validation Loss: 4.7077\n",
      "Epoch [165/60000], Average Loss: 4.3210\n",
      "Epoch [165/60000], Validation Loss: 4.3591\n",
      "Epoch [166/60000], Average Loss: 4.3530\n",
      "Epoch [166/60000], Validation Loss: 4.7116\n",
      "Epoch [167/60000], Average Loss: 4.4646\n",
      "Epoch [167/60000], Validation Loss: 4.0401\n",
      "Epoch [168/60000], Average Loss: 4.5873\n",
      "Epoch [168/60000], Validation Loss: 4.9747\n",
      "Epoch [169/60000], Average Loss: 4.5800\n",
      "Epoch [169/60000], Validation Loss: 4.5370\n",
      "Epoch [170/60000], Average Loss: 4.4489\n",
      "Epoch [170/60000], Validation Loss: 4.8800\n",
      "Epoch [171/60000], Average Loss: 4.6287\n",
      "Epoch [171/60000], Validation Loss: 4.2920\n",
      "Epoch [172/60000], Average Loss: 4.5330\n",
      "Epoch [172/60000], Validation Loss: 4.6268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     12\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to the correct device\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\dataset.py:40\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Load the sequence from the .npy file\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths[idx]\n\u001b[1;32m---> 40\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     seq_len_extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_npyio_impl.py:484\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    482\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\format.py:836\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 836\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    849\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "model = SimpleTransformer(VOCAB_SIZE, N_EMBD, N_HEAD, N_LAYER, FEEDFORWARD_DIM, BLOCK_SIZE, DROPOUT).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader, test_dataloader = get_train_test_dataloaders('F:\\\\GitHub\\\\dataset\\\\np_dataset')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg) in enumerate(train_dataloader):\n",
    "        # Move data to the correct device\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        # Reshape output and target for loss calculation\n",
    "        output = output.view(-1, VOCAB_SIZE)  # Flatten the output to [batch_size * seq_len, vocab_size]\n",
    "        trg = trg.view(-1)  # Flatten the target to [batch_size * seq_len]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Validation loop (optional)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in test_dataloader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            output = model(src)\n",
    "            output = output.view(-1, VOCAB_SIZE)\n",
    "            trg = trg.view(-1)\n",
    "            val_loss += criterion(output, trg).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(VOCAB_SIZE, N_EMBD, N_HEAD, N_LAYER, FEEDFORWARD_DIM, BLOCK_SIZE, DROPOUT).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader, test_dataloader = get_train_test_dataloaders('F:\\\\GitHub\\\\dataset\\\\np_dataset')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg) in enumerate(train_dataloader):\n",
    "        # Move data to the correct device\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
