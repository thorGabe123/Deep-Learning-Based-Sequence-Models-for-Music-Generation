{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import configs.common as cc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiLabelSequenceClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len] - token IDs\n",
    "        embeds = self.embedding(x)                    # [batch_size, seq_len, embed_dim]\n",
    "        out, _ = self.lstm(embeds)                    # [batch_size, seq_len, hidden_dim]\n",
    "        last_hidden = out[:, -1, :]                   # [batch_size, hidden_dim]\n",
    "        logits = self.fc(last_hidden)                 # [batch_size, num_classes]\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = F.softmax(probs).to('cuda')                 # [batch_size, num_classes], probabilities per class\n",
    "        return probs\n",
    "\n",
    "# Usage Example\n",
    "vocab_size = cc.vocab_size\n",
    "embed_dim = 512\n",
    "hidden_dim = 512\n",
    "num_classes = cc.metadata_vocab_size  # Set to your number of possible labels\n",
    "\n",
    "model = MultiLabelSequenceClassifier(vocab_size, embed_dim, hidden_dim, num_classes).to(\"cuda\")\n",
    "criterion = nn.BCELoss()  # or nn.BCEWithLogitsLoss (if you remove the sigmoid in the forward method)\n",
    "\n",
    "def get_set(tensor):\n",
    "    return [torch.unique(row) for row in tensor]\n",
    "\n",
    "def make_meta_target(tensor):\n",
    "    target = torch.zeros(cc.metadata_vocab_size)\n",
    "    target[tensor] = 1\n",
    "    return target\n",
    "\n",
    "def get_all_targets(tensor):\n",
    "    meta_unique = get_set(tensor)\n",
    "    return torch.stack([make_meta_target(m) for m in meta_unique])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "\n",
    "loader = processing.DatasetLoader('E:/GitHub/dataset/np_dataset')\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "# random_sample = loader.get_random_sample('train')\n",
    "# random_sample\n",
    "for src, trg, meta in train_dataloader:\n",
    "    break\n",
    "# src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thor\\AppData\\Local\\Temp\\ipykernel_10696\\4228207880.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(probs).to('cuda')                 # [batch_size, num_classes], probabilities per class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [10/362], Loss: 26.4427\n",
      "Epoch [1/1000], Step [20/362], Loss: 25.1842\n",
      "Epoch [1/1000], Step [30/362], Loss: 27.7017\n",
      "Epoch [1/1000], Step [40/362], Loss: 26.4419\n",
      "Epoch [1/1000], Step [50/362], Loss: 27.7016\n",
      "Epoch [1/1000], Step [60/362], Loss: 27.0702\n",
      "Epoch [1/1000], Step [70/362], Loss: 25.1801\n",
      "Epoch [1/1000], Step [80/362], Loss: 27.0653\n",
      "Epoch [1/1000], Step [90/362], Loss: 27.0577\n",
      "Epoch [1/1000], Step [100/362], Loss: 25.1678\n",
      "Epoch [1/1000], Step [110/362], Loss: 26.4242\n",
      "Epoch [1/1000], Step [120/362], Loss: 25.7920\n",
      "Epoch [1/1000], Step [130/362], Loss: 25.7904\n",
      "Epoch [1/1000], Step [140/362], Loss: 25.1573\n",
      "Epoch [1/1000], Step [150/362], Loss: 26.4168\n",
      "Epoch [1/1000], Step [160/362], Loss: 26.4175\n",
      "Epoch [1/1000], Step [170/362], Loss: 26.4197\n",
      "Epoch [1/1000], Step [180/362], Loss: 27.0444\n",
      "Epoch [1/1000], Step [190/362], Loss: 25.7925\n",
      "Epoch [1/1000], Step [200/362], Loss: 26.4129\n",
      "Epoch [1/1000], Step [210/362], Loss: 25.1540\n",
      "Epoch [1/1000], Step [220/362], Loss: 26.4146\n",
      "Epoch [1/1000], Step [230/362], Loss: 27.0442\n",
      "Epoch [1/1000], Step [240/362], Loss: 25.1563\n",
      "Epoch [1/1000], Step [250/362], Loss: 26.4149\n",
      "Epoch [1/1000], Step [260/362], Loss: 26.4135\n",
      "Epoch [1/1000], Step [270/362], Loss: 25.7831\n",
      "Epoch [1/1000], Step [280/362], Loss: 25.7869\n",
      "Epoch [1/1000], Step [290/362], Loss: 26.4147\n",
      "Epoch [1/1000], Step [300/362], Loss: 27.0465\n",
      "Epoch [1/1000], Step [310/362], Loss: 26.4123\n",
      "Epoch [1/1000], Step [320/362], Loss: 26.4146\n",
      "Epoch [1/1000], Step [330/362], Loss: 25.1536\n",
      "Epoch [1/1000], Step [340/362], Loss: 25.7880\n",
      "Epoch [1/1000], Step [350/362], Loss: 26.4133\n",
      "Epoch [1/1000], Step [360/362], Loss: 25.7814\n",
      "Epoch [1/1000], Average Loss: 26.1638\n",
      "Epoch [1/1000], Validation Loss: 26.1793\n",
      "Epoch [2/1000], Step [10/362], Loss: 27.0438\n",
      "Epoch [2/1000], Step [20/362], Loss: 27.0428\n",
      "Epoch [2/1000], Step [30/362], Loss: 28.3018\n",
      "Epoch [2/1000], Step [40/362], Loss: 25.7826\n",
      "Epoch [2/1000], Step [50/362], Loss: 26.4198\n",
      "Epoch [2/1000], Step [60/362], Loss: 26.4150\n",
      "Epoch [2/1000], Step [70/362], Loss: 24.5245\n",
      "Epoch [2/1000], Step [80/362], Loss: 25.7875\n",
      "Epoch [2/1000], Step [90/362], Loss: 25.1551\n",
      "Epoch [2/1000], Step [100/362], Loss: 26.4157\n",
      "Epoch [2/1000], Step [110/362], Loss: 25.1518\n",
      "Epoch [2/1000], Step [120/362], Loss: 26.4118\n",
      "Epoch [2/1000], Step [130/362], Loss: 27.0435\n",
      "Epoch [2/1000], Step [140/362], Loss: 27.0492\n",
      "Epoch [2/1000], Step [150/362], Loss: 25.1559\n",
      "Epoch [2/1000], Step [160/362], Loss: 26.4119\n",
      "Epoch [2/1000], Step [170/362], Loss: 27.6724\n",
      "Epoch [2/1000], Step [180/362], Loss: 25.1536\n",
      "Epoch [2/1000], Step [190/362], Loss: 27.6698\n",
      "Epoch [2/1000], Step [200/362], Loss: 26.4127\n",
      "Epoch [2/1000], Step [210/362], Loss: 27.0422\n",
      "Epoch [2/1000], Step [220/362], Loss: 25.7841\n",
      "Epoch [2/1000], Step [230/362], Loss: 25.7882\n",
      "Epoch [2/1000], Step [240/362], Loss: 27.0415\n",
      "Epoch [2/1000], Step [250/362], Loss: 26.4168\n",
      "Epoch [2/1000], Step [260/362], Loss: 25.7802\n",
      "Epoch [2/1000], Step [270/362], Loss: 25.7878\n",
      "Epoch [2/1000], Step [280/362], Loss: 26.4154\n",
      "Epoch [2/1000], Step [290/362], Loss: 25.7848\n",
      "Epoch [2/1000], Step [300/362], Loss: 27.0407\n",
      "Epoch [2/1000], Step [310/362], Loss: 26.4155\n",
      "Epoch [2/1000], Step [320/362], Loss: 25.1608\n",
      "Epoch [2/1000], Step [330/362], Loss: 25.7872\n",
      "Epoch [2/1000], Step [340/362], Loss: 26.4124\n",
      "Epoch [2/1000], Step [350/362], Loss: 25.1591\n",
      "Epoch [2/1000], Step [360/362], Loss: 26.4143\n",
      "Epoch [2/1000], Average Loss: 26.0863\n",
      "Epoch [2/1000], Validation Loss: 26.1167\n",
      "Epoch [3/1000], Step [10/362], Loss: 25.7886\n",
      "Epoch [3/1000], Step [20/362], Loss: 25.1645\n",
      "Epoch [3/1000], Step [30/362], Loss: 25.1554\n",
      "Epoch [3/1000], Step [40/362], Loss: 27.6692\n",
      "Epoch [3/1000], Step [50/362], Loss: 26.4210\n",
      "Epoch [3/1000], Step [60/362], Loss: 25.1560\n",
      "Epoch [3/1000], Step [70/362], Loss: 26.4154\n",
      "Epoch [3/1000], Step [80/362], Loss: 25.1537\n",
      "Epoch [3/1000], Step [90/362], Loss: 26.4164\n",
      "Epoch [3/1000], Step [100/362], Loss: 27.0404\n",
      "Epoch [3/1000], Step [110/362], Loss: 26.4091\n",
      "Epoch [3/1000], Step [120/362], Loss: 27.0404\n",
      "Epoch [3/1000], Step [130/362], Loss: 25.7870\n",
      "Epoch [3/1000], Step [140/362], Loss: 25.1582\n",
      "Epoch [3/1000], Step [150/362], Loss: 26.4160\n",
      "Epoch [3/1000], Step [160/362], Loss: 25.1544\n",
      "Epoch [3/1000], Step [170/362], Loss: 28.2970\n",
      "Epoch [3/1000], Step [180/362], Loss: 25.7842\n",
      "Epoch [3/1000], Step [190/362], Loss: 27.0420\n",
      "Epoch [3/1000], Step [200/362], Loss: 23.8964\n",
      "Epoch [3/1000], Step [210/362], Loss: 26.4157\n",
      "Epoch [3/1000], Step [220/362], Loss: 25.1595\n",
      "Epoch [3/1000], Step [230/362], Loss: 25.7849\n",
      "Epoch [3/1000], Step [240/362], Loss: 25.7870\n",
      "Epoch [3/1000], Step [250/362], Loss: 25.1550\n",
      "Epoch [3/1000], Step [260/362], Loss: 27.0445\n",
      "Epoch [3/1000], Step [270/362], Loss: 25.7864\n",
      "Epoch [3/1000], Step [280/362], Loss: 24.5251\n",
      "Epoch [3/1000], Step [290/362], Loss: 26.4105\n",
      "Epoch [3/1000], Step [300/362], Loss: 25.7890\n",
      "Epoch [3/1000], Step [310/362], Loss: 25.1577\n",
      "Epoch [3/1000], Step [320/362], Loss: 26.4140\n",
      "Epoch [3/1000], Step [330/362], Loss: 26.4102\n",
      "Epoch [3/1000], Step [340/362], Loss: 27.0411\n",
      "Epoch [3/1000], Step [350/362], Loss: 28.2988\n",
      "Epoch [3/1000], Step [360/362], Loss: 25.7876\n",
      "Epoch [3/1000], Average Loss: 26.1448\n",
      "Epoch [3/1000], Validation Loss: 26.1304\n",
      "Epoch [4/1000], Step [10/362], Loss: 26.4137\n",
      "Epoch [4/1000], Step [20/362], Loss: 24.5251\n",
      "Epoch [4/1000], Step [30/362], Loss: 24.5265\n",
      "Epoch [4/1000], Step [40/362], Loss: 25.7860\n",
      "Epoch [4/1000], Step [50/362], Loss: 27.0403\n",
      "Epoch [4/1000], Step [60/362], Loss: 26.4200\n",
      "Epoch [4/1000], Step [70/362], Loss: 26.4164\n",
      "Epoch [4/1000], Step [80/362], Loss: 27.0439\n",
      "Epoch [4/1000], Step [90/362], Loss: 25.1588\n",
      "Epoch [4/1000], Step [100/362], Loss: 25.7846\n",
      "Epoch [4/1000], Step [110/362], Loss: 25.7853\n",
      "Epoch [4/1000], Step [120/362], Loss: 25.1572\n",
      "Epoch [4/1000], Step [130/362], Loss: 25.7847\n",
      "Epoch [4/1000], Step [140/362], Loss: 26.4096\n",
      "Epoch [4/1000], Step [150/362], Loss: 27.6702\n",
      "Epoch [4/1000], Step [160/362], Loss: 25.7823\n",
      "Epoch [4/1000], Step [170/362], Loss: 26.4194\n",
      "Epoch [4/1000], Step [180/362], Loss: 25.1578\n",
      "Epoch [4/1000], Step [190/362], Loss: 25.1567\n",
      "Epoch [4/1000], Step [200/362], Loss: 26.4130\n",
      "Epoch [4/1000], Step [210/362], Loss: 25.7849\n",
      "Epoch [4/1000], Step [220/362], Loss: 25.1581\n",
      "Epoch [4/1000], Step [230/362], Loss: 26.4097\n",
      "Epoch [4/1000], Step [240/362], Loss: 26.4154\n",
      "Epoch [4/1000], Step [250/362], Loss: 25.7843\n",
      "Epoch [4/1000], Step [260/362], Loss: 25.7837\n",
      "Epoch [4/1000], Step [270/362], Loss: 26.4097\n",
      "Epoch [4/1000], Step [280/362], Loss: 25.1593\n",
      "Epoch [4/1000], Step [290/362], Loss: 25.1569\n",
      "Epoch [4/1000], Step [300/362], Loss: 25.7874\n",
      "Epoch [4/1000], Step [310/362], Loss: 26.4146\n",
      "Epoch [4/1000], Step [320/362], Loss: 25.7837\n",
      "Epoch [4/1000], Step [330/362], Loss: 28.2988\n",
      "Epoch [4/1000], Step [340/362], Loss: 25.7868\n",
      "Epoch [4/1000], Step [350/362], Loss: 25.1547\n",
      "Epoch [4/1000], Step [360/362], Loss: 27.0440\n",
      "Epoch [4/1000], Average Loss: 26.1448\n",
      "Epoch [4/1000], Validation Loss: 26.2275\n",
      "Epoch [5/1000], Step [10/362], Loss: 25.7880\n",
      "Epoch [5/1000], Step [20/362], Loss: 26.4156\n",
      "Epoch [5/1000], Step [30/362], Loss: 26.4147\n",
      "Epoch [5/1000], Step [40/362], Loss: 26.4137\n",
      "Epoch [5/1000], Step [50/362], Loss: 24.5251\n",
      "Epoch [5/1000], Step [60/362], Loss: 25.1596\n",
      "Epoch [5/1000], Step [70/362], Loss: 25.1587\n",
      "Epoch [5/1000], Step [80/362], Loss: 25.7850\n",
      "Epoch [5/1000], Step [90/362], Loss: 26.4147\n",
      "Epoch [5/1000], Step [100/362], Loss: 27.6723\n",
      "Epoch [5/1000], Step [110/362], Loss: 25.7844\n",
      "Epoch [5/1000], Step [120/362], Loss: 26.4142\n",
      "Epoch [5/1000], Step [130/362], Loss: 25.1563\n",
      "Epoch [5/1000], Step [140/362], Loss: 27.6726\n",
      "Epoch [5/1000], Step [150/362], Loss: 25.7847\n",
      "Epoch [5/1000], Step [160/362], Loss: 27.0428\n",
      "Epoch [5/1000], Step [170/362], Loss: 26.4106\n",
      "Epoch [5/1000], Step [180/362], Loss: 27.0455\n",
      "Epoch [5/1000], Step [190/362], Loss: 25.7880\n",
      "Epoch [5/1000], Step [200/362], Loss: 25.1572\n",
      "Epoch [5/1000], Step [210/362], Loss: 25.7857\n",
      "Epoch [5/1000], Step [220/362], Loss: 24.5280\n",
      "Epoch [5/1000], Step [230/362], Loss: 26.4155\n",
      "Epoch [5/1000], Step [240/362], Loss: 26.4121\n",
      "Epoch [5/1000], Step [250/362], Loss: 25.1544\n",
      "Epoch [5/1000], Step [260/362], Loss: 25.7863\n",
      "Epoch [5/1000], Step [270/362], Loss: 28.2977\n",
      "Epoch [5/1000], Step [280/362], Loss: 27.0381\n",
      "Epoch [5/1000], Step [290/362], Loss: 25.7852\n",
      "Epoch [5/1000], Step [300/362], Loss: 25.7838\n",
      "Epoch [5/1000], Step [310/362], Loss: 26.4125\n",
      "Epoch [5/1000], Step [320/362], Loss: 25.7854\n",
      "Epoch [5/1000], Step [330/362], Loss: 25.7850\n",
      "Epoch [5/1000], Step [340/362], Loss: 25.1615\n",
      "Epoch [5/1000], Step [350/362], Loss: 27.0442\n",
      "Epoch [5/1000], Step [360/362], Loss: 26.4166\n",
      "Epoch [5/1000], Average Loss: 26.0441\n",
      "Epoch [5/1000], Validation Loss: 26.0755\n",
      "Epoch [6/1000], Step [10/362], Loss: 25.1558\n",
      "Epoch [6/1000], Step [20/362], Loss: 25.7840\n",
      "Epoch [6/1000], Step [30/362], Loss: 27.0463\n",
      "Epoch [6/1000], Step [40/362], Loss: 25.1548\n",
      "Epoch [6/1000], Step [50/362], Loss: 28.3016\n",
      "Epoch [6/1000], Step [60/362], Loss: 25.1555\n",
      "Epoch [6/1000], Step [70/362], Loss: 25.7856\n",
      "Epoch [6/1000], Step [80/362], Loss: 25.7856\n",
      "Epoch [6/1000], Step [90/362], Loss: 25.7872\n",
      "Epoch [6/1000], Step [100/362], Loss: 27.0446\n",
      "Epoch [6/1000], Step [110/362], Loss: 25.7882\n",
      "Epoch [6/1000], Step [120/362], Loss: 26.4136\n",
      "Epoch [6/1000], Step [130/362], Loss: 27.0458\n",
      "Epoch [6/1000], Step [140/362], Loss: 26.4156\n",
      "Epoch [6/1000], Step [150/362], Loss: 25.1567\n",
      "Epoch [6/1000], Step [160/362], Loss: 25.7898\n",
      "Epoch [6/1000], Step [170/362], Loss: 25.7856\n",
      "Epoch [6/1000], Step [180/362], Loss: 26.4138\n",
      "Epoch [6/1000], Step [190/362], Loss: 25.1524\n",
      "Epoch [6/1000], Step [200/362], Loss: 25.1527\n",
      "Epoch [6/1000], Step [210/362], Loss: 25.1542\n",
      "Epoch [6/1000], Step [220/362], Loss: 26.4137\n",
      "Epoch [6/1000], Step [230/362], Loss: 24.5268\n",
      "Epoch [6/1000], Step [240/362], Loss: 25.1565\n",
      "Epoch [6/1000], Step [250/362], Loss: 26.4105\n",
      "Epoch [6/1000], Step [260/362], Loss: 26.4174\n",
      "Epoch [6/1000], Step [270/362], Loss: 25.1556\n",
      "Epoch [6/1000], Step [280/362], Loss: 26.4108\n",
      "Epoch [6/1000], Step [290/362], Loss: 25.7857\n",
      "Epoch [6/1000], Step [300/362], Loss: 27.0426\n",
      "Epoch [6/1000], Step [310/362], Loss: 25.7840\n",
      "Epoch [6/1000], Step [320/362], Loss: 25.7858\n",
      "Epoch [6/1000], Step [330/362], Loss: 26.4139\n",
      "Epoch [6/1000], Step [340/362], Loss: 25.7843\n",
      "Epoch [6/1000], Step [350/362], Loss: 26.4124\n",
      "Epoch [6/1000], Step [360/362], Loss: 25.7830\n",
      "Epoch [6/1000], Average Loss: 26.1102\n",
      "Epoch [6/1000], Validation Loss: 25.8544\n",
      "Epoch [7/1000], Step [10/362], Loss: 25.7822\n",
      "Epoch [7/1000], Step [20/362], Loss: 27.0397\n",
      "Epoch [7/1000], Step [30/362], Loss: 25.7822\n",
      "Epoch [7/1000], Step [40/362], Loss: 28.9279\n",
      "Epoch [7/1000], Step [50/362], Loss: 27.0487\n",
      "Epoch [7/1000], Step [60/362], Loss: 25.7833\n",
      "Epoch [7/1000], Step [70/362], Loss: 27.0449\n",
      "Epoch [7/1000], Step [80/362], Loss: 25.7895\n",
      "Epoch [7/1000], Step [90/362], Loss: 25.7876\n",
      "Epoch [7/1000], Step [100/362], Loss: 25.1602\n",
      "Epoch [7/1000], Step [110/362], Loss: 27.6677\n",
      "Epoch [7/1000], Step [120/362], Loss: 25.7851\n",
      "Epoch [7/1000], Step [130/362], Loss: 25.1572\n",
      "Epoch [7/1000], Step [140/362], Loss: 25.7835\n",
      "Epoch [7/1000], Step [150/362], Loss: 26.4149\n",
      "Epoch [7/1000], Step [160/362], Loss: 27.0419\n",
      "Epoch [7/1000], Step [170/362], Loss: 25.7860\n",
      "Epoch [7/1000], Step [180/362], Loss: 25.1578\n",
      "Epoch [7/1000], Step [190/362], Loss: 27.6693\n",
      "Epoch [7/1000], Step [200/362], Loss: 25.1525\n",
      "Epoch [7/1000], Step [210/362], Loss: 25.7864\n",
      "Epoch [7/1000], Step [220/362], Loss: 25.7898\n",
      "Epoch [7/1000], Step [230/362], Loss: 25.7863\n",
      "Epoch [7/1000], Step [240/362], Loss: 26.4135\n",
      "Epoch [7/1000], Step [250/362], Loss: 26.4152\n",
      "Epoch [7/1000], Step [260/362], Loss: 26.4144\n",
      "Epoch [7/1000], Step [270/362], Loss: 27.0410\n",
      "Epoch [7/1000], Step [280/362], Loss: 27.0398\n",
      "Epoch [7/1000], Step [290/362], Loss: 27.0429\n",
      "Epoch [7/1000], Step [300/362], Loss: 25.7853\n",
      "Epoch [7/1000], Step [310/362], Loss: 26.4134\n",
      "Epoch [7/1000], Step [320/362], Loss: 27.0417\n",
      "Epoch [7/1000], Step [330/362], Loss: 25.7870\n",
      "Epoch [7/1000], Step [340/362], Loss: 25.7858\n",
      "Epoch [7/1000], Step [350/362], Loss: 26.4186\n",
      "Epoch [7/1000], Step [360/362], Loss: 25.7875\n",
      "Epoch [7/1000], Average Loss: 26.1761\n",
      "Epoch [7/1000], Validation Loss: 26.0551\n",
      "Epoch [8/1000], Step [10/362], Loss: 25.7824\n",
      "Epoch [8/1000], Step [20/362], Loss: 25.7846\n",
      "Epoch [8/1000], Step [30/362], Loss: 27.0455\n",
      "Epoch [8/1000], Step [40/362], Loss: 26.4127\n",
      "Epoch [8/1000], Step [50/362], Loss: 27.0421\n",
      "Epoch [8/1000], Step [60/362], Loss: 25.7814\n",
      "Epoch [8/1000], Step [70/362], Loss: 27.0505\n",
      "Epoch [8/1000], Step [80/362], Loss: 27.0424\n",
      "Epoch [8/1000], Step [90/362], Loss: 26.4112\n",
      "Epoch [8/1000], Step [100/362], Loss: 25.7823\n",
      "Epoch [8/1000], Step [110/362], Loss: 25.7880\n",
      "Epoch [8/1000], Step [120/362], Loss: 25.1547\n",
      "Epoch [8/1000], Step [130/362], Loss: 27.0439\n",
      "Epoch [8/1000], Step [140/362], Loss: 26.4189\n",
      "Epoch [8/1000], Step [150/362], Loss: 25.1528\n",
      "Epoch [8/1000], Step [160/362], Loss: 27.0456\n",
      "Epoch [8/1000], Step [170/362], Loss: 27.0412\n",
      "Epoch [8/1000], Step [180/362], Loss: 25.7842\n",
      "Epoch [8/1000], Step [190/362], Loss: 25.7871\n",
      "Epoch [8/1000], Step [200/362], Loss: 25.1566\n",
      "Epoch [8/1000], Step [210/362], Loss: 25.7846\n",
      "Epoch [8/1000], Step [220/362], Loss: 26.4115\n",
      "Epoch [8/1000], Step [230/362], Loss: 25.7878\n",
      "Epoch [8/1000], Step [240/362], Loss: 25.7866\n",
      "Epoch [8/1000], Step [250/362], Loss: 25.7846\n",
      "Epoch [8/1000], Step [260/362], Loss: 26.4174\n",
      "Epoch [8/1000], Step [270/362], Loss: 26.4100\n",
      "Epoch [8/1000], Step [280/362], Loss: 27.6686\n",
      "Epoch [8/1000], Step [290/362], Loss: 26.4163\n",
      "Epoch [8/1000], Step [300/362], Loss: 25.7905\n",
      "Epoch [8/1000], Step [310/362], Loss: 26.4121\n",
      "Epoch [8/1000], Step [320/362], Loss: 23.9020\n",
      "Epoch [8/1000], Step [330/362], Loss: 25.1591\n",
      "Epoch [8/1000], Step [340/362], Loss: 25.7841\n",
      "Epoch [8/1000], Step [350/362], Loss: 25.1592\n",
      "Epoch [8/1000], Step [360/362], Loss: 26.4098\n",
      "Epoch [8/1000], Average Loss: 26.1152\n",
      "Epoch [8/1000], Validation Loss: 26.0750\n",
      "Epoch [9/1000], Step [10/362], Loss: 25.1610\n",
      "Epoch [9/1000], Step [20/362], Loss: 26.4135\n",
      "Epoch [9/1000], Step [30/362], Loss: 25.1580\n",
      "Epoch [9/1000], Step [40/362], Loss: 25.7892\n",
      "Epoch [9/1000], Step [50/362], Loss: 25.7847\n",
      "Epoch [9/1000], Step [60/362], Loss: 25.1573\n",
      "Epoch [9/1000], Step [70/362], Loss: 26.4148\n",
      "Epoch [9/1000], Step [80/362], Loss: 27.0436\n",
      "Epoch [9/1000], Step [90/362], Loss: 25.7826\n",
      "Epoch [9/1000], Step [100/362], Loss: 25.7883\n",
      "Epoch [9/1000], Step [110/362], Loss: 26.4107\n",
      "Epoch [9/1000], Step [120/362], Loss: 25.1560\n",
      "Epoch [9/1000], Step [130/362], Loss: 24.5270\n",
      "Epoch [9/1000], Step [140/362], Loss: 25.7834\n",
      "Epoch [9/1000], Step [150/362], Loss: 26.4119\n",
      "Epoch [9/1000], Step [160/362], Loss: 26.4118\n",
      "Epoch [9/1000], Step [170/362], Loss: 27.0429\n",
      "Epoch [9/1000], Step [180/362], Loss: 25.7886\n",
      "Epoch [9/1000], Step [190/362], Loss: 25.1561\n",
      "Epoch [9/1000], Step [200/362], Loss: 25.1590\n",
      "Epoch [9/1000], Step [210/362], Loss: 27.0435\n",
      "Epoch [9/1000], Step [220/362], Loss: 24.5269\n",
      "Epoch [9/1000], Step [230/362], Loss: 26.4164\n",
      "Epoch [9/1000], Step [240/362], Loss: 25.7870\n",
      "Epoch [9/1000], Step [250/362], Loss: 25.7828\n",
      "Epoch [9/1000], Step [260/362], Loss: 26.4169\n",
      "Epoch [9/1000], Step [270/362], Loss: 25.7910\n",
      "Epoch [9/1000], Step [280/362], Loss: 27.0454\n",
      "Epoch [9/1000], Step [290/362], Loss: 26.4144\n",
      "Epoch [9/1000], Step [300/362], Loss: 26.4152\n",
      "Epoch [9/1000], Step [310/362], Loss: 27.0410\n",
      "Epoch [9/1000], Step [320/362], Loss: 26.4169\n",
      "Epoch [9/1000], Step [330/362], Loss: 26.4110\n",
      "Epoch [9/1000], Step [340/362], Loss: 25.1566\n",
      "Epoch [9/1000], Step [350/362], Loss: 26.4125\n",
      "Epoch [9/1000], Step [360/362], Loss: 26.4152\n",
      "Epoch [9/1000], Average Loss: 26.1031\n",
      "Epoch [9/1000], Validation Loss: 26.0127\n",
      "Epoch [10/1000], Step [10/362], Loss: 25.1546\n",
      "Epoch [10/1000], Step [20/362], Loss: 26.4114\n",
      "Epoch [10/1000], Step [30/362], Loss: 25.1577\n",
      "Epoch [10/1000], Step [40/362], Loss: 25.1581\n",
      "Epoch [10/1000], Step [50/362], Loss: 26.4175\n",
      "Epoch [10/1000], Step [60/362], Loss: 25.7865\n",
      "Epoch [10/1000], Step [70/362], Loss: 25.7882\n",
      "Epoch [10/1000], Step [80/362], Loss: 26.4143\n",
      "Epoch [10/1000], Step [90/362], Loss: 26.4123\n",
      "Epoch [10/1000], Step [100/362], Loss: 25.7836\n",
      "Epoch [10/1000], Step [110/362], Loss: 26.4139\n",
      "Epoch [10/1000], Step [120/362], Loss: 27.0397\n",
      "Epoch [10/1000], Step [130/362], Loss: 26.4119\n",
      "Epoch [10/1000], Step [140/362], Loss: 27.0456\n",
      "Epoch [10/1000], Step [150/362], Loss: 27.0446\n",
      "Epoch [10/1000], Step [160/362], Loss: 25.7854\n",
      "Epoch [10/1000], Step [170/362], Loss: 25.7884\n",
      "Epoch [10/1000], Step [180/362], Loss: 25.1555\n",
      "Epoch [10/1000], Step [190/362], Loss: 26.4123\n",
      "Epoch [10/1000], Step [200/362], Loss: 25.1562\n",
      "Epoch [10/1000], Step [210/362], Loss: 25.1557\n",
      "Epoch [10/1000], Step [220/362], Loss: 27.0420\n",
      "Epoch [10/1000], Step [230/362], Loss: 26.4127\n",
      "Epoch [10/1000], Step [240/362], Loss: 27.0439\n",
      "Epoch [10/1000], Step [250/362], Loss: 27.6762\n",
      "Epoch [10/1000], Step [260/362], Loss: 25.1543\n",
      "Epoch [10/1000], Step [270/362], Loss: 26.4126\n",
      "Epoch [10/1000], Step [280/362], Loss: 27.0460\n",
      "Epoch [10/1000], Step [290/362], Loss: 26.4151\n",
      "Epoch [10/1000], Step [300/362], Loss: 25.7841\n",
      "Epoch [10/1000], Step [310/362], Loss: 24.5268\n",
      "Epoch [10/1000], Step [320/362], Loss: 26.4141\n",
      "Epoch [10/1000], Step [330/362], Loss: 26.4156\n",
      "Epoch [10/1000], Step [340/362], Loss: 25.1562\n",
      "Epoch [10/1000], Step [350/362], Loss: 26.4113\n",
      "Epoch [10/1000], Step [360/362], Loss: 27.6699\n",
      "Epoch [10/1000], Average Loss: 26.1485\n",
      "Epoch [10/1000], Validation Loss: 25.9239\n",
      "Epoch [11/1000], Step [10/362], Loss: 25.7857\n",
      "Epoch [11/1000], Step [20/362], Loss: 27.0423\n",
      "Epoch [11/1000], Step [30/362], Loss: 26.4160\n",
      "Epoch [11/1000], Step [40/362], Loss: 26.4139\n",
      "Epoch [11/1000], Step [50/362], Loss: 25.1608\n",
      "Epoch [11/1000], Step [60/362], Loss: 25.7832\n",
      "Epoch [11/1000], Step [70/362], Loss: 25.1528\n",
      "Epoch [11/1000], Step [80/362], Loss: 25.1585\n",
      "Epoch [11/1000], Step [90/362], Loss: 27.0450\n",
      "Epoch [11/1000], Step [100/362], Loss: 25.1516\n",
      "Epoch [11/1000], Step [110/362], Loss: 25.7854\n",
      "Epoch [11/1000], Step [120/362], Loss: 25.7852\n",
      "Epoch [11/1000], Step [130/362], Loss: 25.7891\n",
      "Epoch [11/1000], Step [140/362], Loss: 25.7853\n",
      "Epoch [11/1000], Step [150/362], Loss: 25.1601\n",
      "Epoch [11/1000], Step [160/362], Loss: 26.4178\n",
      "Epoch [11/1000], Step [170/362], Loss: 27.6685\n",
      "Epoch [11/1000], Step [180/362], Loss: 25.7865\n",
      "Epoch [11/1000], Step [190/362], Loss: 25.1550\n",
      "Epoch [11/1000], Step [200/362], Loss: 26.4126\n",
      "Epoch [11/1000], Step [210/362], Loss: 26.4159\n",
      "Epoch [11/1000], Step [220/362], Loss: 25.7913\n",
      "Epoch [11/1000], Step [230/362], Loss: 27.0419\n",
      "Epoch [11/1000], Step [240/362], Loss: 27.0399\n",
      "Epoch [11/1000], Step [250/362], Loss: 25.7836\n",
      "Epoch [11/1000], Step [260/362], Loss: 25.7865\n",
      "Epoch [11/1000], Step [270/362], Loss: 27.0399\n",
      "Epoch [11/1000], Step [280/362], Loss: 26.4141\n",
      "Epoch [11/1000], Step [290/362], Loss: 25.7871\n",
      "Epoch [11/1000], Step [300/362], Loss: 25.7826\n",
      "Epoch [11/1000], Step [310/362], Loss: 26.4162\n",
      "Epoch [11/1000], Step [320/362], Loss: 27.0446\n",
      "Epoch [11/1000], Step [330/362], Loss: 25.7863\n",
      "Epoch [11/1000], Step [340/362], Loss: 27.0414\n",
      "Epoch [11/1000], Step [350/362], Loss: 26.4139\n",
      "Epoch [11/1000], Step [360/362], Loss: 24.5267\n",
      "Epoch [11/1000], Average Loss: 26.1758\n",
      "Epoch [11/1000], Validation Loss: 26.1441\n",
      "Epoch [12/1000], Step [10/362], Loss: 25.7881\n",
      "Epoch [12/1000], Step [20/362], Loss: 25.7853\n",
      "Epoch [12/1000], Step [30/362], Loss: 25.7869\n",
      "Epoch [12/1000], Step [40/362], Loss: 26.4183\n",
      "Epoch [12/1000], Step [50/362], Loss: 25.7869\n",
      "Epoch [12/1000], Step [60/362], Loss: 26.4114\n",
      "Epoch [12/1000], Step [70/362], Loss: 26.4151\n",
      "Epoch [12/1000], Step [80/362], Loss: 26.4132\n",
      "Epoch [12/1000], Step [90/362], Loss: 25.7839\n",
      "Epoch [12/1000], Step [100/362], Loss: 27.6722\n",
      "Epoch [12/1000], Step [110/362], Loss: 25.1562\n",
      "Epoch [12/1000], Step [120/362], Loss: 27.6698\n",
      "Epoch [12/1000], Step [130/362], Loss: 25.1574\n",
      "Epoch [12/1000], Step [140/362], Loss: 25.1606\n",
      "Epoch [12/1000], Step [150/362], Loss: 27.0435\n",
      "Epoch [12/1000], Step [160/362], Loss: 27.0438\n",
      "Epoch [12/1000], Step [170/362], Loss: 26.4136\n",
      "Epoch [12/1000], Step [180/362], Loss: 25.7853\n",
      "Epoch [12/1000], Step [190/362], Loss: 27.6770\n",
      "Epoch [12/1000], Step [200/362], Loss: 26.4147\n",
      "Epoch [12/1000], Step [210/362], Loss: 25.1551\n",
      "Epoch [12/1000], Step [220/362], Loss: 25.7888\n",
      "Epoch [12/1000], Step [230/362], Loss: 26.4164\n",
      "Epoch [12/1000], Step [240/362], Loss: 26.4184\n",
      "Epoch [12/1000], Step [250/362], Loss: 25.7850\n",
      "Epoch [12/1000], Step [260/362], Loss: 28.3006\n",
      "Epoch [12/1000], Step [270/362], Loss: 25.1581\n",
      "Epoch [12/1000], Step [280/362], Loss: 27.0413\n",
      "Epoch [12/1000], Step [290/362], Loss: 26.4127\n",
      "Epoch [12/1000], Step [300/362], Loss: 25.7850\n",
      "Epoch [12/1000], Step [310/362], Loss: 25.1566\n",
      "Epoch [12/1000], Step [320/362], Loss: 25.7887\n",
      "Epoch [12/1000], Step [330/362], Loss: 25.7932\n",
      "Epoch [12/1000], Step [340/362], Loss: 26.4111\n",
      "Epoch [12/1000], Step [350/362], Loss: 27.0425\n",
      "Epoch [12/1000], Step [360/362], Loss: 25.7861\n",
      "Epoch [12/1000], Average Loss: 26.1953\n",
      "Epoch [12/1000], Validation Loss: 26.0066\n",
      "Epoch [13/1000], Step [10/362], Loss: 25.7849\n",
      "Epoch [13/1000], Step [20/362], Loss: 25.7841\n",
      "Epoch [13/1000], Step [30/362], Loss: 27.0446\n",
      "Epoch [13/1000], Step [40/362], Loss: 27.0445\n",
      "Epoch [13/1000], Step [50/362], Loss: 26.4127\n",
      "Epoch [13/1000], Step [60/362], Loss: 25.7852\n",
      "Epoch [13/1000], Step [70/362], Loss: 25.7875\n",
      "Epoch [13/1000], Step [80/362], Loss: 25.7849\n",
      "Epoch [13/1000], Step [90/362], Loss: 25.7842\n",
      "Epoch [13/1000], Step [100/362], Loss: 25.7827\n",
      "Epoch [13/1000], Step [110/362], Loss: 25.7829\n",
      "Epoch [13/1000], Step [120/362], Loss: 25.1577\n",
      "Epoch [13/1000], Step [130/362], Loss: 27.6706\n",
      "Epoch [13/1000], Step [140/362], Loss: 26.4137\n",
      "Epoch [13/1000], Step [150/362], Loss: 25.7814\n",
      "Epoch [13/1000], Step [160/362], Loss: 25.7838\n",
      "Epoch [13/1000], Step [170/362], Loss: 25.1544\n",
      "Epoch [13/1000], Step [180/362], Loss: 25.7878\n",
      "Epoch [13/1000], Step [190/362], Loss: 25.1554\n",
      "Epoch [13/1000], Step [200/362], Loss: 26.4146\n",
      "Epoch [13/1000], Step [210/362], Loss: 25.1575\n",
      "Epoch [13/1000], Step [220/362], Loss: 26.4173\n",
      "Epoch [13/1000], Step [230/362], Loss: 26.4112\n",
      "Epoch [13/1000], Step [240/362], Loss: 26.4123\n",
      "Epoch [13/1000], Step [250/362], Loss: 25.1600\n",
      "Epoch [13/1000], Step [260/362], Loss: 26.4101\n",
      "Epoch [13/1000], Step [270/362], Loss: 25.7818\n",
      "Epoch [13/1000], Step [280/362], Loss: 25.7838\n",
      "Epoch [13/1000], Step [290/362], Loss: 27.0424\n",
      "Epoch [13/1000], Step [300/362], Loss: 26.4139\n",
      "Epoch [13/1000], Step [310/362], Loss: 25.7814\n",
      "Epoch [13/1000], Step [320/362], Loss: 26.4124\n",
      "Epoch [13/1000], Step [330/362], Loss: 25.1577\n",
      "Epoch [13/1000], Step [340/362], Loss: 25.7849\n",
      "Epoch [13/1000], Step [350/362], Loss: 25.7849\n",
      "Epoch [13/1000], Step [360/362], Loss: 27.0409\n",
      "Epoch [13/1000], Average Loss: 26.1116\n",
      "Epoch [13/1000], Validation Loss: 26.0270\n",
      "Epoch [14/1000], Step [10/362], Loss: 25.7851\n",
      "Epoch [14/1000], Step [20/362], Loss: 24.5246\n",
      "Epoch [14/1000], Step [30/362], Loss: 26.4136\n",
      "Epoch [14/1000], Step [40/362], Loss: 26.4160\n",
      "Epoch [14/1000], Step [50/362], Loss: 26.4134\n",
      "Epoch [14/1000], Step [60/362], Loss: 25.1569\n",
      "Epoch [14/1000], Step [70/362], Loss: 25.7874\n",
      "Epoch [14/1000], Step [80/362], Loss: 26.4182\n",
      "Epoch [14/1000], Step [90/362], Loss: 25.7861\n",
      "Epoch [14/1000], Step [100/362], Loss: 25.7882\n",
      "Epoch [14/1000], Step [110/362], Loss: 25.7862\n",
      "Epoch [14/1000], Step [120/362], Loss: 26.4124\n",
      "Epoch [14/1000], Step [130/362], Loss: 25.1541\n",
      "Epoch [14/1000], Step [140/362], Loss: 26.4115\n",
      "Epoch [14/1000], Step [150/362], Loss: 27.0438\n",
      "Epoch [14/1000], Step [160/362], Loss: 25.7837\n",
      "Epoch [14/1000], Step [170/362], Loss: 27.0409\n",
      "Epoch [14/1000], Step [180/362], Loss: 24.5289\n",
      "Epoch [14/1000], Step [190/362], Loss: 24.5258\n",
      "Epoch [14/1000], Step [200/362], Loss: 25.7838\n",
      "Epoch [14/1000], Step [210/362], Loss: 25.7848\n",
      "Epoch [14/1000], Step [220/362], Loss: 26.4130\n",
      "Epoch [14/1000], Step [230/362], Loss: 25.7841\n",
      "Epoch [14/1000], Step [240/362], Loss: 25.1543\n",
      "Epoch [14/1000], Step [250/362], Loss: 25.7854\n",
      "Epoch [14/1000], Step [260/362], Loss: 27.0399\n",
      "Epoch [14/1000], Step [270/362], Loss: 25.7839\n",
      "Epoch [14/1000], Step [280/362], Loss: 26.4128\n",
      "Epoch [14/1000], Step [290/362], Loss: 25.1542\n",
      "Epoch [14/1000], Step [300/362], Loss: 25.1564\n",
      "Epoch [14/1000], Step [310/362], Loss: 25.7831\n",
      "Epoch [14/1000], Step [320/362], Loss: 26.4100\n",
      "Epoch [14/1000], Step [330/362], Loss: 25.7852\n",
      "Epoch [14/1000], Step [340/362], Loss: 26.4162\n",
      "Epoch [14/1000], Step [350/362], Loss: 26.4102\n",
      "Epoch [14/1000], Step [360/362], Loss: 26.4139\n",
      "Epoch [14/1000], Average Loss: 26.0302\n",
      "Epoch [14/1000], Validation Loss: 26.0547\n",
      "Epoch [15/1000], Step [10/362], Loss: 27.0412\n",
      "Epoch [15/1000], Step [20/362], Loss: 26.4136\n",
      "Epoch [15/1000], Step [30/362], Loss: 27.0422\n",
      "Epoch [15/1000], Step [40/362], Loss: 26.4159\n",
      "Epoch [15/1000], Step [50/362], Loss: 25.7829\n",
      "Epoch [15/1000], Step [60/362], Loss: 27.0435\n",
      "Epoch [15/1000], Step [70/362], Loss: 26.4112\n",
      "Epoch [15/1000], Step [80/362], Loss: 27.6707\n",
      "Epoch [15/1000], Step [90/362], Loss: 25.7827\n",
      "Epoch [15/1000], Step [100/362], Loss: 25.7861\n",
      "Epoch [15/1000], Step [110/362], Loss: 25.1575\n",
      "Epoch [15/1000], Step [120/362], Loss: 27.0387\n",
      "Epoch [15/1000], Step [130/362], Loss: 25.1571\n",
      "Epoch [15/1000], Step [140/362], Loss: 25.7825\n",
      "Epoch [15/1000], Step [150/362], Loss: 25.7850\n",
      "Epoch [15/1000], Step [160/362], Loss: 26.4113\n",
      "Epoch [15/1000], Step [170/362], Loss: 25.7938\n",
      "Epoch [15/1000], Step [180/362], Loss: 26.4177\n",
      "Epoch [15/1000], Step [190/362], Loss: 26.4124\n",
      "Epoch [15/1000], Step [200/362], Loss: 26.4112\n",
      "Epoch [15/1000], Step [210/362], Loss: 25.7876\n",
      "Epoch [15/1000], Step [220/362], Loss: 25.7840\n",
      "Epoch [15/1000], Step [230/362], Loss: 25.1565\n",
      "Epoch [15/1000], Step [240/362], Loss: 25.7839\n",
      "Epoch [15/1000], Step [250/362], Loss: 25.7864\n",
      "Epoch [15/1000], Step [260/362], Loss: 25.7875\n",
      "Epoch [15/1000], Step [270/362], Loss: 26.4136\n",
      "Epoch [15/1000], Step [280/362], Loss: 25.1543\n",
      "Epoch [15/1000], Step [290/362], Loss: 26.4124\n",
      "Epoch [15/1000], Step [300/362], Loss: 26.4126\n",
      "Epoch [15/1000], Step [310/362], Loss: 25.7832\n",
      "Epoch [15/1000], Step [320/362], Loss: 27.0458\n",
      "Epoch [15/1000], Step [330/362], Loss: 26.4196\n",
      "Epoch [15/1000], Step [340/362], Loss: 27.0422\n",
      "Epoch [15/1000], Step [350/362], Loss: 25.7852\n",
      "Epoch [15/1000], Step [360/362], Loss: 25.7842\n",
      "Epoch [15/1000], Average Loss: 26.0996\n",
      "Epoch [15/1000], Validation Loss: 25.8886\n",
      "Epoch [16/1000], Step [10/362], Loss: 26.4124\n",
      "Epoch [16/1000], Step [20/362], Loss: 25.1587\n",
      "Epoch [16/1000], Step [30/362], Loss: 25.7861\n",
      "Epoch [16/1000], Step [40/362], Loss: 25.1566\n",
      "Epoch [16/1000], Step [50/362], Loss: 26.4105\n",
      "Epoch [16/1000], Step [60/362], Loss: 26.4137\n",
      "Epoch [16/1000], Step [70/362], Loss: 25.1552\n",
      "Epoch [16/1000], Step [80/362], Loss: 26.4173\n",
      "Epoch [16/1000], Step [90/362], Loss: 25.7862\n",
      "Epoch [16/1000], Step [100/362], Loss: 27.6730\n",
      "Epoch [16/1000], Step [110/362], Loss: 25.7864\n",
      "Epoch [16/1000], Step [120/362], Loss: 25.7826\n",
      "Epoch [16/1000], Step [130/362], Loss: 25.1578\n",
      "Epoch [16/1000], Step [140/362], Loss: 26.4124\n",
      "Epoch [16/1000], Step [150/362], Loss: 26.4125\n",
      "Epoch [16/1000], Step [160/362], Loss: 25.7874\n",
      "Epoch [16/1000], Step [170/362], Loss: 25.7838\n",
      "Epoch [16/1000], Step [180/362], Loss: 26.4146\n",
      "Epoch [16/1000], Step [190/362], Loss: 26.4150\n",
      "Epoch [16/1000], Step [200/362], Loss: 26.4182\n",
      "Epoch [16/1000], Step [210/362], Loss: 27.0412\n",
      "Epoch [16/1000], Step [220/362], Loss: 27.0411\n",
      "Epoch [16/1000], Step [230/362], Loss: 25.7814\n",
      "Epoch [16/1000], Step [240/362], Loss: 25.7873\n",
      "Epoch [16/1000], Step [250/362], Loss: 25.1576\n",
      "Epoch [16/1000], Step [260/362], Loss: 26.4145\n",
      "Epoch [16/1000], Step [270/362], Loss: 28.3019\n",
      "Epoch [16/1000], Step [280/362], Loss: 26.4117\n",
      "Epoch [16/1000], Step [290/362], Loss: 25.7842\n",
      "Epoch [16/1000], Step [300/362], Loss: 26.4120\n",
      "Epoch [16/1000], Step [310/362], Loss: 27.0408\n",
      "Epoch [16/1000], Step [320/362], Loss: 27.0418\n",
      "Epoch [16/1000], Step [330/362], Loss: 26.4193\n",
      "Epoch [16/1000], Step [340/362], Loss: 25.7864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m model.train()\n\u001b[32m     34\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_all_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\processing\\dataset.py:179\u001b[39m, in \u001b[36mSequenceDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    176\u001b[39m     sequence = sequence[ix: ix + seq_len_extra]\n\u001b[32m    178\u001b[39m sequence = torch.tensor(sequence, device=cc.config.values.device, dtype=torch.long)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m sequence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_augementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Fetch metadata for the band\u001b[39;00m\n\u001b[32m    182\u001b[39m path_parts = Path(file_path).parts\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\processing\\dataset.py:142\u001b[39m, in \u001b[36mSequenceDataset.data_augementation\u001b[39m\u001b[34m(self, sequence)\u001b[39m\n\u001b[32m    139\u001b[39m sequence = shift_sequence_drums(sequence, note_r_ints, note_lb, note_ub)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Velocity shifting\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m vel_r_ints = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m vel_lb = cc.start_idx[\u001b[33m'\u001b[39m\u001b[33mdyn\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    144\u001b[39m vel_ub = cc.start_idx[\u001b[33m'\u001b[39m\u001b[33mdyn\u001b[39m\u001b[33m'\u001b[39m] + cc.config.discretization.dyn - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1520.0_x64__qbz5n2kfra8p0\\Lib\\random.py:336\u001b[39m, in \u001b[36mRandom.randint\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mempty range in randrange(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m istart + istep * \u001b[38;5;28mself\u001b[39m._randbelow(n)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.randrange(a, b+\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import configs.paths as paths\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_model(model, loss):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    pretrained_path = paths.config.paths.pretrained\n",
    "\n",
    "    save_path = f'{pretrained_path}/classifier/loss_{loss:.2f}_time_{now}.pth'\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "dataset_path = paths.config.paths.np_dataset\n",
    "loader = processing.DatasetLoader(dataset_path)\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cc.config.values.learning_rate)\n",
    "\n",
    "# Logging setup\n",
    "log_data = []\n",
    "log_file_path = f'training_log_classifier.json'\n",
    "\n",
    "# Training loop\n",
    "num_epochs = cc.config.values.epochs\n",
    "print('Training started!')\n",
    "log_data.append({'timestamp': str(datetime.now()), 'message': 'Training started!'})\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg, meta) in enumerate(train_dataloader):\n",
    "        output = model(src).to('cuda')\n",
    "        trg = get_all_targets(meta).to('cuda')\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % cc.config.values.eval_interval == 0:\n",
    "            msg = f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}'\n",
    "            print(msg)\n",
    "            log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    msg = f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}'\n",
    "    print(msg)\n",
    "    log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, meta in test_dataloader:\n",
    "            output = model(src).to('cuda')\n",
    "            trg = get_all_targets(meta).to('cuda')\n",
    "            val_loss += criterion(output, trg).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    msg = f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}'\n",
    "    print(msg)\n",
    "    log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    if (epoch + 1) % cc.config.values.save_interval == 0:\n",
    "        save_model(model, avg_val_loss)\n",
    "        with open(log_file_path, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "log_data.append({'timestamp': str(datetime.now()), 'message': 'Training complete!'})\n",
    "\n",
    "save_model(model, avg_val_loss)\n",
    "\n",
    "# Final log save\n",
    "with open(log_file_path, 'w') as f:\n",
    "    json.dump(log_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import configs.mamba as cm\n",
    "import configs.xlstm as cx\n",
    "import configs.transformer as ct\n",
    "import configs.common as cc\n",
    "import models\n",
    "import math\n",
    "import processing\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "from datetime import datetime\n",
    "import configs.paths as paths\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import json\n",
    "def get_transformer_dict():\n",
    "    config = ct.config.model_values\n",
    "    config.vocab_size = cc.vocab_size\n",
    "    config.metadata_vocab_size = cc.metadata_vocab_size\n",
    "    config = SimpleNamespace(**vars(config), **vars(cc.config.values))\n",
    "    return config\n",
    "def new_model(type):\n",
    "    if type == \"mamba\":\n",
    "        model = models.mamba.Mamba(512, 12)\n",
    "    elif type == \"xlstm\":\n",
    "        from models.xlstm import xLSTM\n",
    "        model = xLSTM()\n",
    "    elif type == \"transformer\":\n",
    "        transformer_dict = get_transformer_dict()\n",
    "        model = models.transformer.Transformer(transformer_dict)\n",
    "    return model\n",
    "\n",
    "def load_model(type, name):\n",
    "    model = new_model(type)\n",
    "    pretrained = paths.config.paths.pretrained\n",
    "    model.load_state_dict(torch.load(f'{pretrained}/{type}/{name}'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 16,204,002\n"
     ]
    }
   ],
   "source": [
    "model = new_model('transformer')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   45, 16585, 16665,  ..., 16739, 16750, 16960])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(src, meta)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
