{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.xlstm.utils import BlockDiagonal, CausalConv1D\n",
    "class sLSTMblock(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embd = params.vocab_size\n",
    "        self.ln = nn.LayerNorm(self.n_embd)\n",
    "        \n",
    "        self.conv = CausalConv1D(self.n_embd, self.n_embd, int(self.n_embd/8))\n",
    "        self.drop = nn.Dropout(params.dropout)\n",
    "        \n",
    "        self.i_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth)\n",
    "        self.f_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth)\n",
    "        self.o_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth)\n",
    "        self.z_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth)\n",
    "        \n",
    "        self.ri_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth, bias=False)\n",
    "        self.rf_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth, bias=False)\n",
    "        self.ro_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth, bias=False)\n",
    "        self.rz_gate = BlockDiagonal(self.n_embd, self.n_embd, params.depth, bias=False)\n",
    "\n",
    "        self.ln_i = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_f = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_o = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_z = nn.LayerNorm(self.n_embd)\n",
    "        \n",
    "        self.GN = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_c = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_n = nn.LayerNorm(self.n_embd)\n",
    "        self.ln_h = nn.LayerNorm(self.n_embd)\n",
    "        \n",
    "        self.left_linear = nn.Linear(self.n_embd, int(self.n_embd*(4/3)))\n",
    "        self.right_linear = nn.Linear(self.n_embd, int(self.n_embd*(4/3)))\n",
    "\n",
    "        self.ln_out = nn.LayerNorm(int(self.n_embd*(4/3)))\n",
    "        \n",
    "        self.proj = nn.Linear(int(self.n_embd*(4/3)), self.n_embd)\n",
    "        \n",
    "        self.init_states(params)\n",
    "        \n",
    "    def init_states(self, params):\n",
    "        self.nt_1 = torch.zeros(1, 1, self.n_embd, device=params.device)\n",
    "        self.ct_1 = torch.zeros(1, 1, self.n_embd, device=params.device)\n",
    "        self.ht_1 = torch.zeros(1, 1, self.n_embd, device=params.device)\n",
    "        self.mt_1 = torch.zeros(1, 1, self.n_embd, device=params.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        x_conv = F.silu( self.drop(self.conv( x.transpose(1, 2) ).transpose(1, 2) ) )\n",
    "        \n",
    "        batch_size = x.size(0)  # Get dynamic batch size from input\n",
    "        \n",
    "        if self.mt_1.size(0) != batch_size:\n",
    "            # Adjust self.mt_1 to match current batch size.\n",
    "            self.mt_1 = self.mt_1[:batch_size]\n",
    "\n",
    "        # start sLSTM\n",
    "        ht_1 = self.ht_1\n",
    "        \n",
    "        i = torch.exp(self.ln_i( self.i_gate(x_conv) + self.ri_gate(ht_1) ) )\n",
    "        f = torch.exp( self.ln_f(self.f_gate(x_conv) + self.rf_gate(ht_1) ) )\n",
    "\n",
    "        m = torch.max(torch.log(f)+self.mt_1[:, 0, :].unsqueeze(1), torch.log(i))\n",
    "        i = torch.exp(torch.log(i) - m)\n",
    "        f = torch.exp(torch.log(f) + self.mt_1[:, 0, :].unsqueeze(1)-m)\n",
    "        self.mt_1 = m.detach()\n",
    "        \n",
    "        o = torch.sigmoid( self.ln_o(self.o_gate(x) + self.ro_gate(ht_1) ) )\n",
    "        z = torch.tanh( self.ln_z(self.z_gate(x) + self.rz_gate(ht_1) ) )\n",
    "        \n",
    "        ct_1 = self.ct_1\n",
    "        ct = f*ct_1 + i*z\n",
    "        ct = torch.mean(self.ln_c(ct), [0, 1], keepdim=True)\n",
    "        self.ct_1 = ct.detach()\n",
    "        \n",
    "        nt_1 = self.nt_1\n",
    "        nt = f*nt_1 + i\n",
    "        nt = torch.mean(self.ln_n(nt), [0, 1], keepdim=True)\n",
    "        self.nt_1 = nt.detach()\n",
    "        \n",
    "        ht = o*(ct/nt) # torch.Size([4, 8, 16])\n",
    "        ht = torch.mean(self.ln_h(ht), [0, 1], keepdim=True)\n",
    "        self.ht_1 = ht.detach()\n",
    "        # end sLSTM\n",
    "        \n",
    "        slstm_out = self.GN(ht)\n",
    "        \n",
    "        left = self.left_linear(slstm_out)\n",
    "        right = F.gelu(self.right_linear(slstm_out))\n",
    "        \n",
    "        out = self.ln_out(left*right)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import processing\n",
    "train_dataloader, test_dataloader = processing.get_train_test_dataloaders('..\\\\dataset\\\\np_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.xlstm\n",
    "import train \n",
    "xlstm_dict = train.get_xlstm_dict()\n",
    "model = models.xlstm.xLSTM(xlstm_dict)\n",
    "for batch_idx, (src, trg, metadata) in enumerate(train_dataloader):\n",
    "    output = model(src.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = src.float()\n",
    "x = x.unsqueeze(1)\n",
    "x = model.ln(x)\n",
    "\n",
    "x_conv = F.silu( model.drop(model.conv( x.transpose(1, 2) ).transpose(1, 2) ) )\n",
    "\n",
    "batch_size = x.size(0)  # Get dynamic batch size from input\n",
    "\n",
    "if model.mt_1.size(0) != batch_size:\n",
    "    # Adjust self.mt_1 to match current batch size.\n",
    "    model.mt_1 = model.mt_1[:batch_size]\n",
    "\n",
    "# start sLSTM\n",
    "ht_1 = model.ht_1\n",
    "\n",
    "i = torch.exp(model.ln_i( model.i_gate(x_conv) + model.ri_gate(ht_1) ) )\n",
    "f = torch.exp( model.ln_f(model.f_gate(x_conv) + model.rf_gate(ht_1) ) )\n",
    "\n",
    "m = torch.max(torch.log(f)+model.mt_1[:, 0, :].unsqueeze(1), torch.log(i))\n",
    "i = torch.exp(torch.log(i) - m)\n",
    "f = torch.exp(torch.log(f) + model.mt_1[:, 0, :].unsqueeze(1)-m)\n",
    "model.mt_1 = m.detach()\n",
    "\n",
    "o = torch.sigmoid( model.ln_o(model.o_gate(x) + model.ro_gate(ht_1) ) )\n",
    "z = torch.tanh( model.ln_z(model.z_gate(x) + model.rz_gate(ht_1) ) )\n",
    "\n",
    "ct_1 = model.ct_1\n",
    "ct = f*ct_1 + i*z\n",
    "ct = torch.mean(model.ln_c(ct), [0, 1], keepdim=True)\n",
    "model.ct_1 = ct.detach()\n",
    "\n",
    "nt_1 = model.nt_1\n",
    "nt = f*nt_1 + i\n",
    "nt = torch.mean(model.ln_n(nt), [0, 1], keepdim=True)\n",
    "model.nt_1 = nt.detach()\n",
    "\n",
    "ht = o*(ct/nt) # torch.Size([4, 8, 16])\n",
    "ht = torch.mean(model.ln_h(ht), [0, 1], keepdim=True)\n",
    "model.ht_1 = ht.detach()\n",
    "# end sLSTM\n",
    "\n",
    "slstm_out = model.GN(ht)\n",
    "\n",
    "left = model.left_linear(slstm_out)\n",
    "right = F.gelu(model.right_linear(slstm_out))\n",
    "\n",
    "out = model.ln_out(left*right)\n",
    "out = model.proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 128]) \n",
      " torch.Size([3, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([3, 1, 128]) \n",
      " torch.Size([3, 1, 128]) \n",
      " torch.Size([3, 1, 128]) \n",
      " torch.Size([3, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([1, 1, 128]) \n",
      " torch.Size([1, 1, 170]) \n",
      " torch.Size([1, 1, 170]) \n",
      " torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,'\\n',\n",
    "      x_conv.shape,'\\n',\n",
    "      ht_1.shape,'\\n',\n",
    "      i.shape,'\\n',\n",
    "      f.shape,'\\n',\n",
    "      o.shape,'\\n',\n",
    "      z.shape,'\\n',\n",
    "      ct_1.shape,'\\n',\n",
    "      ct.shape,'\\n',\n",
    "      nt_1.shape,'\\n',\n",
    "      ht.shape,'\\n',\n",
    "      slstm_out.shape,'\\n',\n",
    "      left.shape,'\\n',\n",
    "      right.shape,'\\n',\n",
    "      out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(layers=['s', 'm', 's', 'm'],\n",
       "          n_embd=128,\n",
       "          depth=4,\n",
       "          factor=2,\n",
       "          vocab_size=835,\n",
       "          block_len=128,\n",
       "          device='cpu',\n",
       "          metadata_dims=namespace(composer=8),\n",
       "          dropout=0.01,\n",
       "          epochs=200,\n",
       "          eval_interval=100,\n",
       "          save_interval=500,\n",
       "          learning_rate=0.1,\n",
       "          eval_iters=200,\n",
       "          test_ratio=0.2,\n",
       "          batch_size=8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlstm_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
