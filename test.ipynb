{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import processing\n",
    "import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Get_Ready_for_This.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Here_I_Go.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Let_the_Beat_Control_Your_Body.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Let_the_Beat_Control_Your_Body.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Let_the_Beat_Control_Your_Body.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Let_the_Beat_Control_Your_Body.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Maximum_Overdrive.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Maximum_Overdrive.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_Limit.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_Limit.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_Limit.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_Limits.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_Limit_extended_.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_One.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\No_One.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\The_Real_Thing.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\The_Real_Thing.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\The_Real_Thing.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Tribal_Dance_edit_.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Tribal_Dance_edit_.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Tribal_Dance_edit_.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Twilight_Zone.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Twilight_Zone.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Twilight_Zone.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Twilight_Zone.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Workaholic.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\2_Unlimited\\Workaholic.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Andra_tutto_bene_58_.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Andra_tutto_bene_58_.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Chiuditi_nel_cesso.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Chiuditi_nel_cesso.mid]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Draco\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [F:\\GitHub\\dataset\\midi_dataset\\883\\Come_mai_feat._Fiorello_.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Come_mai_feat._Fiorello_.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Come_mai_feat._Fiorello_.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Con_un_deca.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Con_un_deca.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Cumuni.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Dimmi_perche_remix_.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Gli_anni_96_.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Gli_anni_96_.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Grazie_mille.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Grazie_mille.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Hanno_ucciso_luomo_ragno.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Hanno_ucciso_luomo_ragno.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Hanno_ucciso_luomo_ragno.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Innamorare_tanto.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Io_ci_saro.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Io_ci_saro.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\La_dura_legge_del_gol.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\La_regola_dellamico.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Le_luci_di_Natale.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nella_notte.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nella_notte.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nella_notte.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nella_notte_Molella_remix_.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nessun_rimpianto.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nessun_rimpianto.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Non_mi_arrendo.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Non_mi_arrendo.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Non_ti_passa_piu.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nord_Sud_Ovest_Est.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nord_Sud_Ovest_Est.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nord_Sud_Ovest_Est.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nord_Sud_Ovest_Est.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Nord_Sud_Ovest_Est.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Rotta_x_casa_di_Dio.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Sei_un_mito.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Sei_un_mito.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Sei_un_mito.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Sei_un_mito.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Senza_averti_qui.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Senza_averti_qui.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Senza_averti_qui.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Se_tornerai.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Se_tornerai.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Se_tornerai.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Tieni_il_tempo.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Tieni_il_tempo.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Tieni_il_tempo.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Ti_sento_vivere.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Tutto_cio_che_ho.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Una_canzone_damore.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Una_canzone_damore.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Una_canzone_damore.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Un_giorno_cosi.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Un_giorno_cosi.mid] [F:\\GitHub\\dataset\\midi_dataset\\883\\Viaggio_al_centro_del_mondo.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Andante,_Andante.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Angeleyes.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Another_Town,_Another_Train.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Chiquitita.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Chiquitita.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Chiquitita.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Chiquitita.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Chiquitita.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dance_While_the_Music_Still_Goes_on_.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dance_While_the_Music_Still_Goes_on_.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dance_While_the_Music_Still_Goes_on_.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dance_While_the_Music_Still_Goes_on_.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.10.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.11.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.12.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.5.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.6.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.7.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.8.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.9.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Dancing_Queen.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Does_Your_Mother_Know.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Does_Your_Mother_Know.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Does_Your_Mother_Know.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Does_Your_Mother_Know.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Does_Your_Mother_Know.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Eagle.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.10.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.11.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.5.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.6.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.7.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.8.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.9.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Fernando.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Gimme_Gimme_Gimme.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Gimme_Gimme_Gimme.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Hamlet_III,_Part_2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Hasta_Manana.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Hey,_Hey_Helen.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Hey,_Hey_Helen.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Hole_In_Your_Soul.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Honey_Honey.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Honey_Honey.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Im_a_Marionette.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Im_a_Marionette.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Ive_Been_Waiting_For_You.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Am_the_City.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Do,_I_Do,_I_Do,_I_Do.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Do,_I_Do,_I_Do,_I_Do.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Do,_I_Do,_I_Do,_I_Do.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.5.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.6.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Have_a_Dream.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\I_Wonder_Departure_.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Just_Like_That_Full_Sax_version_1983_.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Kisses_of_Fire.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Kisses_of_Fire.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.5.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.6.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.7.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.8.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.9.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Knowing_Me,_Knowing_You.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Lay_All_Your_Love_on_Me.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Lay_All_Your_Love_on_Me.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Lay_All_Your_Love_on_Me.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Like_an_Angel_Passing_Through_My_Room.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Lovers_Live_a_Little_Longer_.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Mamma_Mia.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Mamma_Mia.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Mamma_Mia.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Mamma_Mia.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Mamma_Mia.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Medley.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Medley.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.1.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.10.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.11.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.2.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.3.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.4.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.5.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.6.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.7.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.8.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.9.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Money,_Money,_Money.mid] [F:\\GitHub\\dataset\\midi_dataset\\ABBA\\Move_On.mid]"
     ]
    }
   ],
   "source": [
    "processing.preprocess_midi_files('F:\\\\GitHub\\\\dataset\\\\midi_dataset', 'F:\\\\GitHub\\\\dataset\\\\np_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = processing.get_train_test_dataloaders('F:\\\\GitHub\\\\dataset\\\\np_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Average Loss: 0.0061\n",
      "Epoch [1/200], Validation Loss: 0.0050\n",
      "Epoch [2/200], Average Loss: 0.0047\n",
      "Epoch [2/200], Validation Loss: 0.0044\n",
      "Epoch [3/200], Average Loss: 0.0044\n",
      "Epoch [3/200], Validation Loss: 0.0043\n",
      "Epoch [4/200], Average Loss: 0.0041\n",
      "Epoch [4/200], Validation Loss: 0.0042\n",
      "Epoch [5/200], Average Loss: 0.0042\n",
      "Epoch [5/200], Validation Loss: 0.0041\n",
      "Epoch [6/200], Average Loss: 0.0041\n",
      "Epoch [6/200], Validation Loss: 0.0041\n",
      "Epoch [7/200], Average Loss: 0.0041\n",
      "Epoch [7/200], Validation Loss: 0.0042\n",
      "Epoch [8/200], Average Loss: 0.0041\n",
      "Epoch [8/200], Validation Loss: 0.0042\n",
      "Epoch [9/200], Average Loss: 0.0040\n",
      "Epoch [9/200], Validation Loss: 0.0041\n",
      "Epoch [10/200], Average Loss: 0.0041\n",
      "Epoch [10/200], Validation Loss: 0.0042\n",
      "Epoch [11/200], Average Loss: 0.0040\n",
      "Epoch [11/200], Validation Loss: 0.0041\n",
      "Epoch [12/200], Average Loss: 0.0042\n",
      "Epoch [12/200], Validation Loss: 0.0041\n",
      "Epoch [13/200], Average Loss: 0.0041\n",
      "Epoch [13/200], Validation Loss: 0.0041\n",
      "Epoch [14/200], Average Loss: 0.0041\n",
      "Epoch [14/200], Validation Loss: 0.0042\n",
      "Epoch [15/200], Average Loss: 0.0040\n",
      "Epoch [15/200], Validation Loss: 0.0042\n",
      "Epoch [16/200], Average Loss: 0.0040\n",
      "Epoch [16/200], Validation Loss: 0.0043\n",
      "Epoch [17/200], Average Loss: 0.0041\n",
      "Epoch [17/200], Validation Loss: 0.0043\n",
      "Epoch [18/200], Average Loss: 0.0041\n",
      "Epoch [18/200], Validation Loss: 0.0044\n",
      "Epoch [19/200], Average Loss: 0.0040\n",
      "Epoch [19/200], Validation Loss: 0.0042\n",
      "Epoch [20/200], Average Loss: 0.0041\n",
      "Epoch [20/200], Validation Loss: 0.0041\n",
      "Epoch [21/200], Average Loss: 0.0040\n",
      "Epoch [21/200], Validation Loss: 0.0041\n",
      "Epoch [22/200], Average Loss: 0.0041\n",
      "Epoch [22/200], Validation Loss: 0.0041\n",
      "Epoch [23/200], Average Loss: 0.0041\n",
      "Epoch [23/200], Validation Loss: 0.0040\n",
      "Epoch [24/200], Average Loss: 0.0040\n",
      "Epoch [24/200], Validation Loss: 0.0042\n",
      "Epoch [25/200], Average Loss: 0.0040\n",
      "Epoch [25/200], Validation Loss: 0.0041\n",
      "Epoch [26/200], Average Loss: 0.0040\n",
      "Epoch [26/200], Validation Loss: 0.0043\n",
      "Epoch [27/200], Average Loss: 0.0040\n",
      "Epoch [27/200], Validation Loss: 0.0041\n",
      "Epoch [28/200], Average Loss: 0.0041\n",
      "Epoch [28/200], Validation Loss: 0.0041\n",
      "Epoch [29/200], Average Loss: 0.0040\n",
      "Epoch [29/200], Validation Loss: 0.0039\n",
      "Epoch [30/200], Average Loss: 0.0040\n",
      "Epoch [30/200], Validation Loss: 0.0043\n",
      "Epoch [31/200], Average Loss: 0.0040\n",
      "Epoch [31/200], Validation Loss: 0.0041\n",
      "Epoch [32/200], Average Loss: 0.0040\n",
      "Epoch [32/200], Validation Loss: 0.0042\n",
      "Epoch [33/200], Average Loss: 0.0040\n",
      "Epoch [33/200], Validation Loss: 0.0043\n",
      "Epoch [34/200], Average Loss: 0.0041\n",
      "Epoch [34/200], Validation Loss: 0.0043\n",
      "Epoch [35/200], Average Loss: 0.0040\n",
      "Epoch [35/200], Validation Loss: 0.0040\n",
      "Epoch [36/200], Average Loss: 0.0040\n",
      "Epoch [36/200], Validation Loss: 0.0041\n",
      "Epoch [37/200], Average Loss: 0.0039\n",
      "Epoch [37/200], Validation Loss: 0.0042\n",
      "Epoch [38/200], Average Loss: 0.0041\n",
      "Epoch [38/200], Validation Loss: 0.0043\n",
      "Epoch [39/200], Average Loss: 0.0040\n",
      "Epoch [39/200], Validation Loss: 0.0041\n",
      "Epoch [40/200], Average Loss: 0.0040\n",
      "Epoch [40/200], Validation Loss: 0.0042\n",
      "Epoch [41/200], Average Loss: 0.0041\n",
      "Epoch [41/200], Validation Loss: 0.0042\n",
      "Epoch [42/200], Average Loss: 0.0040\n",
      "Epoch [42/200], Validation Loss: 0.0042\n",
      "Epoch [43/200], Average Loss: 0.0039\n",
      "Epoch [43/200], Validation Loss: 0.0042\n",
      "Epoch [44/200], Average Loss: 0.0040\n",
      "Epoch [44/200], Validation Loss: 0.0041\n",
      "Epoch [45/200], Average Loss: 0.0041\n",
      "Epoch [45/200], Validation Loss: 0.0042\n",
      "Epoch [46/200], Average Loss: 0.0040\n",
      "Epoch [46/200], Validation Loss: 0.0042\n",
      "Epoch [47/200], Average Loss: 0.0040\n",
      "Epoch [47/200], Validation Loss: 0.0042\n",
      "Epoch [48/200], Average Loss: 0.0040\n",
      "Epoch [48/200], Validation Loss: 0.0042\n",
      "Epoch [49/200], Average Loss: 0.0041\n",
      "Epoch [49/200], Validation Loss: 0.0041\n",
      "Epoch [50/200], Average Loss: 0.0040\n",
      "Epoch [50/200], Validation Loss: 0.0041\n",
      "Epoch [51/200], Average Loss: 0.0040\n",
      "Epoch [51/200], Validation Loss: 0.0042\n",
      "Epoch [52/200], Average Loss: 0.0041\n",
      "Epoch [52/200], Validation Loss: 0.0039\n",
      "Epoch [53/200], Average Loss: 0.0040\n",
      "Epoch [53/200], Validation Loss: 0.0041\n",
      "Epoch [54/200], Average Loss: 0.0040\n",
      "Epoch [54/200], Validation Loss: 0.0041\n",
      "Epoch [55/200], Average Loss: 0.0041\n",
      "Epoch [55/200], Validation Loss: 0.0043\n",
      "Epoch [56/200], Average Loss: 0.0040\n",
      "Epoch [56/200], Validation Loss: 0.0041\n",
      "Epoch [57/200], Average Loss: 0.0040\n",
      "Epoch [57/200], Validation Loss: 0.0041\n",
      "Epoch [58/200], Average Loss: 0.0040\n",
      "Epoch [58/200], Validation Loss: 0.0042\n",
      "Epoch [59/200], Average Loss: 0.0040\n",
      "Epoch [59/200], Validation Loss: 0.0040\n",
      "Epoch [60/200], Average Loss: 0.0040\n",
      "Epoch [60/200], Validation Loss: 0.0043\n",
      "Epoch [61/200], Average Loss: 0.0040\n",
      "Epoch [61/200], Validation Loss: 0.0044\n",
      "Epoch [62/200], Average Loss: 0.0040\n",
      "Epoch [62/200], Validation Loss: 0.0042\n",
      "Epoch [63/200], Average Loss: 0.0040\n",
      "Epoch [63/200], Validation Loss: 0.0041\n",
      "Epoch [64/200], Average Loss: 0.0040\n",
      "Epoch [64/200], Validation Loss: 0.0040\n",
      "Epoch [65/200], Average Loss: 0.0041\n",
      "Epoch [65/200], Validation Loss: 0.0041\n",
      "Epoch [66/200], Average Loss: 0.0039\n",
      "Epoch [66/200], Validation Loss: 0.0040\n",
      "Epoch [67/200], Average Loss: 0.0041\n",
      "Epoch [67/200], Validation Loss: 0.0043\n",
      "Epoch [68/200], Average Loss: 0.0041\n",
      "Epoch [68/200], Validation Loss: 0.0043\n",
      "Epoch [69/200], Average Loss: 0.0040\n",
      "Epoch [69/200], Validation Loss: 0.0042\n",
      "Epoch [70/200], Average Loss: 0.0040\n",
      "Epoch [70/200], Validation Loss: 0.0040\n",
      "Epoch [71/200], Average Loss: 0.0040\n",
      "Epoch [71/200], Validation Loss: 0.0042\n",
      "Epoch [72/200], Average Loss: 0.0040\n",
      "Epoch [72/200], Validation Loss: 0.0042\n",
      "Epoch [73/200], Average Loss: 0.0040\n",
      "Epoch [73/200], Validation Loss: 0.0044\n",
      "Epoch [74/200], Average Loss: 0.0041\n",
      "Epoch [74/200], Validation Loss: 0.0041\n",
      "Epoch [75/200], Average Loss: 0.0040\n",
      "Epoch [75/200], Validation Loss: 0.0041\n",
      "Epoch [76/200], Average Loss: 0.0040\n",
      "Epoch [76/200], Validation Loss: 0.0043\n",
      "Epoch [77/200], Average Loss: 0.0040\n",
      "Epoch [77/200], Validation Loss: 0.0039\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m sequence[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m sequence[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# loss = criterion(output, target)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m target_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\models\\model_transformer.py:113\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([x, temp])\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (B, T, C)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)    \u001b[38;5;66;03m# Shape: (B, T, C)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# Shape: (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\models\\model_transformer.py:74\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 74\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\models\\model_transformer.py:42\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\models\\model_transformer.py:42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\models\\model_transformer.py:24\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# compute attention scores (\"affinities\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m C\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('F:\\\\GitHub\\\\dataset\\\\midi_dataset\\\\tokenizations.json', 'r') as f:\n",
    "    tokenizations = json.load(f)\n",
    "METADATA_VOCAB_SIZE = tokenizations['VOCAB_SIZE']\n",
    "model = models.Transformer(VOCAB_SIZE, N_EMBD, N_LAYER, N_HEAD, BLOCK_SIZE, DROPOUT, DEVICE)\n",
    "model.to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (sequence, metadata) in enumerate(train_dataloader):\n",
    "        input = sequence[:, :-1].to(DEVICE)\n",
    "        target = sequence[:, 1:].to(DEVICE)\n",
    "        output = model(input)\n",
    "        # loss = criterion(output, target)\n",
    "        target_one_hot = torch.zeros_like(output)\n",
    "        target_one_hot.scatter_(-1, target.unsqueeze(-1), 1.0)  # Place 1 at the correct index\n",
    "        loss = criterion(output, target_one_hot)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Validation loop (optional)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sequence, metadata in test_dataloader:\n",
    "            input = sequence[:, :-1].to(DEVICE)\n",
    "            target = sequence[:, 1:].to(DEVICE)\n",
    "            output = model(input)\n",
    "            target_one_hot = torch.zeros_like(output)\n",
    "            target_one_hot.scatter_(-1, target.unsqueeze(-1), 1.0)  # Place 1 at the correct index\n",
    "            loss = criterion(output, target_one_hot)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = processing.SequenceDataset(\"F:\\\\GitHub\\\\dataset\\\\np_dataset\")\n",
    "for sequence, metadata in test_dataloader:\n",
    "    input = sequence[:, :-1].to(DEVICE)\n",
    "    target = sequence[:, 1:].to(DEVICE)\n",
    "    output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.7259, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([[[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]],\n",
      "\n",
      "        [[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]],\n",
      "\n",
      "        [[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]],\n",
      "\n",
      "        [[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]],\n",
      "\n",
      "        [[657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         ...,\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657],\n",
      "         [657, 657, 657, 657, 657, 657]]], device='cuda:0')\n",
      "tensor([[[  68,  146,  657,  260, 1082, 1483],\n",
      "         [  42,  143,  657,  260, 1082, 1483],\n",
      "         [  44,  207,  657,  275, 1087, 1483],\n",
      "         ...,\n",
      "         [  44,  207,  657,  272, 1087, 1483],\n",
      "         [  56,  215,  657,  263, 1144, 1483],\n",
      "         [  68,  146,  657,  260, 1082, 1483]],\n",
      "\n",
      "        [[  53,  233,  657,  305, 1084, 1483],\n",
      "         [  48,  231,  657,  257, 1057, 1483],\n",
      "         [  53,  222,  657,  289, 1084, 1483],\n",
      "         ...,\n",
      "         [  51,  213,  677,  281, 1084, 1483],\n",
      "         [  49,  226,  657,  281, 1082, 1483],\n",
      "         [  46,  214,  657,  309, 1084, 1483]],\n",
      "\n",
      "        [[  52,  198,  660,  260, 1090, 1330],\n",
      "         [  52,  238,  660,  260, 1090, 1330],\n",
      "         [  52,  198,  660,  260, 1090, 1330],\n",
      "         ...,\n",
      "         [  52,  198,  660,  260, 1090, 1330],\n",
      "         [  50,  238,  660,  260, 1090, 1330],\n",
      "         [  50,  198,  660,  260, 1090, 1330]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  55,  233,  657,  257, 1131, 1185],\n",
      "         [  83,  218,  657,  257, 1081, 1185],\n",
      "         [  83,  230,  657,  257, 1130, 1185],\n",
      "         ...,\n",
      "         [  46,  203,  657,  257, 1131, 1185],\n",
      "         [  71,  218,  657,  257, 1057, 1185],\n",
      "         [  63,  218,  657,  257, 1081, 1185]],\n",
      "\n",
      "        [[  66,  144,  657,  257, 1057, 1483],\n",
      "         [  54,  165,  657,  257, 1057, 1483],\n",
      "         [  82,  220,  681,  277, 1058, 1483],\n",
      "         ...,\n",
      "         [  54,  187,  657,  257, 1057, 1483],\n",
      "         [  66,  145,  681,  257, 1057, 1483],\n",
      "         [  58,  218,  657,  277, 1058, 1483]],\n",
      "\n",
      "        [[  67,  228,  657,  259, 1144, 1326],\n",
      "         [  55,  220,  657,  262, 1087, 1326],\n",
      "         [  43,  243,  657,  259, 1096, 1326],\n",
      "         ...,\n",
      "         [  41,  238,  657,  258, 1082, 1326],\n",
      "         [  79,  159,  657,  258, 1082, 1326],\n",
      "         [  79,  159,  660,  258, 1082, 1326]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(output[0][1][0][657])\n",
    "print(output.argmax(-1))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7324, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1485 artists>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flattened_tensor = input.flatten()\n",
    "bins = VOCAB_SIZE\n",
    "hist = torch.histc(flattened_tensor.int(), bins=bins, min=0, max=VOCAB_SIZE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(bins), hist.cpu().int().numpy(), width=1, align='center', color='blue', edgecolor='black')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
