{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b851914a-e5a8-4353-b26e-0e0895130641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "import train\n",
    "from train import new_model\n",
    "import torch\n",
    "import configs.paths as paths\n",
    "import argparse\n",
    "import os\n",
    "from collections import Counter\n",
    "from generate import generate\n",
    "\n",
    "data_root = \"/home/s203861/midi-classical-music/np_data/data\"\n",
    "model_name = \"transformer\"\n",
    "\n",
    "if model_name == \"mamba\":\n",
    "    model = new_model(\"mamba\")\n",
    "    model.load_state_dict(torch.load(cc.config.models.mamba))\n",
    "    model.to('cuda')\n",
    "if model_name == \"xlstm\":\n",
    "    model = new_model(\"xlstm\")\n",
    "    model.load_state_dict(torch.load(cc.config.models.xlstm))\n",
    "    model.to('cuda')\n",
    "if model_name == \"transformer\":\n",
    "    model = new_model(\"transformer\")\n",
    "    model.load_state_dict(torch.load(cc.config.models.transformer))\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96a5caf-e49f-43f0-9962-bda53ff6bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per output: 0.038017 seconds: total time 3.8016529083251953\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loader = processing.DatasetLoader(data_root)\n",
    "dataloader = loader.get_dataloader_full()\n",
    "\n",
    "num_outputs = 100\n",
    "inference_times = []\n",
    "model.eval()\n",
    "\n",
    "idx = 0\n",
    "for src, trg, meta in dataloader:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_outputs):\n",
    "        with torch.no_grad():\n",
    "            model(src, meta)\n",
    "        idx += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_output = total_time / num_outputs\n",
    "    \n",
    "    inference_times.append(avg_time_per_output)\n",
    "    \n",
    "    break\n",
    "\n",
    "# Print the average inference time for 100 outputs\n",
    "print(f\"Average inference time per output: {avg_time_per_output:.6f} seconds: total time {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831a3cbb-24fa-4b41-bc90-05cd9b6b33b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414781952"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47372776-7659-460e-bb74-5cd2dfb453d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'inference_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceParams\n\u001b[32m      2\u001b[39m model._decoding_cache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_decoding_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_params\u001b[49m = InferenceParams(\n\u001b[32m      4\u001b[39m     max_seqlen=cc.config.values.block_len+\u001b[32m6\u001b[39m,\n\u001b[32m      5\u001b[39m     max_batch_size=\u001b[38;5;28mlen\u001b[39m(src),\n\u001b[32m      6\u001b[39m     seqlen_offset=\u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m     10\u001b[39m     conv_state, ssm_state = layer.allocate_inference_cache(batch_size=\u001b[38;5;28mlen\u001b[39m(src), max_seqlen=cc.config.values.block_len+\u001b[32m6\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'inference_params'"
     ]
    }
   ],
   "source": [
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "model._decoding_cache.inference_params = InferenceParams(\n",
    "    max_seqlen=cc.config.values.block_len+6,\n",
    "    max_batch_size=len(src),\n",
    "    seqlen_offset=1\n",
    ")\n",
    "\n",
    "for layer in self.layers:\n",
    "    conv_state, ssm_state = layer.allocate_inference_cache(batch_size=len(src), max_seqlen=cc.config.values.block_len+6)\n",
    "    inference_params.key_value_memory_dict[layer.layer_idx] = (conv_state, ssm_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e021aa51-7efa-4d3b-bec0-405dcb35b84c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mamba2.forward() got an unexpected keyword argument 'position_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m src, trg, meta \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/mamba_ssm/utils/generation.py:222\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(input_ids, model, max_length, top_k, top_p, min_p, temperature, repetition_penalty, eos_token_id, teacher_outputs, vocab_size, cg, enable_timing, output_scores, streamer)\u001b[39m\n\u001b[32m    220\u001b[39m sequences_cat = input_ids\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop(sequences[-\u001b[32m1\u001b[39m], inference_params):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     logits = \u001b[43mget_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_scores:\n\u001b[32m    224\u001b[39m         scores.append(logits.clone())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/mamba_ssm/utils/generation.py:185\u001b[39m, in \u001b[36mdecode.<locals>.get_logits\u001b[39m\u001b[34m(input_ids, inference_params)\u001b[39m\n\u001b[32m    183\u001b[39m     position_ids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cg \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decoding:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_last_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.logits.squeeze(dim=\u001b[32m1\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m     logits = model._decoding_cache.run(\n\u001b[32m    193\u001b[39m         input_ids, position_ids, inference_params.seqlen_offset\n\u001b[32m    194\u001b[39m     ).squeeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Mamba2.forward() got an unexpected keyword argument 'position_ids'"
     ]
    }
   ],
   "source": [
    "from mamba_ssm.utils.generation import decode\n",
    "loader = processing.DatasetLoader(data_root)\n",
    "dataloader = loader.get_dataloader_full()\n",
    "for src, trg, meta in dataloader:\n",
    "    break\n",
    "decode(src,model,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8058f31b-7d0d-4952-a09c-4a9de857a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba2\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "model = Mamba2(\n",
    "    d_model=1024, \n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "model._decoding_cache = type(\"Cache\", (), {})()\n",
    "model._decoding_cache.inference_params = InferenceParams(\n",
    "            max_seqlen=cc.config.values.block_len+6,\n",
    "            max_batch_size=len(src),\n",
    "            seqlen_offset=1\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
