{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "\n",
    "loader = processing.DatasetLoader('..\\\\..\\\\dataset\\\\np_dataset')\n",
    "# train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "random_sample = loader.get_random_sample('train')\n",
    "src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTM(\n",
       "  (token_embedding_table): Embedding(835, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0): sLSTMblock(\n",
       "      (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(16,), stride=(1,), padding=(15,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=128, out_features=170, bias=True)\n",
       "      (right_linear): Linear(in_features=128, out_features=170, bias=True)\n",
       "      (ln_out): LayerNorm((170,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=170, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): mLSTMblock(\n",
       "      (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (right): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(12,), stride=(1,), padding=(11,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (f_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (o_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (ln_c): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (ln_proj): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): sLSTMblock(\n",
       "      (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(16,), stride=(1,), padding=(15,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=128, out_features=170, bias=True)\n",
       "      (right_linear): Linear(in_features=128, out_features=170, bias=True)\n",
       "      (ln_out): LayerNorm((170,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=170, out_features=128, bias=True)\n",
       "    )\n",
       "    (3): mLSTMblock(\n",
       "      (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (right): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(12,), stride=(1,), padding=(11,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (f_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (o_gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (ln_c): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (ln_proj): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=835, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import new_model\n",
    "import torch\n",
    "type = 'xlstm'\n",
    "model_name = 'loss_43.30_time_2025-03-15-18-46-18.pth'\n",
    "model = new_model(type)\n",
    "# model.load_state_dict(torch.load(f'../pretrained/{type}/{model_name}'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 128])\n",
      "tensor([281, 381, 487, 833,  46, 236, 281, 491, 833,  48, 217, 281, 457, 833,\n",
      "         46, 236, 281, 359, 487, 833,  42, 236, 281, 457, 833,  56, 236, 305,\n",
      "        379, 487, 833,  44, 236, 281, 491, 833,  48, 217, 281, 457, 833,  42,\n",
      "        236, 281, 359, 457, 833,  56, 236, 305, 487, 833,  48, 245, 281, 379,\n",
      "        457, 833,  46, 236, 281, 491, 833,  46, 236, 281, 359, 457, 833,  48,\n",
      "        217, 281, 379, 457, 833,  46, 236, 281, 487, 833,  46, 236, 281, 491,\n",
      "        833,  46, 236, 281, 359, 487, 833,  58, 236, 281, 379, 487, 833,  46,\n",
      "        236, 281, 491, 833,  48, 217, 281, 457, 833,  58, 236, 281, 359, 487,\n",
      "        833,  42, 236, 281, 457, 833,  46, 236, 281, 379, 487, 833,  48, 217,\n",
      "        281, 457, 833,  46, 236, 281, 491, 833,  42, 236, 281, 359, 457, 833,\n",
      "         46, 236, 281, 487, 833,  49, 236, 281, 379, 491, 833,  48, 245, 281,\n",
      "        457, 833,  61, 236, 281, 487, 833,  46, 236, 281, 359, 457, 833,  61,\n",
      "        236, 281, 487, 833,  53, 236, 281, 379, 487, 833,  48, 217, 281, 457,\n",
      "        833,  41, 236, 281, 491, 833,  53, 236, 281, 359, 487, 833,  48, 217,\n",
      "        281, 379, 457, 833,  70, 236, 281, 487, 833,  51, 236, 281, 491, 833,\n",
      "         63, 236, 281, 359, 487, 833,  42, 236, 281, 457, 833,  49, 236, 281,\n",
      "        379, 491, 833,  68, 236, 281, 487, 833,  48, 217, 281, 457, 833,  42,\n",
      "        236, 281, 359, 457, 833,  61, 236, 281, 487, 833,  46, 236, 281, 379,\n",
      "        491, 833,  65, 236, 281, 487, 833,  48, 245, 281, 457, 833,  46, 236,\n",
      "        281, 359, 457, 833,  58, 236, 281, 487, 833,  45, 236, 281, 379, 491,\n",
      "        833,  64, 236, 281, 487, 833,  48, 217, 281, 457, 833,  57, 236, 281,\n",
      "        359, 487, 833,  48, 217, 281, 379, 457, 833,  44, 236, 281, 491, 833,\n",
      "         63, 236, 281, 487, 833,  42, 236, 281, 359, 457, 833,  56, 236, 281,\n",
      "        487, 833,  41, 236, 281, 379, 491, 833,  48, 217, 281, 457, 833,  60,\n",
      "        236, 281, 487, 833,  42, 236, 281, 359, 457, 833,  53, 236, 281, 487,\n",
      "        833,  48, 245, 281, 379, 457, 833,  59, 236, 281, 487, 833,  40, 236,\n",
      "        281, 491, 833,  52, 236, 281, 359, 487, 833,  46, 236, 281, 457, 833,\n",
      "         39, 236, 281, 379, 491, 833,  58, 236, 281, 487, 833,  48, 217, 281,\n",
      "        457, 833,  51, 236, 281, 359, 487, 833,  66, 236, 305, 379, 487, 833,\n",
      "         42, 236, 281, 491, 833,  61, 236, 305, 487, 833,  54, 236, 305, 487,\n",
      "        833,  48, 217, 281, 457, 833,  66, 236, 305, 359, 487, 833,  42, 236,\n",
      "        281, 457, 833,  61, 236, 305, 487, 833,  54, 236, 305, 487, 833,  48,\n",
      "        217, 281, 379, 457, 833,  44, 236, 281, 491, 833,  42, 236, 281, 359,\n",
      "        457, 833,  68, 236, 281, 379, 487, 833,  44, 236, 281, 491, 833,  63,\n",
      "        236, 281, 487, 833,  48, 245, 281, 457, 833,  56, 236, 281, 487, 833,\n",
      "         46, 236, 281, 359, 457, 833,  56, 236, 281, 487, 833,  63, 236, 281,\n",
      "        487, 833,  68, 236, 281, 487, 833,  66], device='cuda:0')\n",
      "tensor([[ 69,  33,  56,  47,  12,  64,  69,  33,  87,  38, 115, 126,  46, 123,\n",
      "          38,  76,  69,  67,  10,  47,  52,  82,  69,  46, 108,  98, 119,  69,\n",
      "         103, 115, 101,  38, 115,  69,  46,  17,   5, 110,  88,  46,  56,  95,\n",
      "         102,  69,  67,  36, 104,  38,  35,  69,  95,  15,  12,  35,  11,  46,\n",
      "         103, 123,  52,   4,  49,  79,  87,   5,  64,  69,  79,  73, 108,  12,\n",
      "          84,  88,  46, 103,  84,  38,  64,   7,  79,  35,  12,  64,  69,  46,\n",
      "         119,   1,  91,  69, 126, 108, 101,  12,  65,  69,  46, 103, 101,  52,\n",
      "          64,  69,  46,  35,   5,  35, 126,  46,  96,  49,  65,  69, 126,  10,\n",
      "          47,  38,  23,  69,  46,  50,   5,  64,  69,  79, 115, 101,  38,  84,\n",
      "          88,  41, 108,  69,  64,  69,  79,  17,  49, 102,  69,  79,  73,  56,\n",
      "          12,  91,  69,  33,  35,  12, 126,  69,  46, 103, 113,   5, 115,  27,\n",
      "         125, 100,   1,  69,  69,  46, 111,   5,  91,  69,  46,  10, 126,  52,\n",
      "          50,  69, 125,  53,   5,  81,  69,  46, 103,  47,   5,  84,  88,  41,\n",
      "         108,   1,  69,  69,  46,  38,  30,   4,   3,  46,  92,  47,   5,  84,\n",
      "         126,  46,  35,  81,  38,  74,  69, 125,   9,   5,  23,  69,  46,  22,\n",
      "          38,  46,  69,  46,  73, 101,  12,   0,  69,  46, 126,   1,  50,  69,\n",
      "          46, 103, 113,   1,   5,  69,  79,  96,  12,  41, 126,  46, 126,  38,\n",
      "          34,  69,  46, 121, 100,  12,  56,  69,  79, 101,   5,  64,  69,  79,\n",
      "          76, 113,  52,  24,  69,  46,  35,   1,  84,  76,  79,  56,   5,  76,\n",
      "          69,  46,  10, 100,   1, 117,  69, 125,  35,   5,   9,  69,  46, 103,\n",
      "         113,  52,  72,  69,  46, 101,   5,  84,  88,  46,  87,  49,  65,  69,\n",
      "          46,  10,  47,  12,  84,  88,  33,  11,  56,  38,   9,  69, 125,  87,\n",
      "           1,  35,  69,  46,  35,   5,   0,  69,  46,  73, 100,  38, 119,  69,\n",
      "          79,  24,  12,  69,  69,  46, 103, 113,   1, 115, 107, 125, 126,   1,\n",
      "          88,  69,  46,  92,   5,  31,  69,  46,  73,  56,  52,  70,  69,  79,\n",
      "         101,   5, 115,  11,  46,  11, 100,  52, 102,  69, 125,  35,   1,  35,\n",
      "          69,  46,  38,   5,  82,  69,  46,  92, 101,   5,  91,  69,  46, 126,\n",
      "          12,  31,  69,  46, 103, 113,  12,  65,  69,  46,  35,  38, 119, 126,\n",
      "          46,  96,  30, 109,  69, 126, 108,  47,   1,  81,  69, 123,  76,  17,\n",
      "           5, 108,  69,  67,  35,   5,  89,  64,  73, 101,  95,  28,  69,  73,\n",
      "         111,   5,  35, 126,  46,  96,  38,  97,  69, 103,  10, 101,   1, 108,\n",
      "          69,  46,  50,   5,  89,  69, 103,  85,   5,  28,  69, 123, 111,   5,\n",
      "          35, 126,  46,  35,  30,  52,  22,  77, 125,  87,   1,  37,  69,  46,\n",
      "          58,  41,  38,   5,  69,  79, 103,  47,  38, 115,  69,  46, 113,   1,\n",
      "          69,  69,  46,  35,  95,  84, 120, 126,  96,  38,  99,  48,  46, 111,\n",
      "          49,  91,  69,  46,  10,  98,  52,  99,  69, 125,  56,   5,   3,  69,\n",
      "          46,  35,  49,  75,  69,  46,  85,  38]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "output = model(src.unsqueeze(0), meta.unsqueeze(0))\n",
    "print(trg)\n",
    "print(output.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 34, 257, 357, 357, 357, 833, 256, 256, 357, 357, 833, 833,  54, 257,\n",
       "         257, 357, 357, 833, 256, 257, 357, 357, 357, 833, 256, 257, 357, 357,\n",
       "         357, 833, 256, 257, 357, 357, 357, 833, 257, 257, 357, 357, 357, 833,\n",
       "         256, 257, 357, 357, 357, 833, 256, 257, 357, 357, 357, 833, 256, 257,\n",
       "         357, 357, 357, 833, 256, 257, 357, 357, 833, 833,  54, 257, 257, 357,\n",
       "         357, 833, 256, 257, 257, 357, 833, 833,  54, 257, 257, 357, 357, 833,\n",
       "          50, 257, 257, 357, 357, 833,  54, 257, 257, 357, 357, 833,  34, 257,\n",
       "         357, 357, 357, 833,  34, 257, 357, 357, 357, 833, 257, 256, 257, 357,\n",
       "         357, 833, 257, 257, 257, 357, 357, 833, 256, 257, 357, 357, 357, 833,\n",
       "         257, 257]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "softmaxed_tensor = F.softmax(output, dim=-1)\n",
    "softmaxed_tensor.argmax(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
