{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "\n",
    "loader = processing.DatasetLoader('E:\\\\GitHub\\\\dataset\\\\classical piano np\\\\classical piano midi')\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "# random_sample = loader.get_random_sample('train')\n",
    "# random_sample\n",
    "for src, trg, meta in train_dataloader:\n",
    "    break\n",
    "# src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTM(\n",
       "  (token_embedding_table): Embedding(17090, 512)\n",
       "  (metadata_embedding_table): Embedding(154, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0): sLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(64,), stride=(1,), padding=(63,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (right_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (ln_out): LayerNorm((682,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=682, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): mLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (right): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(51,), stride=(1,), padding=(50,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (f_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (o_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (ln_c): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (ln_proj): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): sLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(64,), stride=(1,), padding=(63,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (right_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (ln_out): LayerNorm((682,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=682, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): mLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (right): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(51,), stride=(1,), padding=(50,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (f_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (o_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (ln_c): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (ln_proj): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=17090, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import new_model\n",
    "import torch\n",
    "type = 'xlstm'\n",
    "model_name = 'loss_0.54_time_2025-05-25-00-57-35.pth'\n",
    "model = new_model(type)\n",
    "# model.load_state_dict(torch.load(f'../pretrained/{type}/{model_name}'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7acf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     28\u001b[39m             \u001b[38;5;66;03m# If you want metadata, sample or set as zeros:\u001b[39;00m\n\u001b[32m     29\u001b[39m             \u001b[38;5;66;03m# e.g., meta_ids = torch.cat([meta_ids, new_meta], dim=1)[:, -context_len:]\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generated\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m new_seq = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblock_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, context_len, token_ids, meta_ids, num_tokens, device)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tokens):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(_, end=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, seq_len, vocab_size)\u001b[39;00m\n\u001b[32m     10\u001b[39m     filtered_logits = train.filtered_logit(token_ids, logits)\n\u001b[32m     11\u001b[39m     logits_last = filtered_logits[:, -\u001b[32m1\u001b[39m, :]       \u001b[38;5;66;03m# (1, vocab_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\scripts\\..\\models\\xlstm\\model_xlstm.py:38\u001b[39m, in \u001b[36mxLSTM.forward\u001b[39m\u001b[34m(self, token_ids, meta_ids)\u001b[39m\n\u001b[32m     36\u001b[39m x_original = x.clone()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m      x = \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m + x_original\n\u001b[32m     39\u001b[39m x = \u001b[38;5;28mself\u001b[39m.output_layer(x)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Go from n_embd to vocab_size and then normalize\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\scripts\\..\\models\\xlstm\\sLSTMblock.py:61\u001b[39m, in \u001b[36msLSTMblock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mself\u001b[39m.ht_1 = torch.zeros(batch_size, \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_embd, device=x.device)\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mself\u001b[39m.mt_1 = torch.zeros(batch_size, \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_embd, device=x.device)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m x_conv = F.silu(\u001b[38;5;28mself\u001b[39m.drop(\u001b[38;5;28mself\u001b[39m.conv(x.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)))\n\u001b[32m     65\u001b[39m ht_1 = \u001b[38;5;28mself\u001b[39m.ht_1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\functional.py:2910\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2902\u001b[39m         layer_norm,\n\u001b[32m   2903\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2908\u001b[39m         eps=eps,\n\u001b[32m   2909\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "def generate(model, context_len, token_ids, meta_ids, num_tokens=1000, device='cpu'):\n",
    "    model.eval()\n",
    "    generated = token_ids.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for _ in range(num_tokens):\n",
    "                print(_, end=\"\\r\")\n",
    "                logits = model(token_ids, meta_ids)  # (1, seq_len, vocab_size)\n",
    "                filtered_logits = train.filtered_logit(token_ids, logits)\n",
    "                logits_last = filtered_logits[:, -1, :]       # (1, vocab_size)\n",
    "                probs = torch.softmax(logits_last, dim=-1)  # (1, vocab_size)\n",
    "\n",
    "                # Get top-k\n",
    "                topk_probs, topk_indices = torch.topk(probs, 5, dim=-1)  # Each: shape (1, 5)\n",
    "                # Normalize to sum to 1\n",
    "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # Sample\n",
    "                next_token_idx = torch.multinomial(topk_probs, num_samples=1)  # (1, 1), value in [0..4]\n",
    "                next_token = topk_indices.gather(1, next_token_idx)\n",
    "\n",
    "                generated.append(next_token.item())\n",
    "\n",
    "                token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "                token_ids = token_ids[:, -context_len:]\n",
    "\n",
    "            # If you want metadata, sample or set as zeros:\n",
    "            # e.g., meta_ids = torch.cat([meta_ids, new_meta], dim=1)[:, -context_len:]\n",
    "\n",
    "    return generated\n",
    "\n",
    "new_seq = generate(model, cc.config.values.block_len, src[1].unsqueeze(0), meta[1].unsqueeze(0), num_tokens=400, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37dd9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_notes_old = processing.decode(src[0])\n",
    "processing.note_to_midi(decoded_notes_old, \"comparison.mid\")\n",
    "\n",
    "decoded_notes_new = processing.decode(new_seq)\n",
    "processing.note_to_midi(decoded_notes_new, \"generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e23ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MIDI_note(pitch=54, time_start=0.020833333333333332, time_end=0.0625, dynamic=94, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=0.020833333333333332, time_end=0.0625, dynamic=81, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=74, time_start=0.20833333333333334, time_end=0.33333333333333337, dynamic=120, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=66, time_start=0.22916666666666669, time_end=0.33333333333333337, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=0.22916666666666669, time_end=0.2916666666666667, dynamic=117, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=78, time_start=0.22916666666666669, time_end=0.3125, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=0.22916666666666669, time_end=0.4375, dynamic=121, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=35, time_start=0.22916666666666669, time_end=0.2916666666666667, dynamic=126, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=37, time_start=0.25, time_end=0.2916666666666667, dynamic=121, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=66, time_start=0.2708333333333333, time_end=0.29166666666666663, dynamic=76, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=0.8333333333333333, time_end=1.2916666666666665, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=78, time_start=0.8333333333333333, time_end=1.2916666666666665, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=0.8333333333333333, time_end=1.2916666666666665, dynamic=126, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=1.125, time_end=1.2916666666666667, dynamic=90, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=42, time_start=1.125, time_end=1.2916666666666667, dynamic=102, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=1.3125, time_end=1.7916666666666665, dynamic=103, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=1.5208333333333333, time_end=1.7708333333333333, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=1.5208333333333333, time_end=1.7708333333333333, dynamic=98, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=1.5208333333333333, time_end=1.7708333333333333, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=1.5208333333333333, time_end=1.7708333333333333, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=86, time_start=1.75, time_end=1.875, dynamic=105, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=74, time_start=1.75, time_end=2.7291666666666665, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=1.75, time_end=2.75, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=1.9375, time_end=2.0, dynamic=97, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=42, time_start=1.9375, time_end=2.0, dynamic=83, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=2.1458333333333335, time_end=2.729166666666667, dynamic=87, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=2.3125, time_end=2.7291666666666665, dynamic=108, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=2.3125, time_end=2.75, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=2.3125, time_end=2.75, dynamic=94, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=2.3125, time_end=2.7291666666666665, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=70, time_start=2.5416666666666665, time_end=2.7291666666666665, dynamic=108, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=82, time_start=2.5625, time_end=3.4166666666666665, dynamic=117, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=76, time_start=2.5625, time_end=3.4166666666666665, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=79, time_start=2.5625, time_end=3.4166666666666665, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=52, time_start=2.5625, time_end=3.4166666666666665, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=2.75, time_end=2.8333333333333335, dynamic=89, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=2.75, time_end=2.9166666666666665, dynamic=102, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=41, time_start=2.9583333333333335, time_end=3.3958333333333335, dynamic=81, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=2.979166666666667, time_end=3.3958333333333335, dynamic=75, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=3.1250000000000004, time_end=3.395833333333334, dynamic=101, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=3.145833333333334, time_end=4.041666666666667, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=85, time_start=3.145833333333334, time_end=3.333333333333334, dynamic=110, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=73, time_start=3.208333333333334, time_end=3.395833333333334, dynamic=105, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=3.2291666666666674, time_end=3.395833333333334, dynamic=94, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=3.312500000000001, time_end=3.3958333333333344, dynamic=96, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=78, time_start=3.3333333333333344, time_end=4.166666666666668, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=3.3333333333333344, time_end=4.125000000000001, dynamic=105, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=3.5208333333333344, time_end=3.562500000000001, dynamic=90, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=3.5208333333333344, time_end=3.5833333333333344, dynamic=82, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=42, time_start=3.541666666666668, time_end=3.5833333333333344, dynamic=79, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=3.729166666666668, time_end=3.7708333333333344, dynamic=93, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=3.916666666666668, time_end=4.083333333333335, dynamic=94, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=79, time_start=3.916666666666668, time_end=4.062500000000001, dynamic=82, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=4.125000000000001, time_end=4.520833333333334, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=62, time_start=4.333333333333334, time_end=4.729166666666667, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=4.625000000000001, time_end=5.020833333333334, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=4.916666666666668, time_end=5.312500000000001, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=5.208333333333335, time_end=5.604166666666668, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=37, time_start=5.500000000000002, time_end=5.895833333333335, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=42, time_start=5.791666666666669, time_end=6.187500000000002, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=5.812500000000002, time_end=6.270833333333335, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=6.041666666666669, time_end=6.437500000000002, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=6.250000000000002, time_end=6.437500000000002, dynamic=105, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=6.250000000000002, time_end=6.395833333333335, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=6.437500000000002, time_end=6.833333333333335, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=86, time_start=6.625000000000002, time_end=6.791666666666669, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=6.791666666666669, time_end=7.041666666666669, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=85, time_start=6.958333333333336, time_end=7.958333333333336, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=7.166666666666669, time_end=7.604166666666669, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=7.166666666666669, time_end=8.166666666666668, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=86, time_start=7.166666666666669, time_end=8.187500000000002, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=86, time_start=7.395833333333336, time_end=7.979166666666669, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=7.583333333333336, time_end=8.47916666666667, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=7.583333333333336, time_end=8.16666666666667, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=7.583333333333336, time_end=8.16666666666667, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=7.583333333333336, time_end=8.16666666666667, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=7.812500000000003, time_end=8.708333333333336, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=7.812500000000003, time_end=8.66666666666667, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=41, time_start=7.97916666666667, time_end=8.395833333333336, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=8.145833333333336, time_end=8.72916666666667, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=52, time_start=8.312500000000002, time_end=9.104166666666668, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=8.312500000000002, time_end=9.104166666666668, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.312500000000002, time_end=8.708333333333336, dynamic=122, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=8.312500000000002, time_end=8.708333333333336, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.312500000000002, time_end=8.708333333333336, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=85, time_start=8.375000000000002, time_end=8.770833333333336, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.375000000000002, time_end=8.416666666666668, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.375000000000002, time_end=8.812500000000002, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.375000000000002, time_end=8.812500000000002, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.375000000000002, time_end=8.812500000000002, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.375000000000002, time_end=8.812500000000002, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.375000000000002, time_end=8.812500000000002, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.375000000000002, time_end=9.270833333333336, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.375000000000002, time_end=9.270833333333336, dynamic=123, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=8.375000000000002, time_end=9.208333333333336, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.604166666666668, time_end=9.395833333333334, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=8.833333333333334, time_end=9.625, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=8.833333333333334, time_end=9.625, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=8.833333333333334, time_end=9.729166666666668, dynamic=109, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=8.833333333333334, time_end=9.729166666666668, dynamic=113, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=37, time_start=8.833333333333334, time_end=9.729166666666668, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=8.833333333333334, time_end=9.833333333333334, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.0625, time_end=9.958333333333334, dynamic=109, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.0625, time_end=9.958333333333334, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=64, time_start=9.354166666666666, time_end=9.75, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.791666666666666, dynamic=117, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.354166666666666, time_end=9.9375, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.9375, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=37, time_start=9.354166666666666, time_end=9.9375, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.354166666666666, time_end=9.9375, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.9375, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.9375, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.354166666666666, time_end=9.9375, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=9.354166666666666, time_end=9.9375, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.9375, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=83, time_start=9.354166666666666, time_end=10.354166666666666, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=10.333333333333332, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=9.354166666666666, time_end=10.354166666666666, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=86, time_start=9.354166666666666, time_end=10.354166666666666, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=85, time_start=9.354166666666666, time_end=10.333333333333332, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=41, time_start=9.354166666666666, time_end=10.354166666666666, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=9.354166666666666, time_end=10.208333333333332, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=41, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=52, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=10.145833333333332, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=10.25, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=54, time_start=9.354166666666666, time_end=9.75, dynamic=111, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=9.354166666666666, time_end=9.75, dynamic=116, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.354166666666666, time_end=9.75, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=47, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=118, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=50, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=115, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=124, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=40, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=114, channel=0, tempo=240.0),\n",
       " MIDI_note(pitch=71, time_start=9.583333333333332, time_end=9.979166666666666, dynamic=124, channel=0, tempo=240.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_notes_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6784f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58, 238, 299, 961, 473,  58, 239, 334, 961, 473,  51, 239, 334, 961,\n",
       "         491,  27, 252, 355, 961, 482,  63, 185, 295, 360, 961, 484,  55, 248,\n",
       "         355, 357, 961, 484,  58, 226, 355, 367, 961, 484,  63, 244, 355, 961,\n",
       "         482,  55, 240, 295, 374, 961, 482,  58, 220, 295, 357, 961, 482,  63,\n",
       "         179, 295, 360, 961, 482,  55, 218, 320, 395, 961, 484,  63, 235, 341,\n",
       "         357, 961, 473,  70, 239, 330, 961, 473,  75, 239, 327, 961, 484,  55,\n",
       "         237, 341, 961, 473,  67, 239, 327, 961, 484,  58, 246, 337, 961, 482,\n",
       "          58, 219, 316, 961, 530,  63, 252, 281, 961, 482,  63, 219, 316, 961,\n",
       "         530,  63, 252, 295, 399, 961, 491,  27, 252, 292, 357, 961, 482,  63,\n",
       "         195, 278, 378, 961, 482,  58, 173, 274, 357, 961, 482,  55, 215, 274,\n",
       "         961, 484,  38, 239, 355, 961, 484,  53, 241, 355, 371, 961, 482,  53,\n",
       "         195, 299, 364, 961, 530,  62, 252, 334, 357, 961, 484,  50, 247, 348,\n",
       "         961, 491,  26, 252, 355, 961, 482,  58, 229, 295, 961, 484,  58, 233,\n",
       "         355, 961, 482,  63, 238, 295, 961, 484,  63, 239, 355, 371, 961, 482,\n",
       "          63, 190, 295, 385, 961, 482,  58, 208, 295, 357, 961, 482,  53, 221,\n",
       "         295, 961, 482,  53, 227, 302, 395, 961, 484,  63, 235, 341, 360, 961,\n",
       "         484,  58, 223, 334, 357, 961, 530,  70, 252, 299, 961, 482,  58, 248,\n",
       "         295, 961, 484,  53, 248, 330, 961, 482,  63, 203, 295, 961, 482,  63,\n",
       "         176, 299, 395, 961, 482,  58, 242, 295, 360, 961, 530,  62, 252, 274,\n",
       "         357, 961, 491,  26, 252, 295, 961, 482,  53, 230, 295, 961, 484,  36,\n",
       "         230, 355, 371, 961, 530,  63, 252, 355, 364, 961, 484,  51, 233, 355,\n",
       "         961, 484,  56, 244, 355, 367, 961, 482,  51, 255, 299, 357, 961, 473,\n",
       "          75, 213, 355, 961, 473,  68, 241, 355, 360, 961, 473,  72, 223, 355,\n",
       "         357, 961, 482,  56, 163, 295, 961, 491,  24, 252, 355, 961, 484,  48,\n",
       "         237, 355, 961, 482,  60, 183, 299, 961, 484,  63, 227, 355, 364, 961,\n",
       "         482,  51, 224, 295, 388, 961, 484,  56, 251, 355, 360, 961, 482,  56,\n",
       "         235, 295, 357, 961, 482,  60, 248, 295, 961, 482,  51, 228, 320, 395,\n",
       "         961, 484,  63, 254, 341, 357, 961, 482,  56, 243, 316, 360, 961, 482,\n",
       "          60, 227, 313, 357, 961, 530,  63, 252, 260, 378, 961, 491,  24, 252,\n",
       "         295, 961, 530,  67, 252, 274, 357, 961, 484,  41, 235, 355, 367, 961,\n",
       "         482,  60, 176, 278, 364, 961, 530,  68, 252, 295, 360, 961, 482,  56,\n",
       "         181, 274, 357, 961, 482,  51, 210, 271, 961, 484,  63, 244, 355, 357,\n",
       "         961, 486,  63, 255, 355, 357, 961, 486,  58, 244, 355, 961, 486,  63,\n",
       "         244, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355, 961, 486,\n",
       "          63, 255, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355, 961,\n",
       "         486,  63, 255, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355,\n",
       "         961, 486,  63, 255, 355, 961, 486,  63]], device='cuda:0')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(input, meta[idx].unsqueeze(0))\n",
    "filtered_logits = train.filtered_logit(input, logits)\n",
    "                \n",
    "input = filtered_logits[0].argmax(-1).unsqueeze(0)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6738f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  6.3134,  6.9035,  6.0828,  5.9433,  7.1644,  5.7620,  6.5757,\n",
       "         6.4643,  6.3919,  6.1867,  6.8310,  6.3284,  6.4244,  6.5372,  6.4623,\n",
       "         8.7010,  4.6767,  7.4200,  6.7567,  7.9998,  8.1419,  9.5407,  5.6227,\n",
       "         6.7564,  4.8045,  3.8351, 10.1992,  7.5654,  8.9391,  5.8445,  7.6321,\n",
       "         6.8122,  8.1876,  7.4191,  5.6611,  5.7854,  5.1118,  7.0683,  6.2696,\n",
       "         8.7257,  7.1330,  5.1107,  5.9827,  5.5587, 10.4906,  8.5093,  5.9634,\n",
       "         8.5304,  5.3487,  6.7490,  7.1678,  6.1069,  7.6541, 10.4402,  9.7946,\n",
       "         8.7629,  6.1342, 20.6024,  4.4450,  8.1050,  9.6003,  8.3159,  5.5958,\n",
       "         6.3124,  6.3709,  5.4957,  5.2022,  5.1336,  7.5839,  9.9579,  7.8871,\n",
       "         5.0124,  9.3024,  7.5886,  6.8797,  6.0038,  5.4065,  8.6481,  8.2093,\n",
       "         7.5058,  8.4233,  5.0589,  7.3166,  4.6714,  7.5599, 11.5220,  7.1528,\n",
       "         5.1387,  8.8976,  6.4618,  8.3402,  7.3777,  5.5708,  6.6366,  5.1106,\n",
       "         6.0629,  7.6522,  7.5989,  5.7476,  6.0687,  6.7983,  7.2596,  7.2970,\n",
       "         6.0818,  6.0549,  6.4826,  5.9529,  7.6480,  7.2386,  6.4008,  6.6834,\n",
       "         6.0840,  6.6701,  7.0517,  7.2221,  6.7062,  6.4384,  6.0925,  6.5989,\n",
       "         7.0982,  6.8054,  6.0257,  7.3505,  7.3086,  6.3370,  6.2097,  6.1043],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_logits[0][0][:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "063ab185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.4743,\n",
      "         6.8243,  6.7807, 11.2046,  8.2917,  8.2001,  9.5565,  8.2000,  8.4255,\n",
      "         8.5841,  9.2970,  8.3535,  9.2126], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 6.8611,  5.2008,  5.6286,  7.7482,  7.3960,  6.3381,  6.2766,  5.8202,\n",
      "         9.0022,  6.0999,  5.8930, 13.3240,  5.5961], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "filtered_output[-1][-1].argmax(-1)\n",
    "print(filtered_output[-1][-1][350:370])\n",
    "print(filtered_output[-1][-1][950:970])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a450cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_distributions():\n",
    "    # Prepare output tensor\n",
    "    block_len = cc.config.values.block_len\n",
    "    device = cc.config.values.device\n",
    "    vocab_size = cc.vocab_size\n",
    "    distributions = torch.zeros(6, vocab_size, device=device)\n",
    "\n",
    "    # For each token index, fill in regions with 1 as per your logic.\n",
    "    start = [cc.start_idx[\"pitch\"],\n",
    "             cc.start_idx[\"dyn\"],\n",
    "             cc.start_idx[\"length\"],\n",
    "             cc.start_idx[\"time\"],\n",
    "             cc.start_idx[\"channel\"],\n",
    "             cc.start_idx[\"tempo\"]]\n",
    "    end   = [cc.start_idx[\"dyn\"] - 1,\n",
    "             cc.start_idx[\"length\"] - 1,\n",
    "             cc.start_idx[\"time\"] - 1,\n",
    "             cc.start_idx[\"channel\"] - 1,\n",
    "             cc.start_idx[\"tempo\"] - 1,\n",
    "             cc.vocab_size]   # block_len implies : to the end\n",
    "\n",
    "    for token in range(6):\n",
    "        if token == 0:\n",
    "            distributions[token, start[1]:end[1]] = 1\n",
    "        if token == 1:\n",
    "            distributions[token, start[2]:end[2]] = 1\n",
    "        if token == 2:\n",
    "            distributions[token, start[3]:end[3]] = 1\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 3:\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 4:\n",
    "            distributions[token, start[0]:end[0]] = 1\n",
    "        if token == 5:\n",
    "            distributions[token, start[4]:end[4]] = 1\n",
    "\n",
    "    return distributions\n",
    "\n",
    "\n",
    "def pick_distributions_by_prev_token(\n",
    "        input_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    boundaries = [\n",
    "                cc.start_idx['dyn'] - 1,\n",
    "                cc.start_idx['length'] - 1,\n",
    "                cc.start_idx['time'] - 1,\n",
    "                cc.start_idx['channel'] - 1,\n",
    "                cc.start_idx['tempo'] - 1]\n",
    "        \n",
    "    bins = torch.tensor(boundaries, device=input_tokens.device)\n",
    "        # Each token is assigned to a bucket 0..len(boundaries)\n",
    "        # For example, with bins=[10,20,30]: \n",
    "        #   token <10 → 0, 10<=token<20 → 1, 20<=token<30 → 2, 30<=token → 3\n",
    "    buckets = torch.bucketize(input_tokens, bins, right=False)\n",
    "    distributions = make_distributions()\n",
    "\n",
    "    # Ensure buckets is long (int64) and on the same device as distributions\n",
    "    buckets = buckets.long().to(distributions.device)\n",
    "\n",
    "    # Now use advanced indexing to get values\n",
    "    output = distributions[buckets]  # shape: [6, 963]\n",
    "\n",
    "    return output\n",
    "\n",
    "def cross_entropy_loss(input, output, target, reduction='mean'):\n",
    "    weights = pick_distributions_by_prev_token(input)\n",
    "    log_probs = F.log_softmax(output, dim=1)\n",
    "    loss = log_probs * weights \n",
    "    loss = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else: # 'none'\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027dc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0048, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_loss(pred, target):\n",
    "    # pred: (N, C), target: (N,)\n",
    "    expected = pred.argmax(-1)\n",
    "    probs = torch.log_softmax(pred, dim=1)\n",
    "    loss = -probs[torch.arange(pred.shape[0]), target].mean()\n",
    "    return loss\n",
    "model.to(\"cuda\")\n",
    "output = model(src, meta)\n",
    "output = output.reshape(-1, cc.vocab_size)\n",
    "trg = trg.view(-1)\n",
    "my_custom_loss(output, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec76a161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0704,  1.4107,  1.1998,  2.6456,  3.4005,  1.2786,  1.5597, -0.3986,\n",
       "         0.5784,  0.5274,  2.8592,  0.4845, -1.1300,  0.1472,  0.6268,  1.2091,\n",
       "         1.2610,  0.5960,  2.9577,  1.9989], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][800:820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9456863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0525, device='cuda:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1999a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8828, -1.6690, -2.4971, -4.0834, -2.2464, -3.8021, -1.7690,  9.0525,\n",
       "         6.0496,  7.2018], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][350:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4e9a1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43, device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg[-513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f04cd9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(883, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54995b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6896, 7.8589, 7.1193, 8.7035, 8.3538, 9.9633, 8.1413, 9.2453, 9.2601,\n",
       "        8.3956, 8.1151, 9.1026, 8.0156, 7.8010, 8.5638, 8.9311, 7.8991, 7.7118,\n",
       "        9.2435, 6.8082], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-513][30:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
