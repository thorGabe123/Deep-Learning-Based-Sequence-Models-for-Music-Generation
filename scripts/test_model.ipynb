{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2f4a292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "\n",
    "loader = processing.DatasetLoader('E:\\\\GitHub\\\\dataset\\\\np_dataset')\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "# random_sample = loader.get_random_sample('train')\n",
    "# random_sample\n",
    "for src, trg, meta in train_dataloader:\n",
    "    break\n",
    "# src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "863d8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTM(\n",
       "  (token_embedding_table): Embedding(963, 256)\n",
       "  (metadata_embedding_table): Embedding(154, 256)\n",
       "  (layers): ModuleList(\n",
       "    (0): sLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(32,), stride=(1,), padding=(31,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (right_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (ln_out): LayerNorm((341,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=341, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): mLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (right): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(25,), stride=(1,), padding=(24,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (f_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (o_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (ln_proj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): sLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(32,), stride=(1,), padding=(31,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (right_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (ln_out): LayerNorm((341,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=341, out_features=256, bias=True)\n",
       "    )\n",
       "    (3): mLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (right): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(25,), stride=(1,), padding=(24,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (f_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (o_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (ln_proj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): sLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(32,), stride=(1,), padding=(31,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (right_linear): Linear(in_features=256, out_features=341, bias=True)\n",
       "      (ln_out): LayerNorm((341,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=341, out_features=256, bias=True)\n",
       "    )\n",
       "    (5): mLSTMblock(\n",
       "      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (right): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(25,), stride=(1,), padding=(24,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (f_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (o_gate): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (ln_proj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=256, out_features=963, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import new_model\n",
    "import torch\n",
    "type = 'xlstm'\n",
    "model_name = 'loss_0.16_time_2025-05-12-18-38-40.pth'\n",
    "model = new_model(type)\n",
    "model.load_state_dict(torch.load(f'../pretrained/{type}/{model_name}'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ed7acf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3179, device='cuda:0')\n",
      "tensor([[57, 52, 77]], device='cuda:0')\n",
      "tensor(10.3356, device='cuda:0')\n",
      "tensor([[57, 75, 55]], device='cuda:0')\n",
      "tensor(7.1950, device='cuda:0')\n",
      "tensor([[75, 55, 77]], device='cuda:0')\n",
      "tensor(7.2664, device='cuda:0')\n",
      "tensor([[75, 79, 55]], device='cuda:0')\n",
      "tensor(7.0935, device='cuda:0')\n",
      "tensor([[55, 72, 62]], device='cuda:0')\n",
      "tensor(7.5046, device='cuda:0')\n",
      "tensor([[79, 76, 55]], device='cuda:0')\n",
      "tensor(10.1704, device='cuda:0')\n",
      "tensor([[52, 76, 62]], device='cuda:0')\n",
      "tensor(9.8979, device='cuda:0')\n",
      "tensor([[52, 57, 76]], device='cuda:0')\n",
      "tensor(10.2337, device='cuda:0')\n",
      "tensor([[57, 76, 55]], device='cuda:0')\n",
      "tensor(6.7142, device='cuda:0')\n",
      "tensor([[75, 43, 55]], device='cuda:0')\n",
      "tensor(7.1923, device='cuda:0')\n",
      "tensor([[55, 47, 76]], device='cuda:0')\n",
      "tensor(6.8988, device='cuda:0')\n",
      "tensor([[41, 43, 59]], device='cuda:0')\n",
      "tensor(6.9048, device='cuda:0')\n",
      "tensor([[62, 79, 76]], device='cuda:0')\n",
      "tensor(6.9097, device='cuda:0')\n",
      "tensor([[52, 77, 79]], device='cuda:0')\n",
      "79\r"
     ]
    }
   ],
   "source": [
    "import train\n",
    "def generate(model, context_len, token_ids, meta_ids, num_tokens=1000, device='cpu'):\n",
    "    model.eval()\n",
    "    generated = token_ids.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for _ in range(num_tokens):\n",
    "                print(_, end=\"\\r\")\n",
    "                logits = model(token_ids, meta_ids)  # (1, seq_len, vocab_size)\n",
    "                filtered_logits = train.filtered_logit(token_ids, logits)\n",
    "                logits_last = filtered_logits[:, -1, :]       # (1, vocab_size)\n",
    "                most_likely_token = logits_last[0].argmax(-1)\n",
    "                num_occurrences = (token_ids[0][-30:] == most_likely_token).sum().item()\n",
    "                logits_last[0][most_likely_token] -= 1 * num_occurrences\n",
    "                if most_likely_token < cc.start_idx[\"dyn\"]:\n",
    "                    for idx in range(128):\n",
    "                        num_occurrences = (token_ids[0][-30:] == idx).sum().item()\n",
    "                        logits_last[0][idx] -= 2 * num_occurrences\n",
    "                probs = torch.softmax(logits_last, dim=-1)  # (1, vocab_size)\n",
    "\n",
    "                # Get top-5\n",
    "                topk_probs, topk_indices = torch.topk(probs, 3, dim=-1)  # Each: shape (1, 5)\n",
    "                if most_likely_token < 128:\n",
    "                    print(logits_last[0][most_likely_token])\n",
    "                    print(topk_indices)\n",
    "                # Normalize to sum to 1\n",
    "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # Sample\n",
    "                next_token_idx = torch.multinomial(topk_probs, num_samples=1)  # (1, 1), value in [0..4]\n",
    "                next_token = topk_indices.gather(1, next_token_idx)\n",
    "\n",
    "                generated.append(next_token.item())\n",
    "\n",
    "                token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "                token_ids = token_ids[:, -context_len:]\n",
    "\n",
    "            # If you want metadata, sample or set as zeros:\n",
    "            # e.g., meta_ids = torch.cat([meta_ids, new_meta], dim=1)[:, -context_len:]\n",
    "\n",
    "    return generated\n",
    "\n",
    "new_seq = generate(model, cc.config.values.block_len, src[0].unsqueeze(0), meta[0].unsqueeze(0), num_tokens=80, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9698a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_notes_old = processing.decode(src[0])\n",
    "processing.note_to_midi(decoded_notes_old, \"comparison.mid\")\n",
    "\n",
    "decoded_notes_new = processing.decode(new_seq[-80:])\n",
    "processing.note_to_midi(decoded_notes_new, \"generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2ee46060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MIDI_note(pitch=51, time_start=0.42857142857142855, time_end=0.9285714285714286, dynamic=97, channel=48, tempo=210.0),\n",
       " MIDI_note(pitch=56, time_start=0.8571428571428571, time_end=0.9047619047619047, dynamic=99, channel=26, tempo=248.0),\n",
       " MIDI_note(pitch=76, time_start=0.8571428571428571, time_end=0.8974654377880183, dynamic=97, channel=48, tempo=210.0),\n",
       " MIDI_note(pitch=74, time_start=0.8571428571428571, time_end=0.8928571428571428, dynamic=115, channel=25, tempo=129.0),\n",
       " MIDI_note(pitch=71, time_start=0.8571428571428571, time_end=1.6710963455149501, dynamic=97, channel=0, tempo=248.0),\n",
       " MIDI_note(pitch=78, time_start=0.8571428571428571, time_end=1.3006912442396312, dynamic=94, channel=25, tempo=248.0),\n",
       " MIDI_note(pitch=61, time_start=0.8571428571428571, time_end=1.2805299539170507, dynamic=115, channel=33, tempo=248.0),\n",
       " MIDI_note(pitch=51, time_start=1.2200460829493087, time_end=1.2704493087557602, dynamic=97, channel=25, tempo=248.0),\n",
       " MIDI_note(pitch=56, time_start=1.2200460829493087, time_end=1.2502880184331797, dynamic=115, channel=0, tempo=210.0),\n",
       " MIDI_note(pitch=74, time_start=1.2200460829493087, time_end=1.2557603686635945, dynamic=78, channel=29, tempo=210.0),\n",
       " MIDI_note(pitch=54, time_start=1.2200460829493087, time_end=1.2557603686635945, dynamic=99, channel=33, tempo=248.0),\n",
       " MIDI_note(pitch=40, time_start=1.2200460829493087, time_end=1.26036866359447, dynamic=115, channel=0, tempo=141.0),\n",
       " MIDI_note(pitch=61, time_start=1.326429061672713, time_end=1.4150815439422166, dynamic=78, channel=25, tempo=248.0)]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_notes_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55e23ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "input = src[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6784f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58, 238, 299, 961, 473,  58, 239, 334, 961, 473,  51, 239, 334, 961,\n",
       "         491,  27, 252, 355, 961, 482,  63, 185, 295, 360, 961, 484,  55, 248,\n",
       "         355, 357, 961, 484,  58, 226, 355, 367, 961, 484,  63, 244, 355, 961,\n",
       "         482,  55, 240, 295, 374, 961, 482,  58, 220, 295, 357, 961, 482,  63,\n",
       "         179, 295, 360, 961, 482,  55, 218, 320, 395, 961, 484,  63, 235, 341,\n",
       "         357, 961, 473,  70, 239, 330, 961, 473,  75, 239, 327, 961, 484,  55,\n",
       "         237, 341, 961, 473,  67, 239, 327, 961, 484,  58, 246, 337, 961, 482,\n",
       "          58, 219, 316, 961, 530,  63, 252, 281, 961, 482,  63, 219, 316, 961,\n",
       "         530,  63, 252, 295, 399, 961, 491,  27, 252, 292, 357, 961, 482,  63,\n",
       "         195, 278, 378, 961, 482,  58, 173, 274, 357, 961, 482,  55, 215, 274,\n",
       "         961, 484,  38, 239, 355, 961, 484,  53, 241, 355, 371, 961, 482,  53,\n",
       "         195, 299, 364, 961, 530,  62, 252, 334, 357, 961, 484,  50, 247, 348,\n",
       "         961, 491,  26, 252, 355, 961, 482,  58, 229, 295, 961, 484,  58, 233,\n",
       "         355, 961, 482,  63, 238, 295, 961, 484,  63, 239, 355, 371, 961, 482,\n",
       "          63, 190, 295, 385, 961, 482,  58, 208, 295, 357, 961, 482,  53, 221,\n",
       "         295, 961, 482,  53, 227, 302, 395, 961, 484,  63, 235, 341, 360, 961,\n",
       "         484,  58, 223, 334, 357, 961, 530,  70, 252, 299, 961, 482,  58, 248,\n",
       "         295, 961, 484,  53, 248, 330, 961, 482,  63, 203, 295, 961, 482,  63,\n",
       "         176, 299, 395, 961, 482,  58, 242, 295, 360, 961, 530,  62, 252, 274,\n",
       "         357, 961, 491,  26, 252, 295, 961, 482,  53, 230, 295, 961, 484,  36,\n",
       "         230, 355, 371, 961, 530,  63, 252, 355, 364, 961, 484,  51, 233, 355,\n",
       "         961, 484,  56, 244, 355, 367, 961, 482,  51, 255, 299, 357, 961, 473,\n",
       "          75, 213, 355, 961, 473,  68, 241, 355, 360, 961, 473,  72, 223, 355,\n",
       "         357, 961, 482,  56, 163, 295, 961, 491,  24, 252, 355, 961, 484,  48,\n",
       "         237, 355, 961, 482,  60, 183, 299, 961, 484,  63, 227, 355, 364, 961,\n",
       "         482,  51, 224, 295, 388, 961, 484,  56, 251, 355, 360, 961, 482,  56,\n",
       "         235, 295, 357, 961, 482,  60, 248, 295, 961, 482,  51, 228, 320, 395,\n",
       "         961, 484,  63, 254, 341, 357, 961, 482,  56, 243, 316, 360, 961, 482,\n",
       "          60, 227, 313, 357, 961, 530,  63, 252, 260, 378, 961, 491,  24, 252,\n",
       "         295, 961, 530,  67, 252, 274, 357, 961, 484,  41, 235, 355, 367, 961,\n",
       "         482,  60, 176, 278, 364, 961, 530,  68, 252, 295, 360, 961, 482,  56,\n",
       "         181, 274, 357, 961, 482,  51, 210, 271, 961, 484,  63, 244, 355, 357,\n",
       "         961, 486,  63, 255, 355, 357, 961, 486,  58, 244, 355, 961, 486,  63,\n",
       "         244, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355, 961, 486,\n",
       "          63, 255, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355, 961,\n",
       "         486,  63, 255, 355, 961, 486,  63, 255, 355, 961, 486,  63, 255, 355,\n",
       "         961, 486,  63, 255, 355, 961, 486,  63]], device='cuda:0')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(input, meta[idx].unsqueeze(0))\n",
    "filtered_logits = train.filtered_logit(input, logits)\n",
    "                \n",
    "input = filtered_logits[0].argmax(-1).unsqueeze(0)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6738f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  6.3134,  6.9035,  6.0828,  5.9433,  7.1644,  5.7620,  6.5757,\n",
       "         6.4643,  6.3919,  6.1867,  6.8310,  6.3284,  6.4244,  6.5372,  6.4623,\n",
       "         8.7010,  4.6767,  7.4200,  6.7567,  7.9998,  8.1419,  9.5407,  5.6227,\n",
       "         6.7564,  4.8045,  3.8351, 10.1992,  7.5654,  8.9391,  5.8445,  7.6321,\n",
       "         6.8122,  8.1876,  7.4191,  5.6611,  5.7854,  5.1118,  7.0683,  6.2696,\n",
       "         8.7257,  7.1330,  5.1107,  5.9827,  5.5587, 10.4906,  8.5093,  5.9634,\n",
       "         8.5304,  5.3487,  6.7490,  7.1678,  6.1069,  7.6541, 10.4402,  9.7946,\n",
       "         8.7629,  6.1342, 20.6024,  4.4450,  8.1050,  9.6003,  8.3159,  5.5958,\n",
       "         6.3124,  6.3709,  5.4957,  5.2022,  5.1336,  7.5839,  9.9579,  7.8871,\n",
       "         5.0124,  9.3024,  7.5886,  6.8797,  6.0038,  5.4065,  8.6481,  8.2093,\n",
       "         7.5058,  8.4233,  5.0589,  7.3166,  4.6714,  7.5599, 11.5220,  7.1528,\n",
       "         5.1387,  8.8976,  6.4618,  8.3402,  7.3777,  5.5708,  6.6366,  5.1106,\n",
       "         6.0629,  7.6522,  7.5989,  5.7476,  6.0687,  6.7983,  7.2596,  7.2970,\n",
       "         6.0818,  6.0549,  6.4826,  5.9529,  7.6480,  7.2386,  6.4008,  6.6834,\n",
       "         6.0840,  6.6701,  7.0517,  7.2221,  6.7062,  6.4384,  6.0925,  6.5989,\n",
       "         7.0982,  6.8054,  6.0257,  7.3505,  7.3086,  6.3370,  6.2097,  6.1043],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_logits[0][0][:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "063ab185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.4743,\n",
      "         6.8243,  6.7807, 11.2046,  8.2917,  8.2001,  9.5565,  8.2000,  8.4255,\n",
      "         8.5841,  9.2970,  8.3535,  9.2126], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 6.8611,  5.2008,  5.6286,  7.7482,  7.3960,  6.3381,  6.2766,  5.8202,\n",
      "         9.0022,  6.0999,  5.8930, 13.3240,  5.5961], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "filtered_output[-1][-1].argmax(-1)\n",
    "print(filtered_output[-1][-1][350:370])\n",
    "print(filtered_output[-1][-1][950:970])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a450cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_distributions():\n",
    "    # Prepare output tensor\n",
    "    block_len = cc.config.values.block_len\n",
    "    device = cc.config.values.device\n",
    "    vocab_size = cc.vocab_size\n",
    "    distributions = torch.zeros(6, vocab_size, device=device)\n",
    "\n",
    "    # For each token index, fill in regions with 1 as per your logic.\n",
    "    start = [cc.start_idx[\"pitch\"],\n",
    "             cc.start_idx[\"dyn\"],\n",
    "             cc.start_idx[\"length\"],\n",
    "             cc.start_idx[\"time\"],\n",
    "             cc.start_idx[\"channel\"],\n",
    "             cc.start_idx[\"tempo\"]]\n",
    "    end   = [cc.start_idx[\"dyn\"] - 1,\n",
    "             cc.start_idx[\"length\"] - 1,\n",
    "             cc.start_idx[\"time\"] - 1,\n",
    "             cc.start_idx[\"channel\"] - 1,\n",
    "             cc.start_idx[\"tempo\"] - 1,\n",
    "             cc.vocab_size]   # block_len implies : to the end\n",
    "\n",
    "    for token in range(6):\n",
    "        if token == 0:\n",
    "            distributions[token, start[1]:end[1]] = 1\n",
    "        if token == 1:\n",
    "            distributions[token, start[2]:end[2]] = 1\n",
    "        if token == 2:\n",
    "            distributions[token, start[3]:end[3]] = 1\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 3:\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 4:\n",
    "            distributions[token, start[0]:end[0]] = 1\n",
    "        if token == 5:\n",
    "            distributions[token, start[4]:end[4]] = 1\n",
    "\n",
    "    return distributions\n",
    "\n",
    "\n",
    "def pick_distributions_by_prev_token(\n",
    "        input_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    boundaries = [\n",
    "                cc.start_idx['dyn'] - 1,\n",
    "                cc.start_idx['length'] - 1,\n",
    "                cc.start_idx['time'] - 1,\n",
    "                cc.start_idx['channel'] - 1,\n",
    "                cc.start_idx['tempo'] - 1]\n",
    "        \n",
    "    bins = torch.tensor(boundaries, device=input_tokens.device)\n",
    "        # Each token is assigned to a bucket 0..len(boundaries)\n",
    "        # For example, with bins=[10,20,30]: \n",
    "        #   token <10 → 0, 10<=token<20 → 1, 20<=token<30 → 2, 30<=token → 3\n",
    "    buckets = torch.bucketize(input_tokens, bins, right=False)\n",
    "    distributions = make_distributions()\n",
    "\n",
    "    # Ensure buckets is long (int64) and on the same device as distributions\n",
    "    buckets = buckets.long().to(distributions.device)\n",
    "\n",
    "    # Now use advanced indexing to get values\n",
    "    output = distributions[buckets]  # shape: [6, 963]\n",
    "\n",
    "    return output\n",
    "\n",
    "def cross_entropy_loss(input, output, target, reduction='mean'):\n",
    "    weights = pick_distributions_by_prev_token(input)\n",
    "    log_probs = F.log_softmax(output, dim=1)\n",
    "    loss = log_probs * weights \n",
    "    loss = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else: # 'none'\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027dc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0048, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_loss(pred, target):\n",
    "    # pred: (N, C), target: (N,)\n",
    "    expected = pred.argmax(-1)\n",
    "    probs = torch.log_softmax(pred, dim=1)\n",
    "    loss = -probs[torch.arange(pred.shape[0]), target].mean()\n",
    "    return loss\n",
    "model.to(\"cuda\")\n",
    "output = model(src, meta)\n",
    "output = output.reshape(-1, cc.vocab_size)\n",
    "trg = trg.view(-1)\n",
    "my_custom_loss(output, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec76a161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0704,  1.4107,  1.1998,  2.6456,  3.4005,  1.2786,  1.5597, -0.3986,\n",
       "         0.5784,  0.5274,  2.8592,  0.4845, -1.1300,  0.1472,  0.6268,  1.2091,\n",
       "         1.2610,  0.5960,  2.9577,  1.9989], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][800:820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9456863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0525, device='cuda:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1999a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8828, -1.6690, -2.4971, -4.0834, -2.2464, -3.8021, -1.7690,  9.0525,\n",
       "         6.0496,  7.2018], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][350:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4e9a1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43, device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg[-513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f04cd9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(883, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54995b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6896, 7.8589, 7.1193, 8.7035, 8.3538, 9.9633, 8.1413, 9.2453, 9.2601,\n",
       "        8.3956, 8.1151, 9.1026, 8.0156, 7.8010, 8.5638, 8.9311, 7.8991, 7.7118,\n",
       "        9.2435, 6.8082], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-513][30:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
