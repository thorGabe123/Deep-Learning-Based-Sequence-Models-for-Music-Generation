{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4a292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "\n",
    "loader = processing.DatasetLoader('E:\\\\GitHub\\\\dataset\\\\np_dataset')\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "# random_sample = loader.get_random_sample('train')\n",
    "# random_sample\n",
    "for src, trg, meta in train_dataloader:\n",
    "    break\n",
    "# src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863d8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTM(\n",
       "  (token_embedding_table): Embedding(963, 512)\n",
       "  (metadata_embedding_table): Embedding(154, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0): sLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(64,), stride=(1,), padding=(63,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (right_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (ln_out): LayerNorm((682,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=682, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): mLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (right): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(51,), stride=(1,), padding=(50,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (f_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (o_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (ln_c): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (ln_proj): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): sLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(64,), stride=(1,), padding=(63,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (right_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (ln_out): LayerNorm((682,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=682, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): mLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (right): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(51,), stride=(1,), padding=(50,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (f_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (o_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (ln_c): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (ln_proj): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): sLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(64,), stride=(1,), padding=(63,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.01, inplace=False)\n",
       "      (i_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (f_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (o_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (z_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ri_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rf_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ro_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (rz_gate): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=512, out_features=64, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ln_i): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_z): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_h): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (right_linear): Linear(in_features=512, out_features=682, bias=True)\n",
       "      (ln_out): LayerNorm((682,), eps=1e-05, elementwise_affine=True)\n",
       "      (proj): Linear(in_features=682, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): mLSTMblock(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (left): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (right): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (conv): CausalConv1D(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(51,), stride=(1,), padding=(50,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.11, inplace=False)\n",
       "      (lskip): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (wq): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wk): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (wv): BlockDiagonal(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Linear(in_features=1024, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropq): Dropout(p=0.005, inplace=False)\n",
       "      (dropk): Dropout(p=0.005, inplace=False)\n",
       "      (dropv): Dropout(p=0.005, inplace=False)\n",
       "      (i_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (f_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (o_gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (ln_c): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_n): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lnf): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lno): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lni): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (GN): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop2): Dropout(p=0.01, inplace=False)\n",
       "      (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (ln_proj): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=963, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import new_model\n",
    "import torch\n",
    "type = 'xlstm'\n",
    "model_name = 'loss_0.01_time_2025-04-26-09-44-39.pth'\n",
    "model = new_model(type)\n",
    "model.load_state_dict(torch.load(f'../pretrained/{type}/{model_name}'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7acf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "def generate(model, context_len, token_ids, meta_ids, num_tokens=1000, device='cpu'):\n",
    "    model.eval()\n",
    "    generated = token_ids.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for _ in range(num_tokens):\n",
    "                print(_, end=\"\\r\")\n",
    "                logits = model(token_ids, meta_ids)  # (1, seq_len, vocab_size)\n",
    "                logits_last = logits[:, -1, :]       # (1, vocab_size)\n",
    "                probs = torch.softmax(logits_last, dim=-1)  # (1, vocab_size)\n",
    "\n",
    "                # Get top-5\n",
    "                topk_probs, topk_indices = torch.topk(probs, 2, dim=-1)  # Each: shape (1, 5)\n",
    "\n",
    "                # Normalize to sum to 1\n",
    "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # Sample\n",
    "                next_token_idx = torch.multinomial(topk_probs, num_samples=1)  # (1, 1), value in [0..4]\n",
    "                next_token = topk_indices.gather(1, next_token_idx)\n",
    "\n",
    "                generated.append(next_token.item())\n",
    "\n",
    "                token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "                token_ids = token_ids[:, -context_len:]\n",
    "\n",
    "            # If you want metadata, sample or set as zeros:\n",
    "            # e.g., meta_ids = torch.cat([meta_ids, new_meta], dim=1)[:, -context_len:]\n",
    "\n",
    "    return generated\n",
    "\n",
    "new_seq = generate(model, cc.config.values.block_len, src[0].unsqueeze(0), meta[0].unsqueeze(0), num_tokens=500, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_notes_old = processing.decode(src[0])\n",
    "processing.note_to_midi(decoded_notes_old, \"comparison.mid\")\n",
    "\n",
    "decoded_notes_new = processing.decode(new_seq)\n",
    "processing.note_to_midi(decoded_notes_new, \"generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a450cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_distributions():\n",
    "    # Prepare output tensor\n",
    "    block_len = cc.config.values.block_len\n",
    "    device = cc.config.values.device\n",
    "    vocab_size = cc.vocab_size\n",
    "    distributions = torch.zeros(6, vocab_size, device=device)\n",
    "\n",
    "    # For each token index, fill in regions with 1 as per your logic.\n",
    "    start = [cc.start_idx[\"pitch\"],\n",
    "             cc.start_idx[\"dyn\"],\n",
    "             cc.start_idx[\"length\"],\n",
    "             cc.start_idx[\"time\"],\n",
    "             cc.start_idx[\"channel\"],\n",
    "             cc.start_idx[\"tempo\"]]\n",
    "    end   = [cc.start_idx[\"dyn\"] - 1,\n",
    "             cc.start_idx[\"length\"] - 1,\n",
    "             cc.start_idx[\"time\"] - 1,\n",
    "             cc.start_idx[\"channel\"] - 1,\n",
    "             cc.start_idx[\"tempo\"] - 1,\n",
    "             cc.vocab_size]   # block_len implies : to the end\n",
    "\n",
    "    for token in range(6):\n",
    "        if token == 0:\n",
    "            distributions[token, start[1]:end[1]] = 1\n",
    "        if token == 1:\n",
    "            distributions[token, start[2]:end[2]] = 1\n",
    "        if token == 2:\n",
    "            distributions[token, start[3]:end[3]] = 1\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 3:\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 4:\n",
    "            distributions[token, start[0]:end[0]] = 1\n",
    "        if token == 5:\n",
    "            distributions[token, start[4]:end[4]] = 1\n",
    "\n",
    "    return distributions\n",
    "\n",
    "\n",
    "def pick_distributions_by_prev_token(\n",
    "        input_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    boundaries = [\n",
    "                cc.start_idx['dyn'] - 1,\n",
    "                cc.start_idx['length'] - 1,\n",
    "                cc.start_idx['time'] - 1,\n",
    "                cc.start_idx['channel'] - 1,\n",
    "                cc.start_idx['tempo'] - 1]\n",
    "        \n",
    "    bins = torch.tensor(boundaries, device=input_tokens.device)\n",
    "        # Each token is assigned to a bucket 0..len(boundaries)\n",
    "        # For example, with bins=[10,20,30]: \n",
    "        #   token <10 → 0, 10<=token<20 → 1, 20<=token<30 → 2, 30<=token → 3\n",
    "    buckets = torch.bucketize(input_tokens, bins, right=False)\n",
    "    distributions = make_distributions()\n",
    "\n",
    "    # Ensure buckets is long (int64) and on the same device as distributions\n",
    "    buckets = buckets.long().to(distributions.device)\n",
    "\n",
    "    # Now use advanced indexing to get values\n",
    "    output = distributions[buckets]  # shape: [6, 963]\n",
    "\n",
    "    return output\n",
    "\n",
    "def cross_entropy_loss(input, output, target, reduction='mean'):\n",
    "    weights = pick_distributions_by_prev_token(input)\n",
    "    log_probs = F.log_softmax(output, dim=1)\n",
    "    loss = log_probs * weights \n",
    "    loss = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else: # 'none'\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a027dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(src, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec76a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pick_distributions_by_prev_token(src)\n",
    "log_probs = F.log_softmax(output, dim=1)\n",
    "# loss = log_probs * weights \n",
    "# loss = -log_probs.gather(1, trg.unsqueeze(1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n",
      "Epoch [1/1000], Step [10/362], Loss: 6.5125\n",
      "Epoch [1/1000], Step [20/362], Loss: 5.6272\n",
      "Epoch [1/1000], Step [30/362], Loss: 5.8188\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "train.train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
