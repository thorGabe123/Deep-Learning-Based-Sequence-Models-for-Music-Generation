{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4a292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import configs.common as cc\n",
    "\n",
    "loader = processing.DatasetLoader('E:\\\\GitHub\\\\dataset\\\\classical piano np\\\\classical piano midi')\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "# random_sample = loader.get_random_sample('train')\n",
    "# random_sample\n",
    "for src, trg, meta in train_dataloader:\n",
    "    break\n",
    "# src, trg, meta = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863d8191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding_table): Embedding(17890, 256)\n",
       "  (metadata_embedding_table): Embedding(2789, 256)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=256, out_features=17890, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import new_model\n",
    "import torch\n",
    "type = 'transformer'\n",
    "model_name = 'loss_0.90_time_2025-05-30-00-30-00.pth'\n",
    "model = new_model(type)\n",
    "# model.load_state_dict(torch.load(f'../pretrained/{type}/{model_name}'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7acf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\r"
     ]
    }
   ],
   "source": [
    "import train\n",
    "def generate(model, context_len, token_ids, meta_ids, num_tokens=1000, device='cpu'):\n",
    "    model.eval()\n",
    "    generated = token_ids.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for _ in range(num_tokens):\n",
    "                print(_, end=\"\\r\")\n",
    "                logits = model(token_ids, meta_ids)  # (1, seq_len, vocab_size)\n",
    "                filtered_logits = train.filtered_logit(token_ids, logits)\n",
    "                logits_last = filtered_logits[:, -1, :]       # (1, vocab_size)\n",
    "\n",
    "                # Get top-k\n",
    "                topk_probs, topk_indices = torch.topk(logits_last, 1, dim=-1)  # Each: shape (1, 5)\n",
    "                # Normalize to sum to 1\n",
    "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # Sample\n",
    "                next_token_idx = torch.multinomial(topk_probs, num_samples=1)  # (1, 1), value in [0..4]\n",
    "                next_token = topk_indices.gather(1, next_token_idx)\n",
    "\n",
    "                generated.append(next_token.item())\n",
    "\n",
    "                token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "                token_ids = token_ids[:, -context_len:]\n",
    "\n",
    "            # If you want metadata, sample or set as zeros:\n",
    "            # e.g., meta_ids = torch.cat([meta_ids, new_meta], dim=1)[:, -context_len:]\n",
    "\n",
    "    return generated\n",
    "\n",
    "new_seq = generate(model, cc.config.values.block_len, src[1].unsqueeze(0), meta[1].unsqueeze(0), num_tokens=100, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37dd9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_notes_old = processing.decode(src[1])\n",
    "processing.note_to_midi(decoded_notes_old, \"comparison.mid\")\n",
    "\n",
    "decoded_notes_new = processing.decode(new_seq)\n",
    "processing.note_to_midi(decoded_notes_new, \"generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6784f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16578,\n",
       " 16666,\n",
       " 17140,\n",
       " 17880,\n",
       " 83,\n",
       " 16596,\n",
       " 16696,\n",
       " 17166,\n",
       " 17880,\n",
       " 75,\n",
       " 16598,\n",
       " 16668,\n",
       " 17142,\n",
       " 17880,\n",
       " 80,\n",
       " 16597,\n",
       " 16694,\n",
       " 17140,\n",
       " 17880,\n",
       " 87,\n",
       " 16595,\n",
       " 16694,\n",
       " 17880,\n",
       " 58,\n",
       " 16577,\n",
       " 16652,\n",
       " 17880,\n",
       " 70,\n",
       " 16573,\n",
       " 16644,\n",
       " 17142,\n",
       " 17880,\n",
       " 75,\n",
       " 16585,\n",
       " 16666,\n",
       " 17166,\n",
       " 17880,\n",
       " 71,\n",
       " 16588,\n",
       " 16666,\n",
       " 17140,\n",
       " 17880,\n",
       " 47,\n",
       " 16557,\n",
       " 16664,\n",
       " 17142,\n",
       " 17880,\n",
       " 63,\n",
       " 16563,\n",
       " 16664,\n",
       " 17140,\n",
       " 17880,\n",
       " 68,\n",
       " 16571,\n",
       " 16664,\n",
       " 17880,\n",
       " 36,\n",
       " 16563,\n",
       " 16664,\n",
       " 17880,\n",
       " 48,\n",
       " 16567,\n",
       " 16664,\n",
       " 17880,\n",
       " 38,\n",
       " 16548,\n",
       " 16662,\n",
       " 17142,\n",
       " 17880,\n",
       " 44,\n",
       " 16566,\n",
       " 16648,\n",
       " 17164,\n",
       " 17880,\n",
       " 64,\n",
       " 16573,\n",
       " 16650,\n",
       " 17140,\n",
       " 17880,\n",
       " 72,\n",
       " 16585,\n",
       " 16684,\n",
       " 17880,\n",
       " 51,\n",
       " 16574,\n",
       " 16706,\n",
       " 17880,\n",
       " 60,\n",
       " 16580,\n",
       " 16708,\n",
       " 17880,\n",
       " 69,\n",
       " 16577,\n",
       " 16706,\n",
       " 17880,\n",
       " 79,\n",
       " 16577,\n",
       " 16688,\n",
       " 17160,\n",
       " 17880,\n",
       " 77,\n",
       " 16588,\n",
       " 16676,\n",
       " 17152,\n",
       " 17880,\n",
       " 67,\n",
       " 16572,\n",
       " 16674,\n",
       " 17142,\n",
       " 17880,\n",
       " 73,\n",
       " 16568,\n",
       " 16674,\n",
       " 17140,\n",
       " 17880,\n",
       " 72,\n",
       " 16582,\n",
       " 16662,\n",
       " 17152,\n",
       " 17880,\n",
       " 65,\n",
       " 16557,\n",
       " 16660,\n",
       " 17142,\n",
       " 17880,\n",
       " 58,\n",
       " 16560,\n",
       " 16656,\n",
       " 17144,\n",
       " 17880,\n",
       " 46,\n",
       " 16555,\n",
       " 16656,\n",
       " 17140,\n",
       " 17880,\n",
       " 80,\n",
       " 16600,\n",
       " 16726,\n",
       " 17148,\n",
       " 17880,\n",
       " 71,\n",
       " 16591,\n",
       " 16666,\n",
       " 17142,\n",
       " 17880,\n",
       " 70,\n",
       " 16584,\n",
       " 16744,\n",
       " 17154,\n",
       " 17880,\n",
       " 51,\n",
       " 16576,\n",
       " 16670,\n",
       " 17150,\n",
       " 17880,\n",
       " 39,\n",
       " 16577,\n",
       " 16742,\n",
       " 17140,\n",
       " 17880,\n",
       " 49,\n",
       " 16585,\n",
       " 16732,\n",
       " 17142,\n",
       " 17880,\n",
       " 79,\n",
       " 16593,\n",
       " 16732,\n",
       " 17140,\n",
       " 17880,\n",
       " 71,\n",
       " 16575,\n",
       " 16736,\n",
       " 17880,\n",
       " 77,\n",
       " 16593,\n",
       " 16670,\n",
       " 17880,\n",
       " 75,\n",
       " 16580,\n",
       " 16682,\n",
       " 17156,\n",
       " 17880,\n",
       " 82,\n",
       " 16599,\n",
       " 16800,\n",
       " 17152,\n",
       " 17880,\n",
       " 56,\n",
       " 16580,\n",
       " 16702,\n",
       " 17142,\n",
       " 17880,\n",
       " 51,\n",
       " 16573,\n",
       " 16670,\n",
       " 17140,\n",
       " 17880,\n",
       " 77,\n",
       " 16578,\n",
       " 16702,\n",
       " 17880,\n",
       " 72,\n",
       " 16584,\n",
       " 16690,\n",
       " 17152,\n",
       " 17880,\n",
       " 80,\n",
       " 16593,\n",
       " 16674,\n",
       " 17156,\n",
       " 17880,\n",
       " 75,\n",
       " 16591,\n",
       " 16688,\n",
       " 17142,\n",
       " 17880,\n",
       " 53,\n",
       " 16574,\n",
       " 16672,\n",
       " 17140,\n",
       " 17880,\n",
       " 51,\n",
       " 16572,\n",
       " 16672,\n",
       " 17880,\n",
       " 58,\n",
       " 16583,\n",
       " 16672,\n",
       " 17880,\n",
       " 46,\n",
       " 16565,\n",
       " 16698,\n",
       " 17880,\n",
       " 84,\n",
       " 16590,\n",
       " 16716,\n",
       " 17154,\n",
       " 17880,\n",
       " 73,\n",
       " 16563,\n",
       " 16660,\n",
       " 17140,\n",
       " 17880,\n",
       " 87,\n",
       " 16607,\n",
       " 16682,\n",
       " 17158,\n",
       " 17880,\n",
       " 51,\n",
       " 16576,\n",
       " 16682,\n",
       " 17140,\n",
       " 17880,\n",
       " 63,\n",
       " 16584,\n",
       " 16734,\n",
       " 17142,\n",
       " 17880,\n",
       " 70,\n",
       " 16594,\n",
       " 16734,\n",
       " 17140,\n",
       " 17880,\n",
       " 60,\n",
       " 16587,\n",
       " 16734,\n",
       " 17880,\n",
       " 86,\n",
       " 16583,\n",
       " 16734,\n",
       " 17880,\n",
       " 74,\n",
       " 16594,\n",
       " 16736,\n",
       " 17880,\n",
       " 81,\n",
       " 16589,\n",
       " 16734,\n",
       " 17880,\n",
       " 75,\n",
       " 16591,\n",
       " 16720,\n",
       " 17154,\n",
       " 17880,\n",
       " 80,\n",
       " 16580,\n",
       " 16708,\n",
       " 17152,\n",
       " 17880,\n",
       " 99,\n",
       " 16609,\n",
       " 16694,\n",
       " 17154,\n",
       " 17880,\n",
       " 39,\n",
       " 16591,\n",
       " 16694,\n",
       " 17140,\n",
       " 17880,\n",
       " 51,\n",
       " 16592,\n",
       " 16694,\n",
       " 17880,\n",
       " 87,\n",
       " 16600,\n",
       " 16692,\n",
       " 17142,\n",
       " 17880,\n",
       " 97,\n",
       " 16577,\n",
       " 16690,\n",
       " 17880,\n",
       " 85,\n",
       " 16573,\n",
       " 16690,\n",
       " 17140,\n",
       " 17880,\n",
       " 84,\n",
       " 16592,\n",
       " 16678,\n",
       " 17152,\n",
       " 17880,\n",
       " 89,\n",
       " 16595,\n",
       " 16668,\n",
       " 17150,\n",
       " 17880,\n",
       " 91,\n",
       " 16583,\n",
       " 16666,\n",
       " 17142,\n",
       " 17880,\n",
       " 96,\n",
       " 16573,\n",
       " 16660,\n",
       " 17146,\n",
       " 17880,\n",
       " 46,\n",
       " 16549,\n",
       " 16700,\n",
       " 17188,\n",
       " 17880,\n",
       " 34,\n",
       " 16549,\n",
       " 16642,\n",
       " 17140,\n",
       " 17880,\n",
       " 51,\n",
       " 16558,\n",
       " 16852,\n",
       " 17880,\n",
       " 60,\n",
       " 16562,\n",
       " 16842,\n",
       " 17150,\n",
       " 17880,\n",
       " 67,\n",
       " 16564,\n",
       " 16842,\n",
       " 17140,\n",
       " 17880,\n",
       " 68,\n",
       " 16561,\n",
       " 16842,\n",
       " 17880,\n",
       " 70,\n",
       " 16569,\n",
       " 16836,\n",
       " 17146,\n",
       " 17880,\n",
       " 72,\n",
       " 16583,\n",
       " 16828,\n",
       " 17148,\n",
       " 17880,\n",
       " 74,\n",
       " 16583,\n",
       " 16818,\n",
       " 17150,\n",
       " 17880,\n",
       " 79,\n",
       " 16568,\n",
       " 16812,\n",
       " 17146,\n",
       " 17880,\n",
       " 80,\n",
       " 16583,\n",
       " 16806,\n",
       " 17880,\n",
       " 46,\n",
       " 16570,\n",
       " 16792,\n",
       " 17154,\n",
       " 17880,\n",
       " 34,\n",
       " 16563,\n",
       " 16790,\n",
       " 17142,\n",
       " 17880,\n",
       " 41,\n",
       " 16556,\n",
       " 16790,\n",
       " 17140,\n",
       " 17880,\n",
       " 86,\n",
       " 16565,\n",
       " 16786,\n",
       " 17144,\n",
       " 17880,\n",
       " 91,\n",
       " 16589,\n",
       " 16782,\n",
       " 17880,\n",
       " 92,\n",
       " 16573,\n",
       " 16780,\n",
       " 17142,\n",
       " 17880,\n",
       " 39,\n",
       " 16534,\n",
       " 16788,\n",
       " 17286,\n",
       " 17880,\n",
       " 27,\n",
       " 16540,\n",
       " 16704,\n",
       " 17140,\n",
       " 17880,\n",
       " 51,\n",
       " 16536,\n",
       " 16704,\n",
       " 17880,\n",
       " 54,\n",
       " 16541,\n",
       " 16790,\n",
       " 17880,\n",
       " 23,\n",
       " 16541,\n",
       " 16708,\n",
       " 17202,\n",
       " 17880,\n",
       " 35,\n",
       " 16537,\n",
       " 16724,\n",
       " 17142,\n",
       " 17880,\n",
       " 56,\n",
       " 16544,\n",
       " 16724,\n",
       " 17140,\n",
       " 17880,\n",
       " 47,\n",
       " 16532,\n",
       " 16724,\n",
       " 17880,\n",
       " 51,\n",
       " 16529,\n",
       " 16724,\n",
       " 17880,\n",
       " 57,\n",
       " 16551,\n",
       " 16682,\n",
       " 17182,\n",
       " 17880,\n",
       " 30,\n",
       " 16542,\n",
       " 16682,\n",
       " 17140,\n",
       " 17880,\n",
       " 49,\n",
       " 16538,\n",
       " 16682,\n",
       " 17880,\n",
       " 42,\n",
       " 16540,\n",
       " 16680,\n",
       " 17142,\n",
       " 17880,\n",
       " 37,\n",
       " 16536,\n",
       " 16690,\n",
       " 17140,\n",
       " 17880,\n",
       " 56,\n",
       " 16557,\n",
       " 16716,\n",
       " 17180,\n",
       " 17880,\n",
       " 53,\n",
       " 16545,\n",
       " 16678,\n",
       " 17140,\n",
       " 17880,\n",
       " 41,\n",
       " 16547,\n",
       " 16748,\n",
       " 17142,\n",
       " 17880,\n",
       " 29,\n",
       " 16548,\n",
       " 16750,\n",
       " 17140,\n",
       " 17880,\n",
       " 47,\n",
       " 16542,\n",
       " 16678,\n",
       " 17880,\n",
       " 53,\n",
       " 16550,\n",
       " 16708,\n",
       " 17178,\n",
       " 8376,\n",
       " 16719,\n",
       " 1699,\n",
       " 16846,\n",
       " 5464,\n",
       " 17620,\n",
       " 4877,\n",
       " 17807,\n",
       " 1500,\n",
       " 17741,\n",
       " 3584,\n",
       " 17623,\n",
       " 3161,\n",
       " 17589,\n",
       " 14908,\n",
       " 17747,\n",
       " 16508,\n",
       " 17392,\n",
       " 13769,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 16623,\n",
       " 9924,\n",
       " 17176,\n",
       " 1176,\n",
       " 16997,\n",
       " 8092,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486,\n",
       " 8399,\n",
       " 17486]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6738f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  6.3134,  6.9035,  6.0828,  5.9433,  7.1644,  5.7620,  6.5757,\n",
       "         6.4643,  6.3919,  6.1867,  6.8310,  6.3284,  6.4244,  6.5372,  6.4623,\n",
       "         8.7010,  4.6767,  7.4200,  6.7567,  7.9998,  8.1419,  9.5407,  5.6227,\n",
       "         6.7564,  4.8045,  3.8351, 10.1992,  7.5654,  8.9391,  5.8445,  7.6321,\n",
       "         6.8122,  8.1876,  7.4191,  5.6611,  5.7854,  5.1118,  7.0683,  6.2696,\n",
       "         8.7257,  7.1330,  5.1107,  5.9827,  5.5587, 10.4906,  8.5093,  5.9634,\n",
       "         8.5304,  5.3487,  6.7490,  7.1678,  6.1069,  7.6541, 10.4402,  9.7946,\n",
       "         8.7629,  6.1342, 20.6024,  4.4450,  8.1050,  9.6003,  8.3159,  5.5958,\n",
       "         6.3124,  6.3709,  5.4957,  5.2022,  5.1336,  7.5839,  9.9579,  7.8871,\n",
       "         5.0124,  9.3024,  7.5886,  6.8797,  6.0038,  5.4065,  8.6481,  8.2093,\n",
       "         7.5058,  8.4233,  5.0589,  7.3166,  4.6714,  7.5599, 11.5220,  7.1528,\n",
       "         5.1387,  8.8976,  6.4618,  8.3402,  7.3777,  5.5708,  6.6366,  5.1106,\n",
       "         6.0629,  7.6522,  7.5989,  5.7476,  6.0687,  6.7983,  7.2596,  7.2970,\n",
       "         6.0818,  6.0549,  6.4826,  5.9529,  7.6480,  7.2386,  6.4008,  6.6834,\n",
       "         6.0840,  6.6701,  7.0517,  7.2221,  6.7062,  6.4384,  6.0925,  6.5989,\n",
       "         7.0982,  6.8054,  6.0257,  7.3505,  7.3086,  6.3370,  6.2097,  6.1043],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_logits[0][0][:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "063ab185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.4743,\n",
      "         6.8243,  6.7807, 11.2046,  8.2917,  8.2001,  9.5565,  8.2000,  8.4255,\n",
      "         8.5841,  9.2970,  8.3535,  9.2126], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([ 6.8611,  5.2008,  5.6286,  7.7482,  7.3960,  6.3381,  6.2766,  5.8202,\n",
      "         9.0022,  6.0999,  5.8930, 13.3240,  5.5961], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "filtered_output[-1][-1].argmax(-1)\n",
    "print(filtered_output[-1][-1][350:370])\n",
    "print(filtered_output[-1][-1][950:970])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a450cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_distributions():\n",
    "    # Prepare output tensor\n",
    "    block_len = cc.config.values.block_len\n",
    "    device = cc.config.values.device\n",
    "    vocab_size = cc.vocab_size\n",
    "    distributions = torch.zeros(6, vocab_size, device=device)\n",
    "\n",
    "    # For each token index, fill in regions with 1 as per your logic.\n",
    "    start = [cc.start_idx[\"pitch\"],\n",
    "             cc.start_idx[\"dyn\"],\n",
    "             cc.start_idx[\"length\"],\n",
    "             cc.start_idx[\"time\"],\n",
    "             cc.start_idx[\"channel\"],\n",
    "             cc.start_idx[\"tempo\"]]\n",
    "    end   = [cc.start_idx[\"dyn\"] - 1,\n",
    "             cc.start_idx[\"length\"] - 1,\n",
    "             cc.start_idx[\"time\"] - 1,\n",
    "             cc.start_idx[\"channel\"] - 1,\n",
    "             cc.start_idx[\"tempo\"] - 1,\n",
    "             cc.vocab_size]   # block_len implies : to the end\n",
    "\n",
    "    for token in range(6):\n",
    "        if token == 0:\n",
    "            distributions[token, start[1]:end[1]] = 1\n",
    "        if token == 1:\n",
    "            distributions[token, start[2]:end[2]] = 1\n",
    "        if token == 2:\n",
    "            distributions[token, start[3]:end[3]] = 1\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 3:\n",
    "            distributions[token, start[5]:end[5]] = 1\n",
    "        if token == 4:\n",
    "            distributions[token, start[0]:end[0]] = 1\n",
    "        if token == 5:\n",
    "            distributions[token, start[4]:end[4]] = 1\n",
    "\n",
    "    return distributions\n",
    "\n",
    "\n",
    "def pick_distributions_by_prev_token(\n",
    "        input_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    boundaries = [\n",
    "                cc.start_idx['dyn'] - 1,\n",
    "                cc.start_idx['length'] - 1,\n",
    "                cc.start_idx['time'] - 1,\n",
    "                cc.start_idx['channel'] - 1,\n",
    "                cc.start_idx['tempo'] - 1]\n",
    "        \n",
    "    bins = torch.tensor(boundaries, device=input_tokens.device)\n",
    "        # Each token is assigned to a bucket 0..len(boundaries)\n",
    "        # For example, with bins=[10,20,30]: \n",
    "        #   token <10 → 0, 10<=token<20 → 1, 20<=token<30 → 2, 30<=token → 3\n",
    "    buckets = torch.bucketize(input_tokens, bins, right=False)\n",
    "    distributions = make_distributions()\n",
    "\n",
    "    # Ensure buckets is long (int64) and on the same device as distributions\n",
    "    buckets = buckets.long().to(distributions.device)\n",
    "\n",
    "    # Now use advanced indexing to get values\n",
    "    output = distributions[buckets]  # shape: [6, 963]\n",
    "\n",
    "    return output\n",
    "\n",
    "def cross_entropy_loss(input, output, target, reduction='mean'):\n",
    "    weights = pick_distributions_by_prev_token(input)\n",
    "    log_probs = F.log_softmax(output, dim=1)\n",
    "    loss = log_probs * weights \n",
    "    loss = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else: # 'none'\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027dc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0048, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_loss(pred, target):\n",
    "    # pred: (N, C), target: (N,)\n",
    "    expected = pred.argmax(-1)\n",
    "    probs = torch.log_softmax(pred, dim=1)\n",
    "    loss = -probs[torch.arange(pred.shape[0]), target].mean()\n",
    "    return loss\n",
    "model.to(\"cuda\")\n",
    "output = model(src, meta)\n",
    "output = output.reshape(-1, cc.vocab_size)\n",
    "trg = trg.view(-1)\n",
    "my_custom_loss(output, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec76a161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0704,  1.4107,  1.1998,  2.6456,  3.4005,  1.2786,  1.5597, -0.3986,\n",
       "         0.5784,  0.5274,  2.8592,  0.4845, -1.1300,  0.1472,  0.6268,  1.2091,\n",
       "         1.2610,  0.5960,  2.9577,  1.9989], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][800:820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9456863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.0525, device='cuda:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1999a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8828, -1.6690, -2.4971, -4.0834, -2.2464, -3.8021, -1.7690,  9.0525,\n",
       "         6.0496,  7.2018], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1][350:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4e9a1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43, device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg[-513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f04cd9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(883, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54995b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6896, 7.8589, 7.1193, 8.7035, 8.3538, 9.9633, 8.1413, 9.2453, 9.2601,\n",
       "        8.3956, 8.1151, 9.1026, 8.0156, 7.8010, 8.5638, 8.9311, 7.8991, 7.7118,\n",
       "        9.2435, 6.8082], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-513][30:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
