{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pretty_midi\n",
    "from note import MIDI_note\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import processing\n",
    "\n",
    "def create_canvas(midi_notes):\n",
    "    # Assume tempo_times and tempo_bpm are defined\n",
    "    largest_time_end = int(max(note.time_end for note in midi_notes))\n",
    "    song_canvas = np.zeros((4, 128, largest_time_end), dtype=float)  # Use dtype=float for flexibility\n",
    "\n",
    "    for idx, n in enumerate(midi_notes):\n",
    "        tempo = int(n.tempo)\n",
    "        start = int(n.time_start)  # Convert time_start to integer\n",
    "        end = int(n.time_end)  # Convert time_end to integer\n",
    "\n",
    "        # Ensure indices are within the bounds of the canvas\n",
    "        if end > largest_time_end:\n",
    "            end = largest_time_end\n",
    "\n",
    "        # Fill the song_canvas with the respective values\n",
    "        song_canvas[0, n.pitch, start:end] = (n.channel + 1)\n",
    "        song_canvas[1, n.pitch, start:end] = n.dynamic\n",
    "        song_canvas[2, n.pitch, start] = 1  # Mark note start\n",
    "        song_canvas[3, n.pitch, start:end] = tempo\n",
    "\n",
    "    return song_canvas\n",
    "\n",
    "def show_canvas(song_canvas, feature_idx, pitch_bounds=None, time_bounds=None):\n",
    "    if time_bounds is not None:\n",
    "        lower_bound, upper_bound = time_bounds\n",
    "        array_2d = np.flipud(song_canvas[feature_idx].squeeze()[:, lower_bound:upper_bound])  # Shape becomes (128, 1000) with pitch flipped\n",
    "    else:\n",
    "        array_2d = np.flipud(song_canvas[feature_idx].squeeze()[:, :])  # Shape becomes (128, 1000) with pitch flipped\n",
    "\n",
    "    if pitch_bounds is not None:\n",
    "        lower_bound, upper_bound = pitch_bounds\n",
    "        array_2d = array_2d.squeeze()[lower_bound:upper_bound, :]  # Shape becomes (128, 1000) with pitch flipped\n",
    "    else:\n",
    "        array_2d = array_2d.squeeze()[:, :]  # Shape becomes (128, 1000) with pitch flipped\n",
    "\n",
    "    # Set the minimum value to 0 and normalize to the range [0, 1] for visualization\n",
    "    # array_2d = (array_2d - np.min(array_2d)) / (np.max(array_2d) - np.min(array_2d))\n",
    "\n",
    "    # Create a custom colormap with black for 0 and bright colors for higher values\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [(0, \"black\"), (1 / 128, \"blue\"), (1, \"yellow\")])\n",
    "\n",
    "    # Visualize the array as an image\n",
    "    plt.figure(figsize=(15, 5))  # Adjust figure size for large dimensions\n",
    "    plt.imshow(array_2d, aspect='auto', cmap=custom_cmap)  # Display the image with a custom colormap\n",
    "    plt.colorbar(label='Normalized Value')  # Add colorbar\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Pitch')\n",
    "    plt.title('Song Canvas Visualization with Enhanced Contrast')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def midi2canvas(path):\n",
    "    midi_notes= processing.extract_midi(path)\n",
    "    processing.adjust_note_time(midi_notes)\n",
    "    song_canvas = create_canvas(midi_notes)\n",
    "    return song_canvas\n",
    "\n",
    "def load_diffusion(midi_path):\n",
    "    song_canvas = np.float32(np.load(midi_path))\n",
    "    return song_canvas\n",
    "\n",
    "def encode_midi_diffusion(midi_path, dataset_path):\n",
    "    midi_files = []\n",
    "    midi_file_names = []\n",
    "\n",
    "    # Walk through the midi_path directory to gather MIDI files\n",
    "    for root, dirs, files in os.walk(midi_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.mid', '.midi')):\n",
    "                name_without_extension = os.path.splitext(file)[0]  # Remove the extension\n",
    "                # Check if the corresponding .npy file already exists\n",
    "                if not os.path.exists(os.path.join(dataset_path, f'{name_without_extension}.npy')):\n",
    "                    midi_files.append(os.path.join(root, file))\n",
    "                    midi_file_names.append(name_without_extension)\n",
    "\n",
    "    # Process and save the files that are not already in dataset_path\n",
    "    for idx, path in enumerate(midi_files):\n",
    "        song_canvas = midi2canvas(path)  # Convert MIDI to canvas\n",
    "        np.save(os.path.join(dataset_path, f'{midi_file_names[idx]}.npy'), song_canvas)\n",
    "\n",
    "class RandomSnippet(Dataset):\n",
    "    def __init__(self, image, snippet_size, num_snippets):\n",
    "        self.image = image\n",
    "        self.snippet_size = snippet_size\n",
    "        self.num_snippets = num_snippets\n",
    "        self.max_start_idx = image[1,1,:].size - snippet_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_snippets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a random start index\n",
    "        start = torch.randint(0, self.max_start_idx + 1, (1,)).item()\n",
    "        end = start + self.snippet_size\n",
    "        snippet = self.image[:, :, start:end]\n",
    "        return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_midi_diffusion('E:/GitHub/Deep-Learning-Based-Sequence-Models-for-Music-Generation/scripts/midi','E:/GitHub/Deep-Learning-Based-Sequence-Models-for-Music-Generation/scripts/npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_canvas = load_diffusion(\"E:/GitHub/Deep-Learning-Based-Sequence-Models-for-Music-Generation/scripts/npy/generated_Beethoven_mamba_0.npy\")\n",
    "thing = RandomSnippet(song_canvas, 4000, 1)\n",
    "blabla = thing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'use_value_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m args = model_and_diffusion_defaults()\n\u001b[32m      4\u001b[39m args.update({\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m64\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_channels\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m128\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnoise_schedule\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model, diffusion = \u001b[43mcreate_model_and_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model = model.cuda()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\diffusion\\script_util.py:135\u001b[39m, in \u001b[36mcreate_model_and_diffusion\u001b[39m\u001b[34m(image_size, class_cond, learn_sigma, num_channels, num_res_blocks, channel_mult, num_heads, num_head_channels, num_heads_upsample, attention_resolutions, dropout, diffusion_steps, noise_schedule, timestep_respacing, use_kl, predict_xstart, rescale_timesteps, rescale_learned_sigmas, use_checkpoint, use_scale_shift_norm, resblock_updown, use_fp16, use_new_attention_order, conf)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_model_and_diffusion\u001b[39m(\n\u001b[32m     91\u001b[39m     image_size,\n\u001b[32m     92\u001b[39m     class_cond,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     conf=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m ):\n\u001b[32m    116\u001b[39m     model = create_model(\n\u001b[32m    117\u001b[39m         image_size,\n\u001b[32m    118\u001b[39m         num_channels,\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m         conf=conf\n\u001b[32m    134\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     diffusion = \u001b[43mcreate_gaussian_diffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearn_sigma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearn_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_kl\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_kl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_xstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_xstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrescale_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrescale_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrescale_learned_sigmas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrescale_learned_sigmas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestep_respacing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep_respacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, diffusion\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Deep-Learning-Based-Sequence-Models-for-Music-Generation\\diffusion\\script_util.py:274\u001b[39m, in \u001b[36mcreate_gaussian_diffusion\u001b[39m\u001b[34m(steps, learn_sigma, sigma_small, noise_schedule, use_kl, predict_xstart, rescale_timesteps, rescale_learned_sigmas, timestep_respacing, conf)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_gaussian_diffusion\u001b[39m(\n\u001b[32m    259\u001b[39m     *,\n\u001b[32m    260\u001b[39m     steps=\u001b[32m1000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m     conf=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m ):\n\u001b[32m    272\u001b[39m     betas = gd.get_named_beta_schedule(noise_schedule, steps, use_scale=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_value_logger\u001b[49m:\n\u001b[32m    275\u001b[39m         conf.value_logger.add_value(\n\u001b[32m    276\u001b[39m             betas, \u001b[33m'\u001b[39m\u001b[33mbetas create_gaussian_diffusion\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_kl:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'use_value_logger'"
     ]
    }
   ],
   "source": [
    "from script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "\n",
    "args = model_and_diffusion_defaults()\n",
    "args.update({\n",
    "    \"image_size\": 64,\n",
    "    \"num_channels\": 128,\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"diffusion_steps\": 1000,\n",
    "    \"noise_schedule\": \"linear\",\n",
    "})\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**args)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     show_canvas(blabla,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
