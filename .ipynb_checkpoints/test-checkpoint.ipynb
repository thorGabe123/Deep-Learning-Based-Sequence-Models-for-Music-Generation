{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s203861/newenv/lib/python3.11/site-packages/pretty_midi/instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import processing\n",
    "import os\n",
    "import configs.common as cc\n",
    "loader = processing.DatasetLoader(\"/home/s203861/midi-classical-music/np_data/data\")\n",
    "cc.config.values.block_len = 100\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (src, trg, meta) in enumerate(train_dataloader):\n",
    "    break\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 36,184,241\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5923, 0.4978, 0.4305,  ..., 0.6360, 0.5202, 0.4733],\n",
       "        [0.4488, 0.7453, 0.5956,  ..., 0.3503, 0.5299, 0.5580],\n",
       "        [0.6576, 0.6730, 0.4723,  ..., 0.2760, 0.3968, 0.6303],\n",
       "        ...,\n",
       "        [0.6736, 0.3537, 0.5451,  ..., 0.5299, 0.3238, 0.7350],\n",
       "        [0.3279, 0.4603, 0.4592,  ..., 0.4622, 0.4057, 0.6204],\n",
       "        [0.4731, 0.6600, 0.6917,  ..., 0.2091, 0.7079, 0.8330]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(src)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_all_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trg = \u001b[43mget_all_targets\u001b[49m(meta).to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m trg\n\u001b[32m      3\u001b[39m criterion(output, trg)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_all_targets' is not defined"
     ]
    }
   ],
   "source": [
    "trg = get_all_targets(meta).to('cuda')\n",
    "trg\n",
    "criterion(output, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(tensor):\n",
    "    return [torch.unique(row) for row in tensor]\n",
    "\n",
    "def make_meta_target(tensor):\n",
    "    target = torch.zeros(cc.metadata_vocab_size)\n",
    "    target[tensor] = 1\n",
    "    return target\n",
    "\n",
    "def get_all_targets(tensor):\n",
    "    meta_unique = get_set(tensor)\n",
    "    return torch.stack([make_meta_target(m) for m in meta_unique])\n",
    "\n",
    "def save_model(model, loss):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    pretrained_path = paths.config.paths.pretrained\n",
    "\n",
    "    save_path = f'{pretrained_path}/classifier/loss_{loss:.2f}_time_{now}.pth'\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n",
      "Epoch [1/1000], Step [10/448], Loss: 0.6873\n",
      "Epoch [1/1000], Step [20/448], Loss: 0.6662\n",
      "Epoch [1/1000], Step [30/448], Loss: 0.5719\n",
      "Epoch [1/1000], Step [40/448], Loss: 0.4469\n",
      "Epoch [1/1000], Step [50/448], Loss: 0.3876\n",
      "Epoch [1/1000], Step [60/448], Loss: 0.2339\n",
      "Epoch [1/1000], Step [70/448], Loss: 0.1834\n",
      "Epoch [1/1000], Step [80/448], Loss: 0.1490\n",
      "Epoch [1/1000], Step [90/448], Loss: 0.1269\n",
      "Epoch [1/1000], Step [100/448], Loss: 0.1127\n",
      "Epoch [1/1000], Step [110/448], Loss: 0.1025\n",
      "Epoch [1/1000], Step [120/448], Loss: 0.0912\n",
      "Epoch [1/1000], Step [130/448], Loss: 0.0851\n",
      "Epoch [1/1000], Step [140/448], Loss: 0.0787\n",
      "Epoch [1/1000], Step [150/448], Loss: 0.0713\n",
      "Epoch [1/1000], Step [160/448], Loss: 0.0674\n",
      "Epoch [1/1000], Step [170/448], Loss: 0.0664\n",
      "Epoch [1/1000], Step [180/448], Loss: 0.0626\n",
      "Epoch [1/1000], Step [190/448], Loss: 0.0575\n",
      "Epoch [1/1000], Step [200/448], Loss: 0.0576\n",
      "Epoch [1/1000], Step [210/448], Loss: 0.0554\n",
      "Epoch [1/1000], Step [220/448], Loss: 0.0521\n",
      "Epoch [1/1000], Step [230/448], Loss: 0.0494\n",
      "Epoch [1/1000], Step [240/448], Loss: 0.0504\n",
      "Epoch [1/1000], Step [250/448], Loss: 0.0469\n",
      "Epoch [1/1000], Step [260/448], Loss: 0.0443\n",
      "Epoch [1/1000], Step [270/448], Loss: 0.0462\n",
      "Epoch [1/1000], Step [280/448], Loss: 0.0437\n",
      "Epoch [1/1000], Step [290/448], Loss: 0.0435\n",
      "Epoch [1/1000], Step [300/448], Loss: 0.0435\n",
      "Epoch [1/1000], Step [310/448], Loss: 0.0440\n",
      "Epoch [1/1000], Step [320/448], Loss: 0.0386\n",
      "Epoch [1/1000], Step [330/448], Loss: 0.0395\n",
      "Epoch [1/1000], Step [340/448], Loss: 0.0437\n",
      "Epoch [1/1000], Step [350/448], Loss: 0.0372\n",
      "Epoch [1/1000], Step [360/448], Loss: 0.0378\n",
      "Epoch [1/1000], Step [370/448], Loss: 0.0394\n",
      "Epoch [1/1000], Step [380/448], Loss: 0.0345\n",
      "Epoch [1/1000], Step [390/448], Loss: 0.0360\n",
      "Epoch [1/1000], Step [400/448], Loss: 0.0369\n",
      "Epoch [1/1000], Step [410/448], Loss: 0.0347\n",
      "Epoch [1/1000], Step [420/448], Loss: 0.0357\n",
      "Epoch [1/1000], Step [430/448], Loss: 0.0368\n",
      "Epoch [1/1000], Step [440/448], Loss: 0.0333\n",
      "Epoch [1/1000], Average Loss: 0.1266\n",
      "Epoch [1/1000], Validation Loss: 0.0348\n",
      "Epoch [2/1000], Step [10/448], Loss: 0.0382\n",
      "Epoch [2/1000], Step [20/448], Loss: 0.0349\n",
      "Epoch [2/1000], Step [30/448], Loss: 0.0334\n",
      "Epoch [2/1000], Step [40/448], Loss: 0.0372\n",
      "Epoch [2/1000], Step [50/448], Loss: 0.0333\n",
      "Epoch [2/1000], Step [60/448], Loss: 0.0373\n",
      "Epoch [2/1000], Step [70/448], Loss: 0.0374\n",
      "Epoch [2/1000], Step [80/448], Loss: 0.0303\n",
      "Epoch [2/1000], Step [90/448], Loss: 0.0344\n",
      "Epoch [2/1000], Step [100/448], Loss: 0.0304\n",
      "Epoch [2/1000], Step [110/448], Loss: 0.0315\n",
      "Epoch [2/1000], Step [120/448], Loss: 0.0340\n",
      "Epoch [2/1000], Step [130/448], Loss: 0.0335\n",
      "Epoch [2/1000], Step [140/448], Loss: 0.0374\n",
      "Epoch [2/1000], Step [150/448], Loss: 0.0289\n",
      "Epoch [2/1000], Step [160/448], Loss: 0.0311\n",
      "Epoch [2/1000], Step [170/448], Loss: 0.0317\n",
      "Epoch [2/1000], Step [180/448], Loss: 0.0306\n",
      "Epoch [2/1000], Step [190/448], Loss: 0.0345\n",
      "Epoch [2/1000], Step [200/448], Loss: 0.0316\n",
      "Epoch [2/1000], Step [210/448], Loss: 0.0248\n",
      "Epoch [2/1000], Step [220/448], Loss: 0.0345\n",
      "Epoch [2/1000], Step [230/448], Loss: 0.0299\n",
      "Epoch [2/1000], Step [240/448], Loss: 0.0297\n",
      "Epoch [2/1000], Step [250/448], Loss: 0.0320\n",
      "Epoch [2/1000], Step [260/448], Loss: 0.0322\n",
      "Epoch [2/1000], Step [270/448], Loss: 0.0359\n",
      "Epoch [2/1000], Step [280/448], Loss: 0.0295\n",
      "Epoch [2/1000], Step [290/448], Loss: 0.0326\n",
      "Epoch [2/1000], Step [300/448], Loss: 0.0272\n",
      "Epoch [2/1000], Step [310/448], Loss: 0.0245\n",
      "Epoch [2/1000], Step [320/448], Loss: 0.0334\n",
      "Epoch [2/1000], Step [330/448], Loss: 0.0277\n",
      "Epoch [2/1000], Step [340/448], Loss: 0.0285\n",
      "Epoch [2/1000], Step [350/448], Loss: 0.0261\n",
      "Epoch [2/1000], Step [360/448], Loss: 0.0284\n",
      "Epoch [2/1000], Step [370/448], Loss: 0.0295\n",
      "Epoch [2/1000], Step [380/448], Loss: 0.0271\n",
      "Epoch [2/1000], Step [390/448], Loss: 0.0283\n",
      "Epoch [2/1000], Step [400/448], Loss: 0.0250\n",
      "Epoch [2/1000], Step [410/448], Loss: 0.0301\n",
      "Epoch [2/1000], Step [420/448], Loss: 0.0298\n",
      "Epoch [2/1000], Step [430/448], Loss: 0.0251\n",
      "Epoch [2/1000], Step [440/448], Loss: 0.0315\n",
      "Epoch [2/1000], Average Loss: 0.0310\n",
      "Epoch [2/1000], Validation Loss: 0.0292\n",
      "Epoch [3/1000], Step [10/448], Loss: 0.0308\n",
      "Epoch [3/1000], Step [20/448], Loss: 0.0292\n",
      "Epoch [3/1000], Step [30/448], Loss: 0.0303\n",
      "Epoch [3/1000], Step [40/448], Loss: 0.0285\n",
      "Epoch [3/1000], Step [50/448], Loss: 0.0234\n",
      "Epoch [3/1000], Step [60/448], Loss: 0.0261\n",
      "Epoch [3/1000], Step [70/448], Loss: 0.0259\n",
      "Epoch [3/1000], Step [80/448], Loss: 0.0295\n",
      "Epoch [3/1000], Step [90/448], Loss: 0.0310\n",
      "Epoch [3/1000], Step [100/448], Loss: 0.0280\n",
      "Epoch [3/1000], Step [110/448], Loss: 0.0276\n",
      "Epoch [3/1000], Step [120/448], Loss: 0.0323\n",
      "Epoch [3/1000], Step [130/448], Loss: 0.0280\n",
      "Epoch [3/1000], Step [140/448], Loss: 0.0273\n",
      "Epoch [3/1000], Step [150/448], Loss: 0.0304\n",
      "Epoch [3/1000], Step [160/448], Loss: 0.0289\n",
      "Epoch [3/1000], Step [170/448], Loss: 0.0256\n",
      "Epoch [3/1000], Step [180/448], Loss: 0.0283\n",
      "Epoch [3/1000], Step [190/448], Loss: 0.0292\n",
      "Epoch [3/1000], Step [200/448], Loss: 0.0256\n",
      "Epoch [3/1000], Step [210/448], Loss: 0.0242\n",
      "Epoch [3/1000], Step [220/448], Loss: 0.0284\n",
      "Epoch [3/1000], Step [230/448], Loss: 0.0239\n",
      "Epoch [3/1000], Step [240/448], Loss: 0.0314\n",
      "Epoch [3/1000], Step [250/448], Loss: 0.0362\n",
      "Epoch [3/1000], Step [260/448], Loss: 0.0271\n",
      "Epoch [3/1000], Step [270/448], Loss: 0.0288\n",
      "Epoch [3/1000], Step [280/448], Loss: 0.0228\n",
      "Epoch [3/1000], Step [290/448], Loss: 0.0238\n",
      "Epoch [3/1000], Step [300/448], Loss: 0.0276\n",
      "Epoch [3/1000], Step [310/448], Loss: 0.0272\n",
      "Epoch [3/1000], Step [320/448], Loss: 0.0226\n",
      "Epoch [3/1000], Step [330/448], Loss: 0.0258\n",
      "Epoch [3/1000], Step [340/448], Loss: 0.0264\n",
      "Epoch [3/1000], Step [350/448], Loss: 0.0266\n",
      "Epoch [3/1000], Step [360/448], Loss: 0.0257\n",
      "Epoch [3/1000], Step [370/448], Loss: 0.0244\n",
      "Epoch [3/1000], Step [380/448], Loss: 0.0252\n",
      "Epoch [3/1000], Step [390/448], Loss: 0.0269\n",
      "Epoch [3/1000], Step [400/448], Loss: 0.0253\n",
      "Epoch [3/1000], Step [410/448], Loss: 0.0322\n",
      "Epoch [3/1000], Step [420/448], Loss: 0.0239\n",
      "Epoch [3/1000], Step [430/448], Loss: 0.0255\n",
      "Epoch [3/1000], Step [440/448], Loss: 0.0320\n",
      "Epoch [3/1000], Average Loss: 0.0276\n",
      "Epoch [3/1000], Validation Loss: 0.0282\n",
      "Epoch [4/1000], Step [10/448], Loss: 0.0334\n",
      "Epoch [4/1000], Step [20/448], Loss: 0.0252\n",
      "Epoch [4/1000], Step [30/448], Loss: 0.0285\n",
      "Epoch [4/1000], Step [40/448], Loss: 0.0237\n",
      "Epoch [4/1000], Step [50/448], Loss: 0.0224\n",
      "Epoch [4/1000], Step [60/448], Loss: 0.0279\n",
      "Epoch [4/1000], Step [70/448], Loss: 0.0269\n",
      "Epoch [4/1000], Step [80/448], Loss: 0.0270\n",
      "Epoch [4/1000], Step [90/448], Loss: 0.0226\n",
      "Epoch [4/1000], Step [100/448], Loss: 0.0289\n",
      "Epoch [4/1000], Step [110/448], Loss: 0.0262\n",
      "Epoch [4/1000], Step [120/448], Loss: 0.0312\n",
      "Epoch [4/1000], Step [130/448], Loss: 0.0278\n",
      "Epoch [4/1000], Step [140/448], Loss: 0.0306\n",
      "Epoch [4/1000], Step [150/448], Loss: 0.0236\n",
      "Epoch [4/1000], Step [160/448], Loss: 0.0239\n",
      "Epoch [4/1000], Step [170/448], Loss: 0.0270\n",
      "Epoch [4/1000], Step [180/448], Loss: 0.0251\n",
      "Epoch [4/1000], Step [190/448], Loss: 0.0332\n",
      "Epoch [4/1000], Step [200/448], Loss: 0.0219\n",
      "Epoch [4/1000], Step [210/448], Loss: 0.0305\n",
      "Epoch [4/1000], Step [220/448], Loss: 0.0276\n",
      "Epoch [4/1000], Step [230/448], Loss: 0.0202\n",
      "Epoch [4/1000], Step [240/448], Loss: 0.0282\n",
      "Epoch [4/1000], Step [250/448], Loss: 0.0193\n",
      "Epoch [4/1000], Step [260/448], Loss: 0.0237\n",
      "Epoch [4/1000], Step [270/448], Loss: 0.0371\n",
      "Epoch [4/1000], Step [280/448], Loss: 0.0306\n",
      "Epoch [4/1000], Step [290/448], Loss: 0.0214\n",
      "Epoch [4/1000], Step [300/448], Loss: 0.0216\n",
      "Epoch [4/1000], Step [310/448], Loss: 0.0198\n",
      "Epoch [4/1000], Step [320/448], Loss: 0.0223\n",
      "Epoch [4/1000], Step [330/448], Loss: 0.0286\n",
      "Epoch [4/1000], Step [340/448], Loss: 0.0307\n",
      "Epoch [4/1000], Step [350/448], Loss: 0.0300\n",
      "Epoch [4/1000], Step [360/448], Loss: 0.0261\n",
      "Epoch [4/1000], Step [370/448], Loss: 0.0306\n",
      "Epoch [4/1000], Step [380/448], Loss: 0.0227\n",
      "Epoch [4/1000], Step [390/448], Loss: 0.0249\n",
      "Epoch [4/1000], Step [400/448], Loss: 0.0247\n",
      "Epoch [4/1000], Step [410/448], Loss: 0.0264\n",
      "Epoch [4/1000], Step [420/448], Loss: 0.0248\n",
      "Epoch [4/1000], Step [430/448], Loss: 0.0281\n",
      "Epoch [4/1000], Step [440/448], Loss: 0.0336\n",
      "Epoch [4/1000], Average Loss: 0.0266\n",
      "Epoch [4/1000], Validation Loss: 0.0271\n",
      "Epoch [5/1000], Step [10/448], Loss: 0.0258\n",
      "Epoch [5/1000], Step [20/448], Loss: 0.0289\n",
      "Epoch [5/1000], Step [30/448], Loss: 0.0305\n",
      "Epoch [5/1000], Step [40/448], Loss: 0.0288\n",
      "Epoch [5/1000], Step [50/448], Loss: 0.0244\n",
      "Epoch [5/1000], Step [60/448], Loss: 0.0240\n",
      "Epoch [5/1000], Step [70/448], Loss: 0.0268\n",
      "Epoch [5/1000], Step [80/448], Loss: 0.0349\n",
      "Epoch [5/1000], Step [90/448], Loss: 0.0272\n",
      "Epoch [5/1000], Step [100/448], Loss: 0.0240\n",
      "Epoch [5/1000], Step [110/448], Loss: 0.0230\n",
      "Epoch [5/1000], Step [120/448], Loss: 0.0246\n",
      "Epoch [5/1000], Step [130/448], Loss: 0.0263\n",
      "Epoch [5/1000], Step [140/448], Loss: 0.0267\n",
      "Epoch [5/1000], Step [150/448], Loss: 0.0231\n",
      "Epoch [5/1000], Step [160/448], Loss: 0.0267\n",
      "Epoch [5/1000], Step [170/448], Loss: 0.0249\n",
      "Epoch [5/1000], Step [180/448], Loss: 0.0231\n",
      "Epoch [5/1000], Step [190/448], Loss: 0.0306\n",
      "Epoch [5/1000], Step [200/448], Loss: 0.0249\n",
      "Epoch [5/1000], Step [210/448], Loss: 0.0292\n",
      "Epoch [5/1000], Step [220/448], Loss: 0.0296\n",
      "Epoch [5/1000], Step [230/448], Loss: 0.0299\n",
      "Epoch [5/1000], Step [240/448], Loss: 0.0274\n",
      "Epoch [5/1000], Step [250/448], Loss: 0.0225\n",
      "Epoch [5/1000], Step [260/448], Loss: 0.0349\n",
      "Epoch [5/1000], Step [270/448], Loss: 0.0223\n",
      "Epoch [5/1000], Step [280/448], Loss: 0.0208\n",
      "Epoch [5/1000], Step [290/448], Loss: 0.0250\n",
      "Epoch [5/1000], Step [300/448], Loss: 0.0240\n",
      "Epoch [5/1000], Step [310/448], Loss: 0.0272\n",
      "Epoch [5/1000], Step [320/448], Loss: 0.0260\n",
      "Epoch [5/1000], Step [330/448], Loss: 0.0211\n",
      "Epoch [5/1000], Step [340/448], Loss: 0.0250\n",
      "Epoch [5/1000], Step [350/448], Loss: 0.0263\n",
      "Epoch [5/1000], Step [360/448], Loss: 0.0282\n",
      "Epoch [5/1000], Step [370/448], Loss: 0.0224\n",
      "Epoch [5/1000], Step [380/448], Loss: 0.0296\n",
      "Epoch [5/1000], Step [390/448], Loss: 0.0240\n",
      "Epoch [5/1000], Step [400/448], Loss: 0.0328\n",
      "Epoch [5/1000], Step [410/448], Loss: 0.0255\n",
      "Epoch [5/1000], Step [420/448], Loss: 0.0221\n",
      "Epoch [5/1000], Step [430/448], Loss: 0.0277\n",
      "Epoch [5/1000], Step [440/448], Loss: 0.0257\n",
      "Epoch [5/1000], Average Loss: 0.0263\n",
      "Epoch [5/1000], Validation Loss: 0.0275\n",
      "Epoch [6/1000], Step [10/448], Loss: 0.0289\n",
      "Epoch [6/1000], Step [20/448], Loss: 0.0317\n",
      "Epoch [6/1000], Step [30/448], Loss: 0.0251\n",
      "Epoch [6/1000], Step [40/448], Loss: 0.0211\n",
      "Epoch [6/1000], Step [50/448], Loss: 0.0258\n",
      "Epoch [6/1000], Step [60/448], Loss: 0.0366\n",
      "Epoch [6/1000], Step [70/448], Loss: 0.0268\n",
      "Epoch [6/1000], Step [80/448], Loss: 0.0306\n",
      "Epoch [6/1000], Step [90/448], Loss: 0.0219\n",
      "Epoch [6/1000], Step [100/448], Loss: 0.0304\n",
      "Epoch [6/1000], Step [110/448], Loss: 0.0219\n",
      "Epoch [6/1000], Step [120/448], Loss: 0.0275\n",
      "Epoch [6/1000], Step [130/448], Loss: 0.0247\n",
      "Epoch [6/1000], Step [140/448], Loss: 0.0249\n",
      "Epoch [6/1000], Step [150/448], Loss: 0.0211\n",
      "Epoch [6/1000], Step [160/448], Loss: 0.0300\n",
      "Epoch [6/1000], Step [170/448], Loss: 0.0254\n",
      "Epoch [6/1000], Step [180/448], Loss: 0.0329\n",
      "Epoch [6/1000], Step [190/448], Loss: 0.0225\n",
      "Epoch [6/1000], Step [200/448], Loss: 0.0251\n",
      "Epoch [6/1000], Step [210/448], Loss: 0.0207\n",
      "Epoch [6/1000], Step [220/448], Loss: 0.0321\n",
      "Epoch [6/1000], Step [230/448], Loss: 0.0282\n",
      "Epoch [6/1000], Step [240/448], Loss: 0.0234\n",
      "Epoch [6/1000], Step [250/448], Loss: 0.0233\n",
      "Epoch [6/1000], Step [260/448], Loss: 0.0280\n",
      "Epoch [6/1000], Step [270/448], Loss: 0.0346\n",
      "Epoch [6/1000], Step [280/448], Loss: 0.0238\n",
      "Epoch [6/1000], Step [290/448], Loss: 0.0225\n",
      "Epoch [6/1000], Step [300/448], Loss: 0.0253\n",
      "Epoch [6/1000], Step [310/448], Loss: 0.0282\n",
      "Epoch [6/1000], Step [320/448], Loss: 0.0291\n",
      "Epoch [6/1000], Step [330/448], Loss: 0.0284\n",
      "Epoch [6/1000], Step [340/448], Loss: 0.0228\n",
      "Epoch [6/1000], Step [350/448], Loss: 0.0284\n",
      "Epoch [6/1000], Step [360/448], Loss: 0.0242\n",
      "Epoch [6/1000], Step [370/448], Loss: 0.0202\n",
      "Epoch [6/1000], Step [380/448], Loss: 0.0213\n",
      "Epoch [6/1000], Step [390/448], Loss: 0.0300\n",
      "Epoch [6/1000], Step [400/448], Loss: 0.0281\n",
      "Epoch [6/1000], Step [410/448], Loss: 0.0286\n",
      "Epoch [6/1000], Step [420/448], Loss: 0.0210\n",
      "Epoch [6/1000], Step [430/448], Loss: 0.0228\n",
      "Epoch [6/1000], Step [440/448], Loss: 0.0304\n",
      "Epoch [6/1000], Average Loss: 0.0262\n",
      "Epoch [6/1000], Validation Loss: 0.0270\n",
      "Epoch [7/1000], Step [10/448], Loss: 0.0231\n",
      "Epoch [7/1000], Step [20/448], Loss: 0.0256\n",
      "Epoch [7/1000], Step [30/448], Loss: 0.0219\n",
      "Epoch [7/1000], Step [40/448], Loss: 0.0186\n",
      "Epoch [7/1000], Step [50/448], Loss: 0.0202\n",
      "Epoch [7/1000], Step [60/448], Loss: 0.0196\n",
      "Epoch [7/1000], Step [70/448], Loss: 0.0265\n",
      "Epoch [7/1000], Step [80/448], Loss: 0.0234\n",
      "Epoch [7/1000], Step [90/448], Loss: 0.0264\n",
      "Epoch [7/1000], Step [100/448], Loss: 0.0172\n",
      "Epoch [7/1000], Step [110/448], Loss: 0.0197\n",
      "Epoch [7/1000], Step [120/448], Loss: 0.0197\n",
      "Epoch [7/1000], Step [130/448], Loss: 0.0313\n",
      "Epoch [7/1000], Step [140/448], Loss: 0.0234\n",
      "Epoch [7/1000], Step [150/448], Loss: 0.0240\n",
      "Epoch [7/1000], Step [160/448], Loss: 0.0203\n",
      "Epoch [7/1000], Step [170/448], Loss: 0.0231\n",
      "Epoch [7/1000], Step [180/448], Loss: 0.0300\n",
      "Epoch [7/1000], Step [190/448], Loss: 0.0272\n",
      "Epoch [7/1000], Step [200/448], Loss: 0.0246\n",
      "Epoch [7/1000], Step [210/448], Loss: 0.0290\n",
      "Epoch [7/1000], Step [220/448], Loss: 0.0251\n",
      "Epoch [7/1000], Step [230/448], Loss: 0.0231\n",
      "Epoch [7/1000], Step [240/448], Loss: 0.0248\n",
      "Epoch [7/1000], Step [250/448], Loss: 0.0181\n",
      "Epoch [7/1000], Step [260/448], Loss: 0.0304\n",
      "Epoch [7/1000], Step [270/448], Loss: 0.0264\n",
      "Epoch [7/1000], Step [280/448], Loss: 0.0277\n",
      "Epoch [7/1000], Step [290/448], Loss: 0.0266\n",
      "Epoch [7/1000], Step [300/448], Loss: 0.0228\n",
      "Epoch [7/1000], Step [310/448], Loss: 0.0336\n",
      "Epoch [7/1000], Step [320/448], Loss: 0.0282\n",
      "Epoch [7/1000], Step [330/448], Loss: 0.0221\n",
      "Epoch [7/1000], Step [340/448], Loss: 0.0282\n",
      "Epoch [7/1000], Step [350/448], Loss: 0.0293\n",
      "Epoch [7/1000], Step [360/448], Loss: 0.0269\n",
      "Epoch [7/1000], Step [370/448], Loss: 0.0271\n",
      "Epoch [7/1000], Step [380/448], Loss: 0.0251\n",
      "Epoch [7/1000], Step [390/448], Loss: 0.0275\n",
      "Epoch [7/1000], Step [400/448], Loss: 0.0221\n",
      "Epoch [7/1000], Step [410/448], Loss: 0.0250\n",
      "Epoch [7/1000], Step [420/448], Loss: 0.0232\n",
      "Epoch [7/1000], Step [430/448], Loss: 0.0274\n",
      "Epoch [7/1000], Step [440/448], Loss: 0.0292\n",
      "Epoch [7/1000], Average Loss: 0.0259\n",
      "Epoch [7/1000], Validation Loss: 0.0270\n",
      "Epoch [8/1000], Step [10/448], Loss: 0.0270\n",
      "Epoch [8/1000], Step [20/448], Loss: 0.0272\n",
      "Epoch [8/1000], Step [30/448], Loss: 0.0307\n",
      "Epoch [8/1000], Step [40/448], Loss: 0.0250\n",
      "Epoch [8/1000], Step [50/448], Loss: 0.0288\n",
      "Epoch [8/1000], Step [60/448], Loss: 0.0325\n",
      "Epoch [8/1000], Step [70/448], Loss: 0.0236\n",
      "Epoch [8/1000], Step [80/448], Loss: 0.0303\n",
      "Epoch [8/1000], Step [90/448], Loss: 0.0194\n",
      "Epoch [8/1000], Step [100/448], Loss: 0.0197\n",
      "Epoch [8/1000], Step [110/448], Loss: 0.0209\n",
      "Epoch [8/1000], Step [120/448], Loss: 0.0259\n",
      "Epoch [8/1000], Step [130/448], Loss: 0.0310\n",
      "Epoch [8/1000], Step [140/448], Loss: 0.0222\n",
      "Epoch [8/1000], Step [150/448], Loss: 0.0237\n",
      "Epoch [8/1000], Step [160/448], Loss: 0.0207\n",
      "Epoch [8/1000], Step [170/448], Loss: 0.0218\n",
      "Epoch [8/1000], Step [180/448], Loss: 0.0201\n",
      "Epoch [8/1000], Step [190/448], Loss: 0.0328\n",
      "Epoch [8/1000], Step [200/448], Loss: 0.0244\n",
      "Epoch [8/1000], Step [210/448], Loss: 0.0262\n",
      "Epoch [8/1000], Step [220/448], Loss: 0.0188\n",
      "Epoch [8/1000], Step [230/448], Loss: 0.0229\n",
      "Epoch [8/1000], Step [240/448], Loss: 0.0258\n",
      "Epoch [8/1000], Step [250/448], Loss: 0.0230\n",
      "Epoch [8/1000], Step [260/448], Loss: 0.0243\n",
      "Epoch [8/1000], Step [270/448], Loss: 0.0165\n",
      "Epoch [8/1000], Step [280/448], Loss: 0.0238\n",
      "Epoch [8/1000], Step [290/448], Loss: 0.0308\n",
      "Epoch [8/1000], Step [300/448], Loss: 0.0273\n",
      "Epoch [8/1000], Step [310/448], Loss: 0.0248\n",
      "Epoch [8/1000], Step [320/448], Loss: 0.0247\n",
      "Epoch [8/1000], Step [330/448], Loss: 0.0257\n",
      "Epoch [8/1000], Step [340/448], Loss: 0.0162\n",
      "Epoch [8/1000], Step [350/448], Loss: 0.0258\n",
      "Epoch [8/1000], Step [360/448], Loss: 0.0249\n",
      "Epoch [8/1000], Step [370/448], Loss: 0.0182\n",
      "Epoch [8/1000], Step [380/448], Loss: 0.0229\n",
      "Epoch [8/1000], Step [390/448], Loss: 0.0281\n",
      "Epoch [8/1000], Step [400/448], Loss: 0.0208\n",
      "Epoch [8/1000], Step [410/448], Loss: 0.0271\n",
      "Epoch [8/1000], Step [420/448], Loss: 0.0183\n",
      "Epoch [8/1000], Step [430/448], Loss: 0.0235\n",
      "Epoch [8/1000], Step [440/448], Loss: 0.0280\n",
      "Epoch [8/1000], Average Loss: 0.0250\n",
      "Epoch [8/1000], Validation Loss: 0.0250\n",
      "Epoch [9/1000], Step [10/448], Loss: 0.0198\n",
      "Epoch [9/1000], Step [20/448], Loss: 0.0251\n",
      "Epoch [9/1000], Step [30/448], Loss: 0.0286\n",
      "Epoch [9/1000], Step [40/448], Loss: 0.0226\n",
      "Epoch [9/1000], Step [50/448], Loss: 0.0242\n",
      "Epoch [9/1000], Step [60/448], Loss: 0.0185\n",
      "Epoch [9/1000], Step [70/448], Loss: 0.0179\n",
      "Epoch [9/1000], Step [80/448], Loss: 0.0177\n",
      "Epoch [9/1000], Step [90/448], Loss: 0.0266\n",
      "Epoch [9/1000], Step [100/448], Loss: 0.0244\n",
      "Epoch [9/1000], Step [110/448], Loss: 0.0265\n",
      "Epoch [9/1000], Step [120/448], Loss: 0.0167\n",
      "Epoch [9/1000], Step [130/448], Loss: 0.0199\n",
      "Epoch [9/1000], Step [140/448], Loss: 0.0210\n",
      "Epoch [9/1000], Step [150/448], Loss: 0.0321\n",
      "Epoch [9/1000], Step [160/448], Loss: 0.0204\n",
      "Epoch [9/1000], Step [170/448], Loss: 0.0247\n",
      "Epoch [9/1000], Step [180/448], Loss: 0.0251\n",
      "Epoch [9/1000], Step [190/448], Loss: 0.0272\n",
      "Epoch [9/1000], Step [200/448], Loss: 0.0283\n",
      "Epoch [9/1000], Step [210/448], Loss: 0.0308\n",
      "Epoch [9/1000], Step [220/448], Loss: 0.0198\n",
      "Epoch [9/1000], Step [230/448], Loss: 0.0239\n",
      "Epoch [9/1000], Step [240/448], Loss: 0.0248\n",
      "Epoch [9/1000], Step [250/448], Loss: 0.0200\n",
      "Epoch [9/1000], Step [260/448], Loss: 0.0203\n",
      "Epoch [9/1000], Step [270/448], Loss: 0.0215\n",
      "Epoch [9/1000], Step [280/448], Loss: 0.0220\n",
      "Epoch [9/1000], Step [290/448], Loss: 0.0225\n",
      "Epoch [9/1000], Step [300/448], Loss: 0.0240\n",
      "Epoch [9/1000], Step [310/448], Loss: 0.0224\n",
      "Epoch [9/1000], Step [320/448], Loss: 0.0257\n",
      "Epoch [9/1000], Step [330/448], Loss: 0.0271\n",
      "Epoch [9/1000], Step [340/448], Loss: 0.0272\n",
      "Epoch [9/1000], Step [350/448], Loss: 0.0288\n",
      "Epoch [9/1000], Step [360/448], Loss: 0.0331\n",
      "Epoch [9/1000], Step [370/448], Loss: 0.0194\n",
      "Epoch [9/1000], Step [380/448], Loss: 0.0254\n",
      "Epoch [9/1000], Step [390/448], Loss: 0.0235\n",
      "Epoch [9/1000], Step [400/448], Loss: 0.0151\n",
      "Epoch [9/1000], Step [410/448], Loss: 0.0241\n",
      "Epoch [9/1000], Step [420/448], Loss: 0.0255\n",
      "Epoch [9/1000], Step [430/448], Loss: 0.0289\n",
      "Epoch [9/1000], Step [440/448], Loss: 0.0266\n",
      "Epoch [9/1000], Average Loss: 0.0243\n",
      "Epoch [9/1000], Validation Loss: 0.0254\n",
      "Epoch [10/1000], Step [10/448], Loss: 0.0204\n",
      "Epoch [10/1000], Step [20/448], Loss: 0.0252\n",
      "Epoch [10/1000], Step [30/448], Loss: 0.0208\n",
      "Epoch [10/1000], Step [40/448], Loss: 0.0289\n",
      "Epoch [10/1000], Step [50/448], Loss: 0.0211\n",
      "Epoch [10/1000], Step [60/448], Loss: 0.0243\n",
      "Epoch [10/1000], Step [70/448], Loss: 0.0284\n",
      "Epoch [10/1000], Step [80/448], Loss: 0.0262\n",
      "Epoch [10/1000], Step [90/448], Loss: 0.0210\n",
      "Epoch [10/1000], Step [100/448], Loss: 0.0249\n",
      "Epoch [10/1000], Step [110/448], Loss: 0.0190\n",
      "Epoch [10/1000], Step [120/448], Loss: 0.0269\n",
      "Epoch [10/1000], Step [130/448], Loss: 0.0243\n",
      "Epoch [10/1000], Step [140/448], Loss: 0.0229\n",
      "Epoch [10/1000], Step [150/448], Loss: 0.0268\n",
      "Epoch [10/1000], Step [160/448], Loss: 0.0240\n",
      "Epoch [10/1000], Step [170/448], Loss: 0.0220\n",
      "Epoch [10/1000], Step [180/448], Loss: 0.0191\n",
      "Epoch [10/1000], Step [190/448], Loss: 0.0163\n",
      "Epoch [10/1000], Step [200/448], Loss: 0.0267\n",
      "Epoch [10/1000], Step [210/448], Loss: 0.0266\n",
      "Epoch [10/1000], Step [220/448], Loss: 0.0154\n",
      "Epoch [10/1000], Step [230/448], Loss: 0.0229\n",
      "Epoch [10/1000], Step [240/448], Loss: 0.0279\n",
      "Epoch [10/1000], Step [250/448], Loss: 0.0203\n",
      "Epoch [10/1000], Step [260/448], Loss: 0.0315\n",
      "Epoch [10/1000], Step [270/448], Loss: 0.0321\n",
      "Epoch [10/1000], Step [280/448], Loss: 0.0288\n",
      "Epoch [10/1000], Step [290/448], Loss: 0.0201\n",
      "Epoch [10/1000], Step [300/448], Loss: 0.0243\n",
      "Epoch [10/1000], Step [310/448], Loss: 0.0210\n",
      "Epoch [10/1000], Step [320/448], Loss: 0.0210\n",
      "Epoch [10/1000], Step [330/448], Loss: 0.0164\n",
      "Epoch [10/1000], Step [340/448], Loss: 0.0176\n",
      "Epoch [10/1000], Step [350/448], Loss: 0.0229\n",
      "Epoch [10/1000], Step [360/448], Loss: 0.0241\n",
      "Epoch [10/1000], Step [370/448], Loss: 0.0215\n",
      "Epoch [10/1000], Step [380/448], Loss: 0.0224\n",
      "Epoch [10/1000], Step [390/448], Loss: 0.0240\n",
      "Epoch [10/1000], Step [400/448], Loss: 0.0208\n",
      "Epoch [10/1000], Step [410/448], Loss: 0.0299\n",
      "Epoch [10/1000], Step [420/448], Loss: 0.0358\n",
      "Epoch [10/1000], Step [430/448], Loss: 0.0250\n",
      "Epoch [10/1000], Step [440/448], Loss: 0.0271\n",
      "Epoch [10/1000], Average Loss: 0.0237\n",
      "Epoch [10/1000], Validation Loss: 0.0250\n",
      "Epoch [11/1000], Step [10/448], Loss: 0.0156\n",
      "Epoch [11/1000], Step [20/448], Loss: 0.0230\n",
      "Epoch [11/1000], Step [30/448], Loss: 0.0169\n",
      "Epoch [11/1000], Step [40/448], Loss: 0.0213\n",
      "Epoch [11/1000], Step [50/448], Loss: 0.0229\n",
      "Epoch [11/1000], Step [60/448], Loss: 0.0183\n",
      "Epoch [11/1000], Step [70/448], Loss: 0.0281\n",
      "Epoch [11/1000], Step [80/448], Loss: 0.0219\n",
      "Epoch [11/1000], Step [90/448], Loss: 0.0199\n",
      "Epoch [11/1000], Step [100/448], Loss: 0.0216\n",
      "Epoch [11/1000], Step [110/448], Loss: 0.0193\n",
      "Epoch [11/1000], Step [120/448], Loss: 0.0265\n",
      "Epoch [11/1000], Step [130/448], Loss: 0.0233\n",
      "Epoch [11/1000], Step [140/448], Loss: 0.0202\n",
      "Epoch [11/1000], Step [150/448], Loss: 0.0166\n",
      "Epoch [11/1000], Step [160/448], Loss: 0.0305\n",
      "Epoch [11/1000], Step [170/448], Loss: 0.0248\n",
      "Epoch [11/1000], Step [180/448], Loss: 0.0266\n",
      "Epoch [11/1000], Step [190/448], Loss: 0.0199\n",
      "Epoch [11/1000], Step [200/448], Loss: 0.0236\n",
      "Epoch [11/1000], Step [210/448], Loss: 0.0295\n",
      "Epoch [11/1000], Step [220/448], Loss: 0.0315\n",
      "Epoch [11/1000], Step [230/448], Loss: 0.0221\n",
      "Epoch [11/1000], Step [240/448], Loss: 0.0246\n",
      "Epoch [11/1000], Step [250/448], Loss: 0.0243\n",
      "Epoch [11/1000], Step [260/448], Loss: 0.0224\n",
      "Epoch [11/1000], Step [270/448], Loss: 0.0314\n",
      "Epoch [11/1000], Step [280/448], Loss: 0.0219\n",
      "Epoch [11/1000], Step [290/448], Loss: 0.0224\n",
      "Epoch [11/1000], Step [300/448], Loss: 0.0261\n",
      "Epoch [11/1000], Step [310/448], Loss: 0.0194\n",
      "Epoch [11/1000], Step [320/448], Loss: 0.0162\n",
      "Epoch [11/1000], Step [330/448], Loss: 0.0178\n",
      "Epoch [11/1000], Step [340/448], Loss: 0.0225\n",
      "Epoch [11/1000], Step [350/448], Loss: 0.0181\n",
      "Epoch [11/1000], Step [360/448], Loss: 0.0245\n",
      "Epoch [11/1000], Step [370/448], Loss: 0.0241\n",
      "Epoch [11/1000], Step [380/448], Loss: 0.0204\n",
      "Epoch [11/1000], Step [390/448], Loss: 0.0222\n",
      "Epoch [11/1000], Step [400/448], Loss: 0.0224\n",
      "Epoch [11/1000], Step [410/448], Loss: 0.0275\n",
      "Epoch [11/1000], Step [420/448], Loss: 0.0239\n",
      "Epoch [11/1000], Step [430/448], Loss: 0.0225\n",
      "Epoch [11/1000], Step [440/448], Loss: 0.0190\n",
      "Epoch [11/1000], Average Loss: 0.0233\n",
      "Epoch [11/1000], Validation Loss: 0.0245\n",
      "Epoch [12/1000], Step [10/448], Loss: 0.0277\n",
      "Epoch [12/1000], Step [20/448], Loss: 0.0264\n",
      "Epoch [12/1000], Step [30/448], Loss: 0.0160\n",
      "Epoch [12/1000], Step [40/448], Loss: 0.0227\n",
      "Epoch [12/1000], Step [50/448], Loss: 0.0198\n",
      "Epoch [12/1000], Step [60/448], Loss: 0.0277\n",
      "Epoch [12/1000], Step [70/448], Loss: 0.0255\n",
      "Epoch [12/1000], Step [80/448], Loss: 0.0309\n",
      "Epoch [12/1000], Step [90/448], Loss: 0.0336\n",
      "Epoch [12/1000], Step [100/448], Loss: 0.0273\n",
      "Epoch [12/1000], Step [110/448], Loss: 0.0146\n",
      "Epoch [12/1000], Step [120/448], Loss: 0.0239\n",
      "Epoch [12/1000], Step [130/448], Loss: 0.0208\n",
      "Epoch [12/1000], Step [140/448], Loss: 0.0240\n",
      "Epoch [12/1000], Step [150/448], Loss: 0.0154\n",
      "Epoch [12/1000], Step [160/448], Loss: 0.0287\n",
      "Epoch [12/1000], Step [170/448], Loss: 0.0206\n",
      "Epoch [12/1000], Step [180/448], Loss: 0.0309\n",
      "Epoch [12/1000], Step [190/448], Loss: 0.0311\n",
      "Epoch [12/1000], Step [200/448], Loss: 0.0165\n",
      "Epoch [12/1000], Step [210/448], Loss: 0.0219\n",
      "Epoch [12/1000], Step [220/448], Loss: 0.0149\n",
      "Epoch [12/1000], Step [230/448], Loss: 0.0213\n",
      "Epoch [12/1000], Step [240/448], Loss: 0.0195\n",
      "Epoch [12/1000], Step [250/448], Loss: 0.0210\n",
      "Epoch [12/1000], Step [260/448], Loss: 0.0211\n",
      "Epoch [12/1000], Step [270/448], Loss: 0.0308\n",
      "Epoch [12/1000], Step [280/448], Loss: 0.0193\n",
      "Epoch [12/1000], Step [290/448], Loss: 0.0309\n",
      "Epoch [12/1000], Step [300/448], Loss: 0.0195\n",
      "Epoch [12/1000], Step [310/448], Loss: 0.0200\n",
      "Epoch [12/1000], Step [320/448], Loss: 0.0257\n",
      "Epoch [12/1000], Step [330/448], Loss: 0.0207\n",
      "Epoch [12/1000], Step [340/448], Loss: 0.0318\n",
      "Epoch [12/1000], Step [350/448], Loss: 0.0244\n",
      "Epoch [12/1000], Step [360/448], Loss: 0.0269\n",
      "Epoch [12/1000], Step [370/448], Loss: 0.0261\n",
      "Epoch [12/1000], Step [380/448], Loss: 0.0249\n",
      "Epoch [12/1000], Step [390/448], Loss: 0.0245\n",
      "Epoch [12/1000], Step [400/448], Loss: 0.0200\n",
      "Epoch [12/1000], Step [410/448], Loss: 0.0186\n",
      "Epoch [12/1000], Step [420/448], Loss: 0.0284\n",
      "Epoch [12/1000], Step [430/448], Loss: 0.0183\n",
      "Epoch [12/1000], Step [440/448], Loss: 0.0230\n",
      "Epoch [12/1000], Average Loss: 0.0229\n",
      "Epoch [12/1000], Validation Loss: 0.0231\n",
      "Epoch [13/1000], Step [10/448], Loss: 0.0224\n",
      "Epoch [13/1000], Step [20/448], Loss: 0.0138\n",
      "Epoch [13/1000], Step [30/448], Loss: 0.0257\n",
      "Epoch [13/1000], Step [40/448], Loss: 0.0227\n",
      "Epoch [13/1000], Step [50/448], Loss: 0.0191\n",
      "Epoch [13/1000], Step [60/448], Loss: 0.0256\n",
      "Epoch [13/1000], Step [70/448], Loss: 0.0294\n",
      "Epoch [13/1000], Step [80/448], Loss: 0.0246\n",
      "Epoch [13/1000], Step [90/448], Loss: 0.0277\n",
      "Epoch [13/1000], Step [100/448], Loss: 0.0183\n",
      "Epoch [13/1000], Step [110/448], Loss: 0.0180\n",
      "Epoch [13/1000], Step [120/448], Loss: 0.0226\n",
      "Epoch [13/1000], Step [130/448], Loss: 0.0174\n",
      "Epoch [13/1000], Step [140/448], Loss: 0.0220\n",
      "Epoch [13/1000], Step [150/448], Loss: 0.0250\n",
      "Epoch [13/1000], Step [160/448], Loss: 0.0313\n",
      "Epoch [13/1000], Step [170/448], Loss: 0.0238\n",
      "Epoch [13/1000], Step [180/448], Loss: 0.0162\n",
      "Epoch [13/1000], Step [190/448], Loss: 0.0335\n",
      "Epoch [13/1000], Step [200/448], Loss: 0.0199\n",
      "Epoch [13/1000], Step [210/448], Loss: 0.0194\n",
      "Epoch [13/1000], Step [220/448], Loss: 0.0300\n",
      "Epoch [13/1000], Step [230/448], Loss: 0.0228\n",
      "Epoch [13/1000], Step [240/448], Loss: 0.0225\n",
      "Epoch [13/1000], Step [250/448], Loss: 0.0216\n",
      "Epoch [13/1000], Step [260/448], Loss: 0.0236\n",
      "Epoch [13/1000], Step [270/448], Loss: 0.0203\n",
      "Epoch [13/1000], Step [280/448], Loss: 0.0154\n",
      "Epoch [13/1000], Step [290/448], Loss: 0.0276\n",
      "Epoch [13/1000], Step [300/448], Loss: 0.0189\n",
      "Epoch [13/1000], Step [310/448], Loss: 0.0264\n",
      "Epoch [13/1000], Step [320/448], Loss: 0.0245\n",
      "Epoch [13/1000], Step [330/448], Loss: 0.0220\n",
      "Epoch [13/1000], Step [340/448], Loss: 0.0201\n",
      "Epoch [13/1000], Step [350/448], Loss: 0.0188\n",
      "Epoch [13/1000], Step [360/448], Loss: 0.0227\n",
      "Epoch [13/1000], Step [370/448], Loss: 0.0294\n",
      "Epoch [13/1000], Step [380/448], Loss: 0.0232\n",
      "Epoch [13/1000], Step [390/448], Loss: 0.0180\n",
      "Epoch [13/1000], Step [400/448], Loss: 0.0254\n",
      "Epoch [13/1000], Step [410/448], Loss: 0.0230\n",
      "Epoch [13/1000], Step [420/448], Loss: 0.0210\n",
      "Epoch [13/1000], Step [430/448], Loss: 0.0218\n",
      "Epoch [13/1000], Step [440/448], Loss: 0.0198\n",
      "Epoch [13/1000], Average Loss: 0.0221\n",
      "Epoch [13/1000], Validation Loss: 0.0238\n",
      "Epoch [14/1000], Step [10/448], Loss: 0.0205\n",
      "Epoch [14/1000], Step [20/448], Loss: 0.0187\n",
      "Epoch [14/1000], Step [30/448], Loss: 0.0173\n",
      "Epoch [14/1000], Step [40/448], Loss: 0.0253\n",
      "Epoch [14/1000], Step [50/448], Loss: 0.0208\n",
      "Epoch [14/1000], Step [60/448], Loss: 0.0203\n",
      "Epoch [14/1000], Step [70/448], Loss: 0.0296\n",
      "Epoch [14/1000], Step [80/448], Loss: 0.0234\n",
      "Epoch [14/1000], Step [90/448], Loss: 0.0235\n",
      "Epoch [14/1000], Step [100/448], Loss: 0.0229\n",
      "Epoch [14/1000], Step [110/448], Loss: 0.0213\n",
      "Epoch [14/1000], Step [120/448], Loss: 0.0247\n",
      "Epoch [14/1000], Step [130/448], Loss: 0.0263\n",
      "Epoch [14/1000], Step [140/448], Loss: 0.0273\n",
      "Epoch [14/1000], Step [150/448], Loss: 0.0128\n",
      "Epoch [14/1000], Step [160/448], Loss: 0.0300\n",
      "Epoch [14/1000], Step [170/448], Loss: 0.0240\n",
      "Epoch [14/1000], Step [180/448], Loss: 0.0173\n",
      "Epoch [14/1000], Step [190/448], Loss: 0.0199\n",
      "Epoch [14/1000], Step [200/448], Loss: 0.0205\n",
      "Epoch [14/1000], Step [210/448], Loss: 0.0342\n",
      "Epoch [14/1000], Step [220/448], Loss: 0.0192\n",
      "Epoch [14/1000], Step [230/448], Loss: 0.0143\n",
      "Epoch [14/1000], Step [240/448], Loss: 0.0252\n",
      "Epoch [14/1000], Step [250/448], Loss: 0.0252\n",
      "Epoch [14/1000], Step [260/448], Loss: 0.0212\n",
      "Epoch [14/1000], Step [270/448], Loss: 0.0245\n",
      "Epoch [14/1000], Step [280/448], Loss: 0.0220\n",
      "Epoch [14/1000], Step [290/448], Loss: 0.0206\n",
      "Epoch [14/1000], Step [300/448], Loss: 0.0209\n",
      "Epoch [14/1000], Step [310/448], Loss: 0.0192\n",
      "Epoch [14/1000], Step [320/448], Loss: 0.0190\n",
      "Epoch [14/1000], Step [330/448], Loss: 0.0271\n",
      "Epoch [14/1000], Step [340/448], Loss: 0.0279\n",
      "Epoch [14/1000], Step [350/448], Loss: 0.0198\n",
      "Epoch [14/1000], Step [360/448], Loss: 0.0234\n",
      "Epoch [14/1000], Step [370/448], Loss: 0.0235\n",
      "Epoch [14/1000], Step [380/448], Loss: 0.0179\n",
      "Epoch [14/1000], Step [390/448], Loss: 0.0159\n",
      "Epoch [14/1000], Step [400/448], Loss: 0.0223\n",
      "Epoch [14/1000], Step [410/448], Loss: 0.0293\n",
      "Epoch [14/1000], Step [420/448], Loss: 0.0207\n",
      "Epoch [14/1000], Step [430/448], Loss: 0.0232\n",
      "Epoch [14/1000], Step [440/448], Loss: 0.0195\n",
      "Epoch [14/1000], Average Loss: 0.0223\n",
      "Epoch [14/1000], Validation Loss: 0.0235\n",
      "Epoch [15/1000], Step [10/448], Loss: 0.0164\n",
      "Epoch [15/1000], Step [20/448], Loss: 0.0173\n",
      "Epoch [15/1000], Step [30/448], Loss: 0.0148\n",
      "Epoch [15/1000], Step [40/448], Loss: 0.0218\n",
      "Epoch [15/1000], Step [50/448], Loss: 0.0212\n",
      "Epoch [15/1000], Step [60/448], Loss: 0.0153\n",
      "Epoch [15/1000], Step [70/448], Loss: 0.0264\n",
      "Epoch [15/1000], Step [80/448], Loss: 0.0234\n",
      "Epoch [15/1000], Step [90/448], Loss: 0.0147\n",
      "Epoch [15/1000], Step [100/448], Loss: 0.0208\n",
      "Epoch [15/1000], Step [110/448], Loss: 0.0111\n",
      "Epoch [15/1000], Step [120/448], Loss: 0.0236\n",
      "Epoch [15/1000], Step [130/448], Loss: 0.0186\n",
      "Epoch [15/1000], Step [140/448], Loss: 0.0263\n",
      "Epoch [15/1000], Step [150/448], Loss: 0.0201\n",
      "Epoch [15/1000], Step [160/448], Loss: 0.0283\n",
      "Epoch [15/1000], Step [170/448], Loss: 0.0176\n",
      "Epoch [15/1000], Step [180/448], Loss: 0.0177\n",
      "Epoch [15/1000], Step [190/448], Loss: 0.0167\n",
      "Epoch [15/1000], Step [200/448], Loss: 0.0198\n",
      "Epoch [15/1000], Step [210/448], Loss: 0.0252\n",
      "Epoch [15/1000], Step [220/448], Loss: 0.0242\n",
      "Epoch [15/1000], Step [230/448], Loss: 0.0188\n",
      "Epoch [15/1000], Step [240/448], Loss: 0.0259\n",
      "Epoch [15/1000], Step [250/448], Loss: 0.0205\n",
      "Epoch [15/1000], Step [260/448], Loss: 0.0298\n",
      "Epoch [15/1000], Step [270/448], Loss: 0.0167\n",
      "Epoch [15/1000], Step [280/448], Loss: 0.0270\n",
      "Epoch [15/1000], Step [290/448], Loss: 0.0257\n",
      "Epoch [15/1000], Step [300/448], Loss: 0.0217\n",
      "Epoch [15/1000], Step [310/448], Loss: 0.0157\n",
      "Epoch [15/1000], Step [320/448], Loss: 0.0235\n",
      "Epoch [15/1000], Step [330/448], Loss: 0.0201\n",
      "Epoch [15/1000], Step [340/448], Loss: 0.0219\n",
      "Epoch [15/1000], Step [350/448], Loss: 0.0177\n",
      "Epoch [15/1000], Step [360/448], Loss: 0.0213\n",
      "Epoch [15/1000], Step [370/448], Loss: 0.0209\n",
      "Epoch [15/1000], Step [380/448], Loss: 0.0379\n",
      "Epoch [15/1000], Step [390/448], Loss: 0.0276\n",
      "Epoch [15/1000], Step [400/448], Loss: 0.0159\n",
      "Epoch [15/1000], Step [410/448], Loss: 0.0210\n",
      "Epoch [15/1000], Step [420/448], Loss: 0.0257\n",
      "Epoch [15/1000], Step [430/448], Loss: 0.0167\n",
      "Epoch [15/1000], Step [440/448], Loss: 0.0286\n",
      "Epoch [15/1000], Average Loss: 0.0211\n",
      "Epoch [15/1000], Validation Loss: 0.0226\n",
      "Epoch [16/1000], Step [10/448], Loss: 0.0150\n",
      "Epoch [16/1000], Step [20/448], Loss: 0.0264\n",
      "Epoch [16/1000], Step [30/448], Loss: 0.0178\n",
      "Epoch [16/1000], Step [40/448], Loss: 0.0235\n",
      "Epoch [16/1000], Step [50/448], Loss: 0.0208\n",
      "Epoch [16/1000], Step [60/448], Loss: 0.0198\n",
      "Epoch [16/1000], Step [70/448], Loss: 0.0205\n",
      "Epoch [16/1000], Step [80/448], Loss: 0.0242\n",
      "Epoch [16/1000], Step [90/448], Loss: 0.0232\n",
      "Epoch [16/1000], Step [100/448], Loss: 0.0207\n",
      "Epoch [16/1000], Step [110/448], Loss: 0.0150\n",
      "Epoch [16/1000], Step [120/448], Loss: 0.0169\n",
      "Epoch [16/1000], Step [130/448], Loss: 0.0185\n",
      "Epoch [16/1000], Step [140/448], Loss: 0.0187\n",
      "Epoch [16/1000], Step [150/448], Loss: 0.0133\n",
      "Epoch [16/1000], Step [160/448], Loss: 0.0263\n",
      "Epoch [16/1000], Step [170/448], Loss: 0.0291\n",
      "Epoch [16/1000], Step [180/448], Loss: 0.0269\n",
      "Epoch [16/1000], Step [190/448], Loss: 0.0230\n",
      "Epoch [16/1000], Step [200/448], Loss: 0.0292\n",
      "Epoch [16/1000], Step [210/448], Loss: 0.0257\n",
      "Epoch [16/1000], Step [220/448], Loss: 0.0192\n",
      "Epoch [16/1000], Step [230/448], Loss: 0.0255\n",
      "Epoch [16/1000], Step [240/448], Loss: 0.0208\n",
      "Epoch [16/1000], Step [250/448], Loss: 0.0157\n",
      "Epoch [16/1000], Step [260/448], Loss: 0.0277\n",
      "Epoch [16/1000], Step [270/448], Loss: 0.0222\n",
      "Epoch [16/1000], Step [280/448], Loss: 0.0228\n",
      "Epoch [16/1000], Step [290/448], Loss: 0.0156\n",
      "Epoch [16/1000], Step [300/448], Loss: 0.0255\n",
      "Epoch [16/1000], Step [310/448], Loss: 0.0198\n",
      "Epoch [16/1000], Step [320/448], Loss: 0.0285\n",
      "Epoch [16/1000], Step [330/448], Loss: 0.0303\n",
      "Epoch [16/1000], Step [340/448], Loss: 0.0279\n",
      "Epoch [16/1000], Step [350/448], Loss: 0.0247\n",
      "Epoch [16/1000], Step [360/448], Loss: 0.0285\n",
      "Epoch [16/1000], Step [370/448], Loss: 0.0223\n",
      "Epoch [16/1000], Step [380/448], Loss: 0.0196\n",
      "Epoch [16/1000], Step [390/448], Loss: 0.0218\n",
      "Epoch [16/1000], Step [400/448], Loss: 0.0179\n",
      "Epoch [16/1000], Step [410/448], Loss: 0.0214\n",
      "Epoch [16/1000], Step [420/448], Loss: 0.0234\n",
      "Epoch [16/1000], Step [430/448], Loss: 0.0178\n",
      "Epoch [16/1000], Step [440/448], Loss: 0.0195\n",
      "Epoch [16/1000], Average Loss: 0.0212\n",
      "Epoch [16/1000], Validation Loss: 0.0239\n",
      "Epoch [17/1000], Step [10/448], Loss: 0.0172\n",
      "Epoch [17/1000], Step [20/448], Loss: 0.0334\n",
      "Epoch [17/1000], Step [30/448], Loss: 0.0225\n",
      "Epoch [17/1000], Step [40/448], Loss: 0.0198\n",
      "Epoch [17/1000], Step [50/448], Loss: 0.0308\n",
      "Epoch [17/1000], Step [60/448], Loss: 0.0214\n",
      "Epoch [17/1000], Step [70/448], Loss: 0.0221\n",
      "Epoch [17/1000], Step [80/448], Loss: 0.0134\n",
      "Epoch [17/1000], Step [90/448], Loss: 0.0186\n",
      "Epoch [17/1000], Step [100/448], Loss: 0.0194\n",
      "Epoch [17/1000], Step [110/448], Loss: 0.0156\n",
      "Epoch [17/1000], Step [120/448], Loss: 0.0236\n",
      "Epoch [17/1000], Step [130/448], Loss: 0.0158\n",
      "Epoch [17/1000], Step [140/448], Loss: 0.0241\n",
      "Epoch [17/1000], Step [150/448], Loss: 0.0227\n",
      "Epoch [17/1000], Step [160/448], Loss: 0.0243\n",
      "Epoch [17/1000], Step [170/448], Loss: 0.0271\n",
      "Epoch [17/1000], Step [180/448], Loss: 0.0210\n",
      "Epoch [17/1000], Step [190/448], Loss: 0.0167\n",
      "Epoch [17/1000], Step [200/448], Loss: 0.0151\n",
      "Epoch [17/1000], Step [210/448], Loss: 0.0173\n",
      "Epoch [17/1000], Step [220/448], Loss: 0.0186\n",
      "Epoch [17/1000], Step [230/448], Loss: 0.0263\n",
      "Epoch [17/1000], Step [240/448], Loss: 0.0186\n",
      "Epoch [17/1000], Step [250/448], Loss: 0.0194\n",
      "Epoch [17/1000], Step [260/448], Loss: 0.0140\n",
      "Epoch [17/1000], Step [270/448], Loss: 0.0192\n",
      "Epoch [17/1000], Step [280/448], Loss: 0.0217\n",
      "Epoch [17/1000], Step [290/448], Loss: 0.0194\n",
      "Epoch [17/1000], Step [300/448], Loss: 0.0200\n",
      "Epoch [17/1000], Step [310/448], Loss: 0.0204\n",
      "Epoch [17/1000], Step [320/448], Loss: 0.0226\n",
      "Epoch [17/1000], Step [330/448], Loss: 0.0173\n",
      "Epoch [17/1000], Step [340/448], Loss: 0.0240\n",
      "Epoch [17/1000], Step [350/448], Loss: 0.0209\n",
      "Epoch [17/1000], Step [360/448], Loss: 0.0241\n",
      "Epoch [17/1000], Step [370/448], Loss: 0.0189\n",
      "Epoch [17/1000], Step [380/448], Loss: 0.0211\n",
      "Epoch [17/1000], Step [390/448], Loss: 0.0161\n",
      "Epoch [17/1000], Step [400/448], Loss: 0.0202\n",
      "Epoch [17/1000], Step [410/448], Loss: 0.0221\n",
      "Epoch [17/1000], Step [420/448], Loss: 0.0158\n",
      "Epoch [17/1000], Step [430/448], Loss: 0.0204\n",
      "Epoch [17/1000], Step [440/448], Loss: 0.0228\n",
      "Epoch [17/1000], Average Loss: 0.0204\n",
      "Epoch [17/1000], Validation Loss: 0.0231\n",
      "Epoch [18/1000], Step [10/448], Loss: 0.0282\n",
      "Epoch [18/1000], Step [20/448], Loss: 0.0298\n",
      "Epoch [18/1000], Step [30/448], Loss: 0.0118\n",
      "Epoch [18/1000], Step [40/448], Loss: 0.0209\n",
      "Epoch [18/1000], Step [50/448], Loss: 0.0249\n",
      "Epoch [18/1000], Step [60/448], Loss: 0.0191\n",
      "Epoch [18/1000], Step [70/448], Loss: 0.0167\n",
      "Epoch [18/1000], Step [80/448], Loss: 0.0198\n",
      "Epoch [18/1000], Step [90/448], Loss: 0.0199\n",
      "Epoch [18/1000], Step [100/448], Loss: 0.0204\n",
      "Epoch [18/1000], Step [110/448], Loss: 0.0158\n",
      "Epoch [18/1000], Step [120/448], Loss: 0.0179\n",
      "Epoch [18/1000], Step [130/448], Loss: 0.0226\n",
      "Epoch [18/1000], Step [140/448], Loss: 0.0152\n",
      "Epoch [18/1000], Step [150/448], Loss: 0.0206\n",
      "Epoch [18/1000], Step [160/448], Loss: 0.0173\n",
      "Epoch [18/1000], Step [170/448], Loss: 0.0172\n",
      "Epoch [18/1000], Step [180/448], Loss: 0.0208\n",
      "Epoch [18/1000], Step [190/448], Loss: 0.0250\n",
      "Epoch [18/1000], Step [200/448], Loss: 0.0206\n",
      "Epoch [18/1000], Step [210/448], Loss: 0.0224\n",
      "Epoch [18/1000], Step [220/448], Loss: 0.0265\n",
      "Epoch [18/1000], Step [230/448], Loss: 0.0195\n",
      "Epoch [18/1000], Step [240/448], Loss: 0.0188\n",
      "Epoch [18/1000], Step [250/448], Loss: 0.0195\n",
      "Epoch [18/1000], Step [260/448], Loss: 0.0155\n",
      "Epoch [18/1000], Step [270/448], Loss: 0.0186\n",
      "Epoch [18/1000], Step [280/448], Loss: 0.0322\n",
      "Epoch [18/1000], Step [290/448], Loss: 0.0190\n",
      "Epoch [18/1000], Step [300/448], Loss: 0.0233\n",
      "Epoch [18/1000], Step [310/448], Loss: 0.0266\n",
      "Epoch [18/1000], Step [320/448], Loss: 0.0281\n",
      "Epoch [18/1000], Step [330/448], Loss: 0.0154\n",
      "Epoch [18/1000], Step [340/448], Loss: 0.0139\n",
      "Epoch [18/1000], Step [350/448], Loss: 0.0250\n",
      "Epoch [18/1000], Step [360/448], Loss: 0.0232\n",
      "Epoch [18/1000], Step [370/448], Loss: 0.0234\n",
      "Epoch [18/1000], Step [380/448], Loss: 0.0281\n",
      "Epoch [18/1000], Step [390/448], Loss: 0.0184\n",
      "Epoch [18/1000], Step [400/448], Loss: 0.0136\n",
      "Epoch [18/1000], Step [410/448], Loss: 0.0192\n",
      "Epoch [18/1000], Step [420/448], Loss: 0.0140\n",
      "Epoch [18/1000], Step [430/448], Loss: 0.0148\n",
      "Epoch [18/1000], Step [440/448], Loss: 0.0265\n",
      "Epoch [18/1000], Average Loss: 0.0204\n",
      "Epoch [18/1000], Validation Loss: 0.0224\n",
      "Epoch [19/1000], Step [10/448], Loss: 0.0176\n",
      "Epoch [19/1000], Step [20/448], Loss: 0.0235\n",
      "Epoch [19/1000], Step [30/448], Loss: 0.0200\n",
      "Epoch [19/1000], Step [40/448], Loss: 0.0148\n",
      "Epoch [19/1000], Step [50/448], Loss: 0.0204\n",
      "Epoch [19/1000], Step [60/448], Loss: 0.0283\n",
      "Epoch [19/1000], Step [70/448], Loss: 0.0225\n",
      "Epoch [19/1000], Step [80/448], Loss: 0.0139\n",
      "Epoch [19/1000], Step [90/448], Loss: 0.0208\n",
      "Epoch [19/1000], Step [100/448], Loss: 0.0182\n",
      "Epoch [19/1000], Step [110/448], Loss: 0.0188\n",
      "Epoch [19/1000], Step [120/448], Loss: 0.0147\n",
      "Epoch [19/1000], Step [130/448], Loss: 0.0287\n",
      "Epoch [19/1000], Step [140/448], Loss: 0.0168\n",
      "Epoch [19/1000], Step [150/448], Loss: 0.0279\n",
      "Epoch [19/1000], Step [160/448], Loss: 0.0240\n",
      "Epoch [19/1000], Step [170/448], Loss: 0.0215\n",
      "Epoch [19/1000], Step [180/448], Loss: 0.0224\n",
      "Epoch [19/1000], Step [190/448], Loss: 0.0227\n",
      "Epoch [19/1000], Step [200/448], Loss: 0.0229\n",
      "Epoch [19/1000], Step [210/448], Loss: 0.0182\n",
      "Epoch [19/1000], Step [220/448], Loss: 0.0130\n",
      "Epoch [19/1000], Step [230/448], Loss: 0.0251\n",
      "Epoch [19/1000], Step [240/448], Loss: 0.0245\n",
      "Epoch [19/1000], Step [250/448], Loss: 0.0228\n",
      "Epoch [19/1000], Step [260/448], Loss: 0.0229\n",
      "Epoch [19/1000], Step [270/448], Loss: 0.0191\n",
      "Epoch [19/1000], Step [280/448], Loss: 0.0142\n",
      "Epoch [19/1000], Step [290/448], Loss: 0.0173\n",
      "Epoch [19/1000], Step [300/448], Loss: 0.0174\n",
      "Epoch [19/1000], Step [310/448], Loss: 0.0104\n",
      "Epoch [19/1000], Step [320/448], Loss: 0.0222\n",
      "Epoch [19/1000], Step [330/448], Loss: 0.0186\n",
      "Epoch [19/1000], Step [340/448], Loss: 0.0136\n",
      "Epoch [19/1000], Step [350/448], Loss: 0.0204\n",
      "Epoch [19/1000], Step [360/448], Loss: 0.0264\n",
      "Epoch [19/1000], Step [370/448], Loss: 0.0146\n",
      "Epoch [19/1000], Step [380/448], Loss: 0.0160\n",
      "Epoch [19/1000], Step [390/448], Loss: 0.0196\n",
      "Epoch [19/1000], Step [400/448], Loss: 0.0173\n",
      "Epoch [19/1000], Step [410/448], Loss: 0.0232\n",
      "Epoch [19/1000], Step [420/448], Loss: 0.0159\n",
      "Epoch [19/1000], Step [430/448], Loss: 0.0201\n",
      "Epoch [19/1000], Step [440/448], Loss: 0.0274\n",
      "Epoch [19/1000], Average Loss: 0.0201\n",
      "Epoch [19/1000], Validation Loss: 0.0219\n",
      "Epoch [20/1000], Step [10/448], Loss: 0.0132\n",
      "Epoch [20/1000], Step [20/448], Loss: 0.0167\n",
      "Epoch [20/1000], Step [30/448], Loss: 0.0248\n",
      "Epoch [20/1000], Step [40/448], Loss: 0.0293\n",
      "Epoch [20/1000], Step [50/448], Loss: 0.0215\n",
      "Epoch [20/1000], Step [60/448], Loss: 0.0142\n",
      "Epoch [20/1000], Step [70/448], Loss: 0.0211\n",
      "Epoch [20/1000], Step [80/448], Loss: 0.0151\n",
      "Epoch [20/1000], Step [90/448], Loss: 0.0169\n",
      "Epoch [20/1000], Step [100/448], Loss: 0.0238\n",
      "Epoch [20/1000], Step [110/448], Loss: 0.0272\n",
      "Epoch [20/1000], Step [120/448], Loss: 0.0250\n",
      "Epoch [20/1000], Step [130/448], Loss: 0.0128\n",
      "Epoch [20/1000], Step [140/448], Loss: 0.0198\n",
      "Epoch [20/1000], Step [150/448], Loss: 0.0154\n",
      "Epoch [20/1000], Step [160/448], Loss: 0.0321\n",
      "Epoch [20/1000], Step [170/448], Loss: 0.0265\n",
      "Epoch [20/1000], Step [180/448], Loss: 0.0218\n",
      "Epoch [20/1000], Step [190/448], Loss: 0.0209\n",
      "Epoch [20/1000], Step [200/448], Loss: 0.0262\n",
      "Epoch [20/1000], Step [210/448], Loss: 0.0211\n",
      "Epoch [20/1000], Step [220/448], Loss: 0.0273\n",
      "Epoch [20/1000], Step [230/448], Loss: 0.0183\n",
      "Epoch [20/1000], Step [240/448], Loss: 0.0246\n",
      "Epoch [20/1000], Step [250/448], Loss: 0.0217\n",
      "Epoch [20/1000], Step [260/448], Loss: 0.0232\n",
      "Epoch [20/1000], Step [270/448], Loss: 0.0165\n",
      "Epoch [20/1000], Step [280/448], Loss: 0.0222\n",
      "Epoch [20/1000], Step [290/448], Loss: 0.0238\n",
      "Epoch [20/1000], Step [300/448], Loss: 0.0273\n",
      "Epoch [20/1000], Step [310/448], Loss: 0.0188\n",
      "Epoch [20/1000], Step [320/448], Loss: 0.0161\n",
      "Epoch [20/1000], Step [330/448], Loss: 0.0145\n",
      "Epoch [20/1000], Step [340/448], Loss: 0.0196\n",
      "Epoch [20/1000], Step [350/448], Loss: 0.0215\n",
      "Epoch [20/1000], Step [360/448], Loss: 0.0279\n",
      "Epoch [20/1000], Step [370/448], Loss: 0.0186\n",
      "Epoch [20/1000], Step [380/448], Loss: 0.0166\n",
      "Epoch [20/1000], Step [390/448], Loss: 0.0274\n",
      "Epoch [20/1000], Step [400/448], Loss: 0.0221\n",
      "Epoch [20/1000], Step [410/448], Loss: 0.0205\n",
      "Epoch [20/1000], Step [420/448], Loss: 0.0164\n",
      "Epoch [20/1000], Step [430/448], Loss: 0.0219\n",
      "Epoch [20/1000], Step [440/448], Loss: 0.0202\n",
      "Epoch [20/1000], Average Loss: 0.0201\n",
      "Epoch [20/1000], Validation Loss: 0.0219\n",
      "Epoch [21/1000], Step [10/448], Loss: 0.0156\n",
      "Epoch [21/1000], Step [20/448], Loss: 0.0141\n",
      "Epoch [21/1000], Step [30/448], Loss: 0.0222\n",
      "Epoch [21/1000], Step [40/448], Loss: 0.0158\n",
      "Epoch [21/1000], Step [50/448], Loss: 0.0169\n",
      "Epoch [21/1000], Step [60/448], Loss: 0.0291\n",
      "Epoch [21/1000], Step [70/448], Loss: 0.0239\n",
      "Epoch [21/1000], Step [80/448], Loss: 0.0142\n",
      "Epoch [21/1000], Step [90/448], Loss: 0.0090\n",
      "Epoch [21/1000], Step [100/448], Loss: 0.0157\n",
      "Epoch [21/1000], Step [110/448], Loss: 0.0143\n",
      "Epoch [21/1000], Step [120/448], Loss: 0.0196\n",
      "Epoch [21/1000], Step [130/448], Loss: 0.0157\n",
      "Epoch [21/1000], Step [140/448], Loss: 0.0293\n",
      "Epoch [21/1000], Step [150/448], Loss: 0.0162\n",
      "Epoch [21/1000], Step [160/448], Loss: 0.0158\n",
      "Epoch [21/1000], Step [170/448], Loss: 0.0167\n",
      "Epoch [21/1000], Step [180/448], Loss: 0.0171\n",
      "Epoch [21/1000], Step [190/448], Loss: 0.0145\n",
      "Epoch [21/1000], Step [200/448], Loss: 0.0155\n",
      "Epoch [21/1000], Step [210/448], Loss: 0.0165\n",
      "Epoch [21/1000], Step [220/448], Loss: 0.0207\n",
      "Epoch [21/1000], Step [230/448], Loss: 0.0202\n",
      "Epoch [21/1000], Step [240/448], Loss: 0.0171\n",
      "Epoch [21/1000], Step [250/448], Loss: 0.0274\n",
      "Epoch [21/1000], Step [260/448], Loss: 0.0198\n",
      "Epoch [21/1000], Step [270/448], Loss: 0.0137\n",
      "Epoch [21/1000], Step [280/448], Loss: 0.0106\n",
      "Epoch [21/1000], Step [290/448], Loss: 0.0224\n",
      "Epoch [21/1000], Step [300/448], Loss: 0.0197\n",
      "Epoch [21/1000], Step [310/448], Loss: 0.0278\n",
      "Epoch [21/1000], Step [320/448], Loss: 0.0145\n",
      "Epoch [21/1000], Step [330/448], Loss: 0.0177\n",
      "Epoch [21/1000], Step [340/448], Loss: 0.0202\n",
      "Epoch [21/1000], Step [350/448], Loss: 0.0253\n",
      "Epoch [21/1000], Step [360/448], Loss: 0.0189\n",
      "Epoch [21/1000], Step [370/448], Loss: 0.0205\n",
      "Epoch [21/1000], Step [380/448], Loss: 0.0169\n",
      "Epoch [21/1000], Step [390/448], Loss: 0.0182\n",
      "Epoch [21/1000], Step [400/448], Loss: 0.0166\n",
      "Epoch [21/1000], Step [410/448], Loss: 0.0126\n",
      "Epoch [21/1000], Step [420/448], Loss: 0.0181\n",
      "Epoch [21/1000], Step [430/448], Loss: 0.0236\n",
      "Epoch [21/1000], Step [440/448], Loss: 0.0143\n",
      "Epoch [21/1000], Average Loss: 0.0194\n",
      "Epoch [21/1000], Validation Loss: 0.0216\n",
      "Epoch [22/1000], Step [10/448], Loss: 0.0199\n",
      "Epoch [22/1000], Step [20/448], Loss: 0.0221\n",
      "Epoch [22/1000], Step [30/448], Loss: 0.0214\n",
      "Epoch [22/1000], Step [40/448], Loss: 0.0186\n",
      "Epoch [22/1000], Step [50/448], Loss: 0.0246\n",
      "Epoch [22/1000], Step [60/448], Loss: 0.0196\n",
      "Epoch [22/1000], Step [70/448], Loss: 0.0244\n",
      "Epoch [22/1000], Step [80/448], Loss: 0.0172\n",
      "Epoch [22/1000], Step [90/448], Loss: 0.0233\n",
      "Epoch [22/1000], Step [100/448], Loss: 0.0202\n",
      "Epoch [22/1000], Step [110/448], Loss: 0.0157\n",
      "Epoch [22/1000], Step [120/448], Loss: 0.0221\n",
      "Epoch [22/1000], Step [130/448], Loss: 0.0198\n",
      "Epoch [22/1000], Step [140/448], Loss: 0.0215\n",
      "Epoch [22/1000], Step [150/448], Loss: 0.0148\n",
      "Epoch [22/1000], Step [160/448], Loss: 0.0190\n",
      "Epoch [22/1000], Step [170/448], Loss: 0.0145\n",
      "Epoch [22/1000], Step [180/448], Loss: 0.0191\n",
      "Epoch [22/1000], Step [190/448], Loss: 0.0208\n",
      "Epoch [22/1000], Step [200/448], Loss: 0.0212\n",
      "Epoch [22/1000], Step [210/448], Loss: 0.0211\n",
      "Epoch [22/1000], Step [220/448], Loss: 0.0203\n",
      "Epoch [22/1000], Step [230/448], Loss: 0.0196\n",
      "Epoch [22/1000], Step [240/448], Loss: 0.0195\n",
      "Epoch [22/1000], Step [250/448], Loss: 0.0203\n",
      "Epoch [22/1000], Step [260/448], Loss: 0.0231\n",
      "Epoch [22/1000], Step [270/448], Loss: 0.0164\n",
      "Epoch [22/1000], Step [280/448], Loss: 0.0172\n",
      "Epoch [22/1000], Step [290/448], Loss: 0.0166\n",
      "Epoch [22/1000], Step [300/448], Loss: 0.0159\n",
      "Epoch [22/1000], Step [310/448], Loss: 0.0199\n",
      "Epoch [22/1000], Step [320/448], Loss: 0.0156\n",
      "Epoch [22/1000], Step [330/448], Loss: 0.0205\n",
      "Epoch [22/1000], Step [340/448], Loss: 0.0186\n",
      "Epoch [22/1000], Step [350/448], Loss: 0.0222\n",
      "Epoch [22/1000], Step [360/448], Loss: 0.0162\n",
      "Epoch [22/1000], Step [370/448], Loss: 0.0165\n",
      "Epoch [22/1000], Step [380/448], Loss: 0.0082\n",
      "Epoch [22/1000], Step [390/448], Loss: 0.0218\n",
      "Epoch [22/1000], Step [400/448], Loss: 0.0225\n",
      "Epoch [22/1000], Step [410/448], Loss: 0.0205\n",
      "Epoch [22/1000], Step [420/448], Loss: 0.0159\n",
      "Epoch [22/1000], Step [430/448], Loss: 0.0191\n",
      "Epoch [22/1000], Step [440/448], Loss: 0.0232\n",
      "Epoch [22/1000], Average Loss: 0.0190\n",
      "Epoch [22/1000], Validation Loss: 0.0217\n",
      "Epoch [23/1000], Step [10/448], Loss: 0.0230\n",
      "Epoch [23/1000], Step [20/448], Loss: 0.0141\n",
      "Epoch [23/1000], Step [30/448], Loss: 0.0204\n",
      "Epoch [23/1000], Step [40/448], Loss: 0.0160\n",
      "Epoch [23/1000], Step [50/448], Loss: 0.0251\n",
      "Epoch [23/1000], Step [60/448], Loss: 0.0220\n",
      "Epoch [23/1000], Step [70/448], Loss: 0.0172\n",
      "Epoch [23/1000], Step [80/448], Loss: 0.0143\n",
      "Epoch [23/1000], Step [90/448], Loss: 0.0216\n",
      "Epoch [23/1000], Step [100/448], Loss: 0.0144\n",
      "Epoch [23/1000], Step [110/448], Loss: 0.0137\n",
      "Epoch [23/1000], Step [120/448], Loss: 0.0190\n",
      "Epoch [23/1000], Step [130/448], Loss: 0.0153\n",
      "Epoch [23/1000], Step [140/448], Loss: 0.0207\n",
      "Epoch [23/1000], Step [150/448], Loss: 0.0181\n",
      "Epoch [23/1000], Step [160/448], Loss: 0.0247\n",
      "Epoch [23/1000], Step [170/448], Loss: 0.0199\n",
      "Epoch [23/1000], Step [180/448], Loss: 0.0171\n",
      "Epoch [23/1000], Step [190/448], Loss: 0.0194\n",
      "Epoch [23/1000], Step [200/448], Loss: 0.0206\n",
      "Epoch [23/1000], Step [210/448], Loss: 0.0236\n",
      "Epoch [23/1000], Step [220/448], Loss: 0.0304\n",
      "Epoch [23/1000], Step [230/448], Loss: 0.0178\n",
      "Epoch [23/1000], Step [240/448], Loss: 0.0256\n",
      "Epoch [23/1000], Step [250/448], Loss: 0.0229\n",
      "Epoch [23/1000], Step [260/448], Loss: 0.0245\n",
      "Epoch [23/1000], Step [270/448], Loss: 0.0148\n",
      "Epoch [23/1000], Step [280/448], Loss: 0.0199\n",
      "Epoch [23/1000], Step [290/448], Loss: 0.0200\n",
      "Epoch [23/1000], Step [300/448], Loss: 0.0203\n",
      "Epoch [23/1000], Step [310/448], Loss: 0.0255\n",
      "Epoch [23/1000], Step [320/448], Loss: 0.0254\n",
      "Epoch [23/1000], Step [330/448], Loss: 0.0151\n",
      "Epoch [23/1000], Step [340/448], Loss: 0.0153\n",
      "Epoch [23/1000], Step [350/448], Loss: 0.0245\n",
      "Epoch [23/1000], Step [360/448], Loss: 0.0171\n",
      "Epoch [23/1000], Step [370/448], Loss: 0.0157\n",
      "Epoch [23/1000], Step [380/448], Loss: 0.0202\n",
      "Epoch [23/1000], Step [390/448], Loss: 0.0259\n",
      "Epoch [23/1000], Step [400/448], Loss: 0.0142\n",
      "Epoch [23/1000], Step [410/448], Loss: 0.0184\n",
      "Epoch [23/1000], Step [420/448], Loss: 0.0207\n",
      "Epoch [23/1000], Step [430/448], Loss: 0.0176\n",
      "Epoch [23/1000], Step [440/448], Loss: 0.0154\n",
      "Epoch [23/1000], Average Loss: 0.0194\n",
      "Epoch [23/1000], Validation Loss: 0.0221\n",
      "Epoch [24/1000], Step [10/448], Loss: 0.0178\n",
      "Epoch [24/1000], Step [20/448], Loss: 0.0216\n",
      "Epoch [24/1000], Step [30/448], Loss: 0.0090\n",
      "Epoch [24/1000], Step [40/448], Loss: 0.0139\n",
      "Epoch [24/1000], Step [50/448], Loss: 0.0177\n",
      "Epoch [24/1000], Step [60/448], Loss: 0.0162\n",
      "Epoch [24/1000], Step [70/448], Loss: 0.0201\n",
      "Epoch [24/1000], Step [80/448], Loss: 0.0143\n",
      "Epoch [24/1000], Step [90/448], Loss: 0.0219\n",
      "Epoch [24/1000], Step [100/448], Loss: 0.0282\n",
      "Epoch [24/1000], Step [110/448], Loss: 0.0197\n",
      "Epoch [24/1000], Step [120/448], Loss: 0.0256\n",
      "Epoch [24/1000], Step [130/448], Loss: 0.0148\n",
      "Epoch [24/1000], Step [140/448], Loss: 0.0169\n",
      "Epoch [24/1000], Step [150/448], Loss: 0.0095\n",
      "Epoch [24/1000], Step [160/448], Loss: 0.0222\n",
      "Epoch [24/1000], Step [170/448], Loss: 0.0205\n",
      "Epoch [24/1000], Step [180/448], Loss: 0.0198\n",
      "Epoch [24/1000], Step [190/448], Loss: 0.0124\n",
      "Epoch [24/1000], Step [200/448], Loss: 0.0228\n",
      "Epoch [24/1000], Step [210/448], Loss: 0.0168\n",
      "Epoch [24/1000], Step [220/448], Loss: 0.0338\n",
      "Epoch [24/1000], Step [230/448], Loss: 0.0220\n",
      "Epoch [24/1000], Step [240/448], Loss: 0.0153\n",
      "Epoch [24/1000], Step [250/448], Loss: 0.0308\n",
      "Epoch [24/1000], Step [260/448], Loss: 0.0218\n",
      "Epoch [24/1000], Step [270/448], Loss: 0.0233\n",
      "Epoch [24/1000], Step [280/448], Loss: 0.0170\n",
      "Epoch [24/1000], Step [290/448], Loss: 0.0180\n",
      "Epoch [24/1000], Step [300/448], Loss: 0.0178\n",
      "Epoch [24/1000], Step [310/448], Loss: 0.0202\n",
      "Epoch [24/1000], Step [320/448], Loss: 0.0220\n",
      "Epoch [24/1000], Step [330/448], Loss: 0.0208\n",
      "Epoch [24/1000], Step [340/448], Loss: 0.0090\n",
      "Epoch [24/1000], Step [350/448], Loss: 0.0215\n",
      "Epoch [24/1000], Step [360/448], Loss: 0.0159\n",
      "Epoch [24/1000], Step [370/448], Loss: 0.0176\n",
      "Epoch [24/1000], Step [380/448], Loss: 0.0138\n",
      "Epoch [24/1000], Step [390/448], Loss: 0.0130\n",
      "Epoch [24/1000], Step [400/448], Loss: 0.0153\n",
      "Epoch [24/1000], Step [410/448], Loss: 0.0279\n",
      "Epoch [24/1000], Step [420/448], Loss: 0.0230\n",
      "Epoch [24/1000], Step [430/448], Loss: 0.0179\n",
      "Epoch [24/1000], Step [440/448], Loss: 0.0194\n",
      "Epoch [24/1000], Average Loss: 0.0188\n",
      "Epoch [24/1000], Validation Loss: 0.0216\n",
      "Epoch [25/1000], Step [10/448], Loss: 0.0172\n",
      "Epoch [25/1000], Step [20/448], Loss: 0.0286\n",
      "Epoch [25/1000], Step [30/448], Loss: 0.0181\n",
      "Epoch [25/1000], Step [40/448], Loss: 0.0176\n",
      "Epoch [25/1000], Step [50/448], Loss: 0.0160\n",
      "Epoch [25/1000], Step [60/448], Loss: 0.0129\n",
      "Epoch [25/1000], Step [70/448], Loss: 0.0204\n",
      "Epoch [25/1000], Step [80/448], Loss: 0.0172\n",
      "Epoch [25/1000], Step [90/448], Loss: 0.0160\n",
      "Epoch [25/1000], Step [100/448], Loss: 0.0097\n",
      "Epoch [25/1000], Step [110/448], Loss: 0.0108\n",
      "Epoch [25/1000], Step [120/448], Loss: 0.0204\n",
      "Epoch [25/1000], Step [130/448], Loss: 0.0070\n",
      "Epoch [25/1000], Step [140/448], Loss: 0.0210\n",
      "Epoch [25/1000], Step [150/448], Loss: 0.0189\n",
      "Epoch [25/1000], Step [160/448], Loss: 0.0297\n",
      "Epoch [25/1000], Step [170/448], Loss: 0.0163\n",
      "Epoch [25/1000], Step [180/448], Loss: 0.0231\n",
      "Epoch [25/1000], Step [190/448], Loss: 0.0248\n",
      "Epoch [25/1000], Step [200/448], Loss: 0.0132\n",
      "Epoch [25/1000], Step [210/448], Loss: 0.0211\n",
      "Epoch [25/1000], Step [220/448], Loss: 0.0197\n",
      "Epoch [25/1000], Step [230/448], Loss: 0.0164\n",
      "Epoch [25/1000], Step [240/448], Loss: 0.0241\n",
      "Epoch [25/1000], Step [250/448], Loss: 0.0207\n",
      "Epoch [25/1000], Step [260/448], Loss: 0.0112\n",
      "Epoch [25/1000], Step [270/448], Loss: 0.0229\n",
      "Epoch [25/1000], Step [280/448], Loss: 0.0189\n",
      "Epoch [25/1000], Step [290/448], Loss: 0.0136\n",
      "Epoch [25/1000], Step [300/448], Loss: 0.0160\n",
      "Epoch [25/1000], Step [310/448], Loss: 0.0122\n",
      "Epoch [25/1000], Step [320/448], Loss: 0.0112\n",
      "Epoch [25/1000], Step [330/448], Loss: 0.0308\n",
      "Epoch [25/1000], Step [340/448], Loss: 0.0116\n",
      "Epoch [25/1000], Step [350/448], Loss: 0.0165\n",
      "Epoch [25/1000], Step [360/448], Loss: 0.0212\n",
      "Epoch [25/1000], Step [370/448], Loss: 0.0164\n",
      "Epoch [25/1000], Step [380/448], Loss: 0.0229\n",
      "Epoch [25/1000], Step [390/448], Loss: 0.0120\n",
      "Epoch [25/1000], Step [400/448], Loss: 0.0158\n",
      "Epoch [25/1000], Step [410/448], Loss: 0.0236\n",
      "Epoch [25/1000], Step [420/448], Loss: 0.0143\n",
      "Epoch [25/1000], Step [430/448], Loss: 0.0104\n",
      "Epoch [25/1000], Step [440/448], Loss: 0.0143\n",
      "Epoch [25/1000], Average Loss: 0.0186\n",
      "Epoch [25/1000], Validation Loss: 0.0215\n",
      "Epoch [26/1000], Step [10/448], Loss: 0.0132\n",
      "Epoch [26/1000], Step [20/448], Loss: 0.0235\n",
      "Epoch [26/1000], Step [30/448], Loss: 0.0179\n",
      "Epoch [26/1000], Step [40/448], Loss: 0.0195\n",
      "Epoch [26/1000], Step [50/448], Loss: 0.0150\n",
      "Epoch [26/1000], Step [60/448], Loss: 0.0287\n",
      "Epoch [26/1000], Step [70/448], Loss: 0.0172\n",
      "Epoch [26/1000], Step [80/448], Loss: 0.0200\n",
      "Epoch [26/1000], Step [90/448], Loss: 0.0200\n",
      "Epoch [26/1000], Step [100/448], Loss: 0.0177\n",
      "Epoch [26/1000], Step [110/448], Loss: 0.0121\n",
      "Epoch [26/1000], Step [120/448], Loss: 0.0162\n",
      "Epoch [26/1000], Step [130/448], Loss: 0.0125\n",
      "Epoch [26/1000], Step [140/448], Loss: 0.0209\n",
      "Epoch [26/1000], Step [150/448], Loss: 0.0259\n",
      "Epoch [26/1000], Step [160/448], Loss: 0.0237\n",
      "Epoch [26/1000], Step [170/448], Loss: 0.0206\n",
      "Epoch [26/1000], Step [180/448], Loss: 0.0107\n",
      "Epoch [26/1000], Step [190/448], Loss: 0.0194\n",
      "Epoch [26/1000], Step [200/448], Loss: 0.0213\n",
      "Epoch [26/1000], Step [210/448], Loss: 0.0155\n",
      "Epoch [26/1000], Step [220/448], Loss: 0.0263\n",
      "Epoch [26/1000], Step [230/448], Loss: 0.0227\n",
      "Epoch [26/1000], Step [240/448], Loss: 0.0183\n",
      "Epoch [26/1000], Step [250/448], Loss: 0.0228\n",
      "Epoch [26/1000], Step [260/448], Loss: 0.0147\n",
      "Epoch [26/1000], Step [270/448], Loss: 0.0104\n",
      "Epoch [26/1000], Step [280/448], Loss: 0.0149\n",
      "Epoch [26/1000], Step [290/448], Loss: 0.0255\n",
      "Epoch [26/1000], Step [300/448], Loss: 0.0190\n",
      "Epoch [26/1000], Step [310/448], Loss: 0.0101\n",
      "Epoch [26/1000], Step [320/448], Loss: 0.0248\n",
      "Epoch [26/1000], Step [330/448], Loss: 0.0271\n",
      "Epoch [26/1000], Step [340/448], Loss: 0.0224\n",
      "Epoch [26/1000], Step [350/448], Loss: 0.0186\n",
      "Epoch [26/1000], Step [360/448], Loss: 0.0182\n",
      "Epoch [26/1000], Step [370/448], Loss: 0.0192\n",
      "Epoch [26/1000], Step [380/448], Loss: 0.0278\n",
      "Epoch [26/1000], Step [390/448], Loss: 0.0154\n",
      "Epoch [26/1000], Step [400/448], Loss: 0.0200\n",
      "Epoch [26/1000], Step [410/448], Loss: 0.0298\n",
      "Epoch [26/1000], Step [420/448], Loss: 0.0184\n",
      "Epoch [26/1000], Step [430/448], Loss: 0.0195\n",
      "Epoch [26/1000], Step [440/448], Loss: 0.0183\n",
      "Epoch [26/1000], Average Loss: 0.0184\n",
      "Epoch [26/1000], Validation Loss: 0.0217\n",
      "Epoch [27/1000], Step [10/448], Loss: 0.0174\n",
      "Epoch [27/1000], Step [20/448], Loss: 0.0209\n",
      "Epoch [27/1000], Step [30/448], Loss: 0.0209\n",
      "Epoch [27/1000], Step [40/448], Loss: 0.0189\n",
      "Epoch [27/1000], Step [50/448], Loss: 0.0166\n",
      "Epoch [27/1000], Step [60/448], Loss: 0.0157\n",
      "Epoch [27/1000], Step [70/448], Loss: 0.0131\n",
      "Epoch [27/1000], Step [80/448], Loss: 0.0128\n",
      "Epoch [27/1000], Step [90/448], Loss: 0.0148\n",
      "Epoch [27/1000], Step [100/448], Loss: 0.0215\n",
      "Epoch [27/1000], Step [110/448], Loss: 0.0146\n",
      "Epoch [27/1000], Step [120/448], Loss: 0.0208\n",
      "Epoch [27/1000], Step [130/448], Loss: 0.0134\n",
      "Epoch [27/1000], Step [140/448], Loss: 0.0140\n",
      "Epoch [27/1000], Step [150/448], Loss: 0.0162\n",
      "Epoch [27/1000], Step [160/448], Loss: 0.0173\n",
      "Epoch [27/1000], Step [170/448], Loss: 0.0112\n",
      "Epoch [27/1000], Step [180/448], Loss: 0.0210\n",
      "Epoch [27/1000], Step [190/448], Loss: 0.0314\n",
      "Epoch [27/1000], Step [200/448], Loss: 0.0151\n",
      "Epoch [27/1000], Step [210/448], Loss: 0.0161\n",
      "Epoch [27/1000], Step [220/448], Loss: 0.0218\n",
      "Epoch [27/1000], Step [230/448], Loss: 0.0155\n",
      "Epoch [27/1000], Step [240/448], Loss: 0.0206\n",
      "Epoch [27/1000], Step [250/448], Loss: 0.0166\n",
      "Epoch [27/1000], Step [260/448], Loss: 0.0044\n",
      "Epoch [27/1000], Step [270/448], Loss: 0.0240\n",
      "Epoch [27/1000], Step [280/448], Loss: 0.0206\n",
      "Epoch [27/1000], Step [290/448], Loss: 0.0260\n",
      "Epoch [27/1000], Step [300/448], Loss: 0.0212\n",
      "Epoch [27/1000], Step [310/448], Loss: 0.0170\n",
      "Epoch [27/1000], Step [320/448], Loss: 0.0286\n",
      "Epoch [27/1000], Step [330/448], Loss: 0.0127\n",
      "Epoch [27/1000], Step [340/448], Loss: 0.0134\n",
      "Epoch [27/1000], Step [350/448], Loss: 0.0147\n",
      "Epoch [27/1000], Step [360/448], Loss: 0.0225\n",
      "Epoch [27/1000], Step [370/448], Loss: 0.0134\n",
      "Epoch [27/1000], Step [380/448], Loss: 0.0104\n",
      "Epoch [27/1000], Step [390/448], Loss: 0.0190\n",
      "Epoch [27/1000], Step [400/448], Loss: 0.0216\n",
      "Epoch [27/1000], Step [410/448], Loss: 0.0222\n",
      "Epoch [27/1000], Step [420/448], Loss: 0.0124\n",
      "Epoch [27/1000], Step [430/448], Loss: 0.0182\n",
      "Epoch [27/1000], Step [440/448], Loss: 0.0209\n",
      "Epoch [27/1000], Average Loss: 0.0179\n",
      "Epoch [27/1000], Validation Loss: 0.0211\n",
      "Epoch [28/1000], Step [10/448], Loss: 0.0183\n",
      "Epoch [28/1000], Step [20/448], Loss: 0.0217\n",
      "Epoch [28/1000], Step [30/448], Loss: 0.0206\n",
      "Epoch [28/1000], Step [40/448], Loss: 0.0131\n",
      "Epoch [28/1000], Step [50/448], Loss: 0.0235\n",
      "Epoch [28/1000], Step [60/448], Loss: 0.0160\n",
      "Epoch [28/1000], Step [70/448], Loss: 0.0194\n",
      "Epoch [28/1000], Step [80/448], Loss: 0.0156\n",
      "Epoch [28/1000], Step [90/448], Loss: 0.0185\n",
      "Epoch [28/1000], Step [100/448], Loss: 0.0130\n",
      "Epoch [28/1000], Step [110/448], Loss: 0.0150\n",
      "Epoch [28/1000], Step [120/448], Loss: 0.0262\n",
      "Epoch [28/1000], Step [130/448], Loss: 0.0166\n",
      "Epoch [28/1000], Step [140/448], Loss: 0.0221\n",
      "Epoch [28/1000], Step [150/448], Loss: 0.0245\n",
      "Epoch [28/1000], Step [160/448], Loss: 0.0306\n",
      "Epoch [28/1000], Step [170/448], Loss: 0.0112\n",
      "Epoch [28/1000], Step [180/448], Loss: 0.0257\n",
      "Epoch [28/1000], Step [190/448], Loss: 0.0128\n",
      "Epoch [28/1000], Step [200/448], Loss: 0.0222\n",
      "Epoch [28/1000], Step [210/448], Loss: 0.0195\n",
      "Epoch [28/1000], Step [220/448], Loss: 0.0141\n",
      "Epoch [28/1000], Step [230/448], Loss: 0.0212\n",
      "Epoch [28/1000], Step [240/448], Loss: 0.0120\n",
      "Epoch [28/1000], Step [250/448], Loss: 0.0176\n",
      "Epoch [28/1000], Step [260/448], Loss: 0.0201\n",
      "Epoch [28/1000], Step [270/448], Loss: 0.0157\n",
      "Epoch [28/1000], Step [280/448], Loss: 0.0249\n",
      "Epoch [28/1000], Step [290/448], Loss: 0.0226\n",
      "Epoch [28/1000], Step [300/448], Loss: 0.0189\n",
      "Epoch [28/1000], Step [310/448], Loss: 0.0214\n",
      "Epoch [28/1000], Step [320/448], Loss: 0.0154\n",
      "Epoch [28/1000], Step [330/448], Loss: 0.0121\n",
      "Epoch [28/1000], Step [340/448], Loss: 0.0154\n",
      "Epoch [28/1000], Step [350/448], Loss: 0.0178\n",
      "Epoch [28/1000], Step [360/448], Loss: 0.0220\n",
      "Epoch [28/1000], Step [370/448], Loss: 0.0192\n",
      "Epoch [28/1000], Step [380/448], Loss: 0.0140\n",
      "Epoch [28/1000], Step [390/448], Loss: 0.0231\n",
      "Epoch [28/1000], Step [400/448], Loss: 0.0228\n",
      "Epoch [28/1000], Step [410/448], Loss: 0.0258\n",
      "Epoch [28/1000], Step [420/448], Loss: 0.0178\n",
      "Epoch [28/1000], Step [430/448], Loss: 0.0182\n",
      "Epoch [28/1000], Step [440/448], Loss: 0.0148\n",
      "Epoch [28/1000], Average Loss: 0.0184\n",
      "Epoch [28/1000], Validation Loss: 0.0205\n",
      "Epoch [29/1000], Step [10/448], Loss: 0.0127\n",
      "Epoch [29/1000], Step [20/448], Loss: 0.0179\n",
      "Epoch [29/1000], Step [30/448], Loss: 0.0122\n",
      "Epoch [29/1000], Step [40/448], Loss: 0.0157\n",
      "Epoch [29/1000], Step [50/448], Loss: 0.0172\n",
      "Epoch [29/1000], Step [60/448], Loss: 0.0204\n",
      "Epoch [29/1000], Step [70/448], Loss: 0.0130\n",
      "Epoch [29/1000], Step [80/448], Loss: 0.0189\n",
      "Epoch [29/1000], Step [90/448], Loss: 0.0109\n",
      "Epoch [29/1000], Step [100/448], Loss: 0.0173\n",
      "Epoch [29/1000], Step [110/448], Loss: 0.0207\n",
      "Epoch [29/1000], Step [120/448], Loss: 0.0181\n",
      "Epoch [29/1000], Step [130/448], Loss: 0.0188\n",
      "Epoch [29/1000], Step [140/448], Loss: 0.0170\n",
      "Epoch [29/1000], Step [150/448], Loss: 0.0213\n",
      "Epoch [29/1000], Step [160/448], Loss: 0.0149\n",
      "Epoch [29/1000], Step [170/448], Loss: 0.0243\n",
      "Epoch [29/1000], Step [180/448], Loss: 0.0151\n",
      "Epoch [29/1000], Step [190/448], Loss: 0.0140\n",
      "Epoch [29/1000], Step [200/448], Loss: 0.0215\n",
      "Epoch [29/1000], Step [210/448], Loss: 0.0160\n",
      "Epoch [29/1000], Step [220/448], Loss: 0.0213\n",
      "Epoch [29/1000], Step [230/448], Loss: 0.0131\n",
      "Epoch [29/1000], Step [240/448], Loss: 0.0176\n",
      "Epoch [29/1000], Step [250/448], Loss: 0.0134\n",
      "Epoch [29/1000], Step [260/448], Loss: 0.0210\n",
      "Epoch [29/1000], Step [270/448], Loss: 0.0084\n",
      "Epoch [29/1000], Step [280/448], Loss: 0.0158\n",
      "Epoch [29/1000], Step [290/448], Loss: 0.0140\n",
      "Epoch [29/1000], Step [300/448], Loss: 0.0127\n",
      "Epoch [29/1000], Step [310/448], Loss: 0.0200\n",
      "Epoch [29/1000], Step [320/448], Loss: 0.0173\n",
      "Epoch [29/1000], Step [330/448], Loss: 0.0142\n",
      "Epoch [29/1000], Step [340/448], Loss: 0.0208\n",
      "Epoch [29/1000], Step [350/448], Loss: 0.0123\n",
      "Epoch [29/1000], Step [360/448], Loss: 0.0298\n",
      "Epoch [29/1000], Step [370/448], Loss: 0.0131\n",
      "Epoch [29/1000], Step [380/448], Loss: 0.0249\n",
      "Epoch [29/1000], Step [390/448], Loss: 0.0241\n",
      "Epoch [29/1000], Step [400/448], Loss: 0.0242\n",
      "Epoch [29/1000], Step [410/448], Loss: 0.0105\n",
      "Epoch [29/1000], Step [420/448], Loss: 0.0167\n",
      "Epoch [29/1000], Step [430/448], Loss: 0.0198\n",
      "Epoch [29/1000], Step [440/448], Loss: 0.0163\n",
      "Epoch [29/1000], Average Loss: 0.0179\n",
      "Epoch [29/1000], Validation Loss: 0.0217\n",
      "Epoch [30/1000], Step [10/448], Loss: 0.0120\n",
      "Epoch [30/1000], Step [20/448], Loss: 0.0149\n",
      "Epoch [30/1000], Step [30/448], Loss: 0.0170\n",
      "Epoch [30/1000], Step [40/448], Loss: 0.0148\n",
      "Epoch [30/1000], Step [50/448], Loss: 0.0195\n",
      "Epoch [30/1000], Step [60/448], Loss: 0.0068\n",
      "Epoch [30/1000], Step [70/448], Loss: 0.0241\n",
      "Epoch [30/1000], Step [80/448], Loss: 0.0241\n",
      "Epoch [30/1000], Step [90/448], Loss: 0.0143\n",
      "Epoch [30/1000], Step [100/448], Loss: 0.0180\n",
      "Epoch [30/1000], Step [110/448], Loss: 0.0263\n",
      "Epoch [30/1000], Step [120/448], Loss: 0.0210\n",
      "Epoch [30/1000], Step [130/448], Loss: 0.0144\n",
      "Epoch [30/1000], Step [140/448], Loss: 0.0159\n",
      "Epoch [30/1000], Step [150/448], Loss: 0.0180\n",
      "Epoch [30/1000], Step [160/448], Loss: 0.0163\n",
      "Epoch [30/1000], Step [170/448], Loss: 0.0330\n",
      "Epoch [30/1000], Step [180/448], Loss: 0.0196\n",
      "Epoch [30/1000], Step [190/448], Loss: 0.0168\n",
      "Epoch [30/1000], Step [200/448], Loss: 0.0175\n",
      "Epoch [30/1000], Step [210/448], Loss: 0.0174\n",
      "Epoch [30/1000], Step [220/448], Loss: 0.0167\n",
      "Epoch [30/1000], Step [230/448], Loss: 0.0149\n",
      "Epoch [30/1000], Step [240/448], Loss: 0.0167\n",
      "Epoch [30/1000], Step [250/448], Loss: 0.0207\n",
      "Epoch [30/1000], Step [260/448], Loss: 0.0157\n",
      "Epoch [30/1000], Step [270/448], Loss: 0.0111\n",
      "Epoch [30/1000], Step [280/448], Loss: 0.0249\n",
      "Epoch [30/1000], Step [290/448], Loss: 0.0122\n",
      "Epoch [30/1000], Step [300/448], Loss: 0.0233\n",
      "Epoch [30/1000], Step [310/448], Loss: 0.0189\n",
      "Epoch [30/1000], Step [320/448], Loss: 0.0164\n",
      "Epoch [30/1000], Step [330/448], Loss: 0.0197\n",
      "Epoch [30/1000], Step [340/448], Loss: 0.0194\n",
      "Epoch [30/1000], Step [350/448], Loss: 0.0197\n",
      "Epoch [30/1000], Step [360/448], Loss: 0.0159\n",
      "Epoch [30/1000], Step [370/448], Loss: 0.0175\n",
      "Epoch [30/1000], Step [380/448], Loss: 0.0174\n",
      "Epoch [30/1000], Step [390/448], Loss: 0.0209\n",
      "Epoch [30/1000], Step [400/448], Loss: 0.0179\n",
      "Epoch [30/1000], Step [410/448], Loss: 0.0200\n",
      "Epoch [30/1000], Step [420/448], Loss: 0.0244\n",
      "Epoch [30/1000], Step [430/448], Loss: 0.0114\n",
      "Epoch [30/1000], Step [440/448], Loss: 0.0163\n",
      "Epoch [30/1000], Average Loss: 0.0173\n",
      "Epoch [30/1000], Validation Loss: 0.0221\n",
      "Epoch [31/1000], Step [10/448], Loss: 0.0184\n",
      "Epoch [31/1000], Step [20/448], Loss: 0.0225\n",
      "Epoch [31/1000], Step [30/448], Loss: 0.0163\n",
      "Epoch [31/1000], Step [40/448], Loss: 0.0180\n",
      "Epoch [31/1000], Step [50/448], Loss: 0.0223\n",
      "Epoch [31/1000], Step [60/448], Loss: 0.0135\n",
      "Epoch [31/1000], Step [70/448], Loss: 0.0124\n",
      "Epoch [31/1000], Step [80/448], Loss: 0.0144\n",
      "Epoch [31/1000], Step [90/448], Loss: 0.0138\n",
      "Epoch [31/1000], Step [100/448], Loss: 0.0245\n",
      "Epoch [31/1000], Step [110/448], Loss: 0.0167\n",
      "Epoch [31/1000], Step [120/448], Loss: 0.0170\n",
      "Epoch [31/1000], Step [130/448], Loss: 0.0254\n",
      "Epoch [31/1000], Step [140/448], Loss: 0.0204\n",
      "Epoch [31/1000], Step [150/448], Loss: 0.0236\n",
      "Epoch [31/1000], Step [160/448], Loss: 0.0221\n",
      "Epoch [31/1000], Step [170/448], Loss: 0.0074\n",
      "Epoch [31/1000], Step [180/448], Loss: 0.0185\n",
      "Epoch [31/1000], Step [190/448], Loss: 0.0281\n",
      "Epoch [31/1000], Step [200/448], Loss: 0.0173\n",
      "Epoch [31/1000], Step [210/448], Loss: 0.0136\n",
      "Epoch [31/1000], Step [220/448], Loss: 0.0298\n",
      "Epoch [31/1000], Step [230/448], Loss: 0.0124\n",
      "Epoch [31/1000], Step [240/448], Loss: 0.0249\n",
      "Epoch [31/1000], Step [250/448], Loss: 0.0132\n",
      "Epoch [31/1000], Step [260/448], Loss: 0.0199\n",
      "Epoch [31/1000], Step [270/448], Loss: 0.0170\n",
      "Epoch [31/1000], Step [280/448], Loss: 0.0166\n",
      "Epoch [31/1000], Step [290/448], Loss: 0.0200\n",
      "Epoch [31/1000], Step [300/448], Loss: 0.0225\n",
      "Epoch [31/1000], Step [310/448], Loss: 0.0221\n",
      "Epoch [31/1000], Step [320/448], Loss: 0.0204\n",
      "Epoch [31/1000], Step [330/448], Loss: 0.0151\n",
      "Epoch [31/1000], Step [340/448], Loss: 0.0156\n",
      "Epoch [31/1000], Step [350/448], Loss: 0.0180\n",
      "Epoch [31/1000], Step [360/448], Loss: 0.0136\n",
      "Epoch [31/1000], Step [370/448], Loss: 0.0153\n",
      "Epoch [31/1000], Step [380/448], Loss: 0.0183\n",
      "Epoch [31/1000], Step [390/448], Loss: 0.0088\n",
      "Epoch [31/1000], Step [400/448], Loss: 0.0159\n",
      "Epoch [31/1000], Step [410/448], Loss: 0.0118\n",
      "Epoch [31/1000], Step [420/448], Loss: 0.0235\n",
      "Epoch [31/1000], Step [430/448], Loss: 0.0221\n",
      "Epoch [31/1000], Step [440/448], Loss: 0.0119\n",
      "Epoch [31/1000], Average Loss: 0.0171\n",
      "Epoch [31/1000], Validation Loss: 0.0213\n",
      "Epoch [32/1000], Step [10/448], Loss: 0.0192\n",
      "Epoch [32/1000], Step [20/448], Loss: 0.0201\n",
      "Epoch [32/1000], Step [30/448], Loss: 0.0201\n",
      "Epoch [32/1000], Step [40/448], Loss: 0.0186\n",
      "Epoch [32/1000], Step [50/448], Loss: 0.0104\n",
      "Epoch [32/1000], Step [60/448], Loss: 0.0138\n",
      "Epoch [32/1000], Step [70/448], Loss: 0.0154\n",
      "Epoch [32/1000], Step [80/448], Loss: 0.0173\n",
      "Epoch [32/1000], Step [90/448], Loss: 0.0158\n",
      "Epoch [32/1000], Step [100/448], Loss: 0.0182\n",
      "Epoch [32/1000], Step [110/448], Loss: 0.0185\n",
      "Epoch [32/1000], Step [120/448], Loss: 0.0120\n",
      "Epoch [32/1000], Step [130/448], Loss: 0.0198\n",
      "Epoch [32/1000], Step [140/448], Loss: 0.0234\n",
      "Epoch [32/1000], Step [150/448], Loss: 0.0163\n",
      "Epoch [32/1000], Step [160/448], Loss: 0.0153\n",
      "Epoch [32/1000], Step [170/448], Loss: 0.0236\n",
      "Epoch [32/1000], Step [180/448], Loss: 0.0180\n",
      "Epoch [32/1000], Step [190/448], Loss: 0.0243\n",
      "Epoch [32/1000], Step [200/448], Loss: 0.0096\n",
      "Epoch [32/1000], Step [210/448], Loss: 0.0207\n",
      "Epoch [32/1000], Step [220/448], Loss: 0.0278\n",
      "Epoch [32/1000], Step [230/448], Loss: 0.0166\n",
      "Epoch [32/1000], Step [240/448], Loss: 0.0144\n",
      "Epoch [32/1000], Step [250/448], Loss: 0.0287\n",
      "Epoch [32/1000], Step [260/448], Loss: 0.0150\n",
      "Epoch [32/1000], Step [270/448], Loss: 0.0185\n",
      "Epoch [32/1000], Step [280/448], Loss: 0.0231\n",
      "Epoch [32/1000], Step [290/448], Loss: 0.0155\n",
      "Epoch [32/1000], Step [300/448], Loss: 0.0172\n",
      "Epoch [32/1000], Step [310/448], Loss: 0.0167\n",
      "Epoch [32/1000], Step [320/448], Loss: 0.0121\n",
      "Epoch [32/1000], Step [330/448], Loss: 0.0144\n",
      "Epoch [32/1000], Step [340/448], Loss: 0.0114\n",
      "Epoch [32/1000], Step [350/448], Loss: 0.0196\n",
      "Epoch [32/1000], Step [360/448], Loss: 0.0162\n",
      "Epoch [32/1000], Step [370/448], Loss: 0.0109\n",
      "Epoch [32/1000], Step [380/448], Loss: 0.0156\n",
      "Epoch [32/1000], Step [390/448], Loss: 0.0242\n",
      "Epoch [32/1000], Step [400/448], Loss: 0.0179\n",
      "Epoch [32/1000], Step [410/448], Loss: 0.0143\n",
      "Epoch [32/1000], Step [420/448], Loss: 0.0173\n",
      "Epoch [32/1000], Step [430/448], Loss: 0.0151\n",
      "Epoch [32/1000], Step [440/448], Loss: 0.0117\n",
      "Epoch [32/1000], Average Loss: 0.0173\n",
      "Epoch [32/1000], Validation Loss: 0.0214\n",
      "Epoch [33/1000], Step [10/448], Loss: 0.0297\n",
      "Epoch [33/1000], Step [20/448], Loss: 0.0137\n",
      "Epoch [33/1000], Step [30/448], Loss: 0.0165\n",
      "Epoch [33/1000], Step [40/448], Loss: 0.0225\n",
      "Epoch [33/1000], Step [50/448], Loss: 0.0119\n",
      "Epoch [33/1000], Step [60/448], Loss: 0.0151\n",
      "Epoch [33/1000], Step [70/448], Loss: 0.0207\n",
      "Epoch [33/1000], Step [80/448], Loss: 0.0186\n",
      "Epoch [33/1000], Step [90/448], Loss: 0.0109\n",
      "Epoch [33/1000], Step [100/448], Loss: 0.0150\n",
      "Epoch [33/1000], Step [110/448], Loss: 0.0218\n",
      "Epoch [33/1000], Step [120/448], Loss: 0.0254\n",
      "Epoch [33/1000], Step [130/448], Loss: 0.0159\n",
      "Epoch [33/1000], Step [140/448], Loss: 0.0186\n",
      "Epoch [33/1000], Step [150/448], Loss: 0.0181\n",
      "Epoch [33/1000], Step [160/448], Loss: 0.0128\n",
      "Epoch [33/1000], Step [170/448], Loss: 0.0133\n",
      "Epoch [33/1000], Step [180/448], Loss: 0.0182\n",
      "Epoch [33/1000], Step [190/448], Loss: 0.0149\n",
      "Epoch [33/1000], Step [200/448], Loss: 0.0136\n",
      "Epoch [33/1000], Step [210/448], Loss: 0.0202\n",
      "Epoch [33/1000], Step [220/448], Loss: 0.0199\n",
      "Epoch [33/1000], Step [230/448], Loss: 0.0174\n",
      "Epoch [33/1000], Step [240/448], Loss: 0.0231\n",
      "Epoch [33/1000], Step [250/448], Loss: 0.0153\n",
      "Epoch [33/1000], Step [260/448], Loss: 0.0111\n",
      "Epoch [33/1000], Step [270/448], Loss: 0.0161\n",
      "Epoch [33/1000], Step [280/448], Loss: 0.0245\n",
      "Epoch [33/1000], Step [290/448], Loss: 0.0129\n",
      "Epoch [33/1000], Step [300/448], Loss: 0.0201\n",
      "Epoch [33/1000], Step [310/448], Loss: 0.0229\n",
      "Epoch [33/1000], Step [320/448], Loss: 0.0216\n",
      "Epoch [33/1000], Step [330/448], Loss: 0.0204\n",
      "Epoch [33/1000], Step [340/448], Loss: 0.0161\n",
      "Epoch [33/1000], Step [350/448], Loss: 0.0187\n",
      "Epoch [33/1000], Step [360/448], Loss: 0.0201\n",
      "Epoch [33/1000], Step [370/448], Loss: 0.0117\n",
      "Epoch [33/1000], Step [380/448], Loss: 0.0228\n",
      "Epoch [33/1000], Step [390/448], Loss: 0.0151\n",
      "Epoch [33/1000], Step [400/448], Loss: 0.0189\n",
      "Epoch [33/1000], Step [410/448], Loss: 0.0228\n",
      "Epoch [33/1000], Step [420/448], Loss: 0.0143\n",
      "Epoch [33/1000], Step [430/448], Loss: 0.0138\n",
      "Epoch [33/1000], Step [440/448], Loss: 0.0225\n",
      "Epoch [33/1000], Average Loss: 0.0172\n",
      "Epoch [33/1000], Validation Loss: 0.0199\n",
      "Epoch [34/1000], Step [10/448], Loss: 0.0113\n",
      "Epoch [34/1000], Step [20/448], Loss: 0.0221\n",
      "Epoch [34/1000], Step [30/448], Loss: 0.0207\n",
      "Epoch [34/1000], Step [40/448], Loss: 0.0156\n",
      "Epoch [34/1000], Step [50/448], Loss: 0.0079\n",
      "Epoch [34/1000], Step [60/448], Loss: 0.0199\n",
      "Epoch [34/1000], Step [70/448], Loss: 0.0147\n",
      "Epoch [34/1000], Step [80/448], Loss: 0.0095\n",
      "Epoch [34/1000], Step [90/448], Loss: 0.0175\n",
      "Epoch [34/1000], Step [100/448], Loss: 0.0271\n",
      "Epoch [34/1000], Step [110/448], Loss: 0.0247\n",
      "Epoch [34/1000], Step [120/448], Loss: 0.0120\n",
      "Epoch [34/1000], Step [130/448], Loss: 0.0158\n",
      "Epoch [34/1000], Step [140/448], Loss: 0.0131\n",
      "Epoch [34/1000], Step [150/448], Loss: 0.0159\n",
      "Epoch [34/1000], Step [160/448], Loss: 0.0201\n",
      "Epoch [34/1000], Step [170/448], Loss: 0.0141\n",
      "Epoch [34/1000], Step [180/448], Loss: 0.0144\n",
      "Epoch [34/1000], Step [190/448], Loss: 0.0176\n",
      "Epoch [34/1000], Step [200/448], Loss: 0.0189\n",
      "Epoch [34/1000], Step [210/448], Loss: 0.0231\n",
      "Epoch [34/1000], Step [220/448], Loss: 0.0182\n",
      "Epoch [34/1000], Step [230/448], Loss: 0.0241\n",
      "Epoch [34/1000], Step [240/448], Loss: 0.0050\n",
      "Epoch [34/1000], Step [250/448], Loss: 0.0159\n",
      "Epoch [34/1000], Step [260/448], Loss: 0.0135\n",
      "Epoch [34/1000], Step [270/448], Loss: 0.0153\n",
      "Epoch [34/1000], Step [280/448], Loss: 0.0119\n",
      "Epoch [34/1000], Step [290/448], Loss: 0.0181\n",
      "Epoch [34/1000], Step [300/448], Loss: 0.0191\n",
      "Epoch [34/1000], Step [310/448], Loss: 0.0205\n",
      "Epoch [34/1000], Step [320/448], Loss: 0.0147\n",
      "Epoch [34/1000], Step [330/448], Loss: 0.0144\n",
      "Epoch [34/1000], Step [340/448], Loss: 0.0240\n",
      "Epoch [34/1000], Step [350/448], Loss: 0.0196\n",
      "Epoch [34/1000], Step [360/448], Loss: 0.0140\n",
      "Epoch [34/1000], Step [370/448], Loss: 0.0239\n",
      "Epoch [34/1000], Step [380/448], Loss: 0.0271\n",
      "Epoch [34/1000], Step [390/448], Loss: 0.0178\n",
      "Epoch [34/1000], Step [400/448], Loss: 0.0287\n",
      "Epoch [34/1000], Step [410/448], Loss: 0.0164\n",
      "Epoch [34/1000], Step [420/448], Loss: 0.0056\n",
      "Epoch [34/1000], Step [430/448], Loss: 0.0116\n",
      "Epoch [34/1000], Step [440/448], Loss: 0.0214\n",
      "Epoch [34/1000], Average Loss: 0.0169\n",
      "Epoch [34/1000], Validation Loss: 0.0210\n",
      "Epoch [35/1000], Step [10/448], Loss: 0.0122\n",
      "Epoch [35/1000], Step [20/448], Loss: 0.0224\n",
      "Epoch [35/1000], Step [30/448], Loss: 0.0050\n",
      "Epoch [35/1000], Step [40/448], Loss: 0.0176\n",
      "Epoch [35/1000], Step [50/448], Loss: 0.0229\n",
      "Epoch [35/1000], Step [60/448], Loss: 0.0149\n",
      "Epoch [35/1000], Step [70/448], Loss: 0.0191\n",
      "Epoch [35/1000], Step [80/448], Loss: 0.0180\n",
      "Epoch [35/1000], Step [90/448], Loss: 0.0138\n",
      "Epoch [35/1000], Step [100/448], Loss: 0.0204\n",
      "Epoch [35/1000], Step [110/448], Loss: 0.0189\n",
      "Epoch [35/1000], Step [120/448], Loss: 0.0220\n",
      "Epoch [35/1000], Step [130/448], Loss: 0.0132\n",
      "Epoch [35/1000], Step [140/448], Loss: 0.0167\n",
      "Epoch [35/1000], Step [150/448], Loss: 0.0113\n",
      "Epoch [35/1000], Step [160/448], Loss: 0.0157\n",
      "Epoch [35/1000], Step [170/448], Loss: 0.0129\n",
      "Epoch [35/1000], Step [180/448], Loss: 0.0228\n",
      "Epoch [35/1000], Step [190/448], Loss: 0.0305\n",
      "Epoch [35/1000], Step [200/448], Loss: 0.0186\n",
      "Epoch [35/1000], Step [210/448], Loss: 0.0304\n",
      "Epoch [35/1000], Step [220/448], Loss: 0.0094\n",
      "Epoch [35/1000], Step [230/448], Loss: 0.0147\n",
      "Epoch [35/1000], Step [240/448], Loss: 0.0205\n",
      "Epoch [35/1000], Step [250/448], Loss: 0.0231\n",
      "Epoch [35/1000], Step [260/448], Loss: 0.0218\n",
      "Epoch [35/1000], Step [270/448], Loss: 0.0124\n",
      "Epoch [35/1000], Step [280/448], Loss: 0.0154\n",
      "Epoch [35/1000], Step [290/448], Loss: 0.0166\n",
      "Epoch [35/1000], Step [300/448], Loss: 0.0096\n",
      "Epoch [35/1000], Step [310/448], Loss: 0.0181\n",
      "Epoch [35/1000], Step [320/448], Loss: 0.0206\n",
      "Epoch [35/1000], Step [330/448], Loss: 0.0259\n",
      "Epoch [35/1000], Step [340/448], Loss: 0.0139\n",
      "Epoch [35/1000], Step [350/448], Loss: 0.0188\n",
      "Epoch [35/1000], Step [360/448], Loss: 0.0175\n",
      "Epoch [35/1000], Step [370/448], Loss: 0.0123\n",
      "Epoch [35/1000], Step [380/448], Loss: 0.0149\n",
      "Epoch [35/1000], Step [390/448], Loss: 0.0111\n",
      "Epoch [35/1000], Step [400/448], Loss: 0.0120\n",
      "Epoch [35/1000], Step [410/448], Loss: 0.0197\n",
      "Epoch [35/1000], Step [420/448], Loss: 0.0220\n",
      "Epoch [35/1000], Step [430/448], Loss: 0.0192\n",
      "Epoch [35/1000], Step [440/448], Loss: 0.0122\n",
      "Epoch [35/1000], Average Loss: 0.0170\n",
      "Epoch [35/1000], Validation Loss: 0.0211\n",
      "Epoch [36/1000], Step [10/448], Loss: 0.0138\n",
      "Epoch [36/1000], Step [20/448], Loss: 0.0171\n",
      "Epoch [36/1000], Step [30/448], Loss: 0.0165\n",
      "Epoch [36/1000], Step [40/448], Loss: 0.0155\n",
      "Epoch [36/1000], Step [50/448], Loss: 0.0243\n",
      "Epoch [36/1000], Step [60/448], Loss: 0.0214\n",
      "Epoch [36/1000], Step [70/448], Loss: 0.0166\n",
      "Epoch [36/1000], Step [80/448], Loss: 0.0160\n",
      "Epoch [36/1000], Step [90/448], Loss: 0.0172\n",
      "Epoch [36/1000], Step [100/448], Loss: 0.0193\n",
      "Epoch [36/1000], Step [110/448], Loss: 0.0182\n",
      "Epoch [36/1000], Step [120/448], Loss: 0.0155\n",
      "Epoch [36/1000], Step [130/448], Loss: 0.0120\n",
      "Epoch [36/1000], Step [140/448], Loss: 0.0110\n",
      "Epoch [36/1000], Step [150/448], Loss: 0.0174\n",
      "Epoch [36/1000], Step [160/448], Loss: 0.0216\n",
      "Epoch [36/1000], Step [170/448], Loss: 0.0146\n",
      "Epoch [36/1000], Step [180/448], Loss: 0.0153\n",
      "Epoch [36/1000], Step [190/448], Loss: 0.0156\n",
      "Epoch [36/1000], Step [200/448], Loss: 0.0144\n",
      "Epoch [36/1000], Step [210/448], Loss: 0.0107\n",
      "Epoch [36/1000], Step [220/448], Loss: 0.0176\n",
      "Epoch [36/1000], Step [230/448], Loss: 0.0152\n",
      "Epoch [36/1000], Step [240/448], Loss: 0.0151\n",
      "Epoch [36/1000], Step [250/448], Loss: 0.0120\n",
      "Epoch [36/1000], Step [260/448], Loss: 0.0213\n",
      "Epoch [36/1000], Step [270/448], Loss: 0.0177\n",
      "Epoch [36/1000], Step [280/448], Loss: 0.0100\n",
      "Epoch [36/1000], Step [290/448], Loss: 0.0166\n",
      "Epoch [36/1000], Step [300/448], Loss: 0.0164\n",
      "Epoch [36/1000], Step [310/448], Loss: 0.0158\n",
      "Epoch [36/1000], Step [320/448], Loss: 0.0284\n",
      "Epoch [36/1000], Step [330/448], Loss: 0.0126\n",
      "Epoch [36/1000], Step [340/448], Loss: 0.0165\n",
      "Epoch [36/1000], Step [350/448], Loss: 0.0133\n",
      "Epoch [36/1000], Step [360/448], Loss: 0.0174\n",
      "Epoch [36/1000], Step [370/448], Loss: 0.0083\n",
      "Epoch [36/1000], Step [380/448], Loss: 0.0148\n",
      "Epoch [36/1000], Step [390/448], Loss: 0.0107\n",
      "Epoch [36/1000], Step [400/448], Loss: 0.0215\n",
      "Epoch [36/1000], Step [410/448], Loss: 0.0093\n",
      "Epoch [36/1000], Step [420/448], Loss: 0.0189\n",
      "Epoch [36/1000], Step [430/448], Loss: 0.0158\n",
      "Epoch [36/1000], Step [440/448], Loss: 0.0172\n",
      "Epoch [36/1000], Average Loss: 0.0166\n",
      "Epoch [36/1000], Validation Loss: 0.0221\n",
      "Epoch [37/1000], Step [10/448], Loss: 0.0220\n",
      "Epoch [37/1000], Step [20/448], Loss: 0.0173\n",
      "Epoch [37/1000], Step [30/448], Loss: 0.0150\n",
      "Epoch [37/1000], Step [40/448], Loss: 0.0257\n",
      "Epoch [37/1000], Step [50/448], Loss: 0.0294\n",
      "Epoch [37/1000], Step [60/448], Loss: 0.0196\n",
      "Epoch [37/1000], Step [70/448], Loss: 0.0085\n",
      "Epoch [37/1000], Step [80/448], Loss: 0.0148\n",
      "Epoch [37/1000], Step [90/448], Loss: 0.0229\n",
      "Epoch [37/1000], Step [100/448], Loss: 0.0178\n",
      "Epoch [37/1000], Step [110/448], Loss: 0.0084\n",
      "Epoch [37/1000], Step [120/448], Loss: 0.0137\n",
      "Epoch [37/1000], Step [130/448], Loss: 0.0172\n",
      "Epoch [37/1000], Step [140/448], Loss: 0.0079\n",
      "Epoch [37/1000], Step [150/448], Loss: 0.0149\n",
      "Epoch [37/1000], Step [160/448], Loss: 0.0192\n",
      "Epoch [37/1000], Step [170/448], Loss: 0.0215\n",
      "Epoch [37/1000], Step [180/448], Loss: 0.0143\n",
      "Epoch [37/1000], Step [190/448], Loss: 0.0120\n",
      "Epoch [37/1000], Step [200/448], Loss: 0.0150\n",
      "Epoch [37/1000], Step [210/448], Loss: 0.0227\n",
      "Epoch [37/1000], Step [220/448], Loss: 0.0117\n",
      "Epoch [37/1000], Step [230/448], Loss: 0.0205\n",
      "Epoch [37/1000], Step [240/448], Loss: 0.0133\n",
      "Epoch [37/1000], Step [250/448], Loss: 0.0158\n",
      "Epoch [37/1000], Step [260/448], Loss: 0.0200\n",
      "Epoch [37/1000], Step [270/448], Loss: 0.0210\n",
      "Epoch [37/1000], Step [280/448], Loss: 0.0204\n",
      "Epoch [37/1000], Step [290/448], Loss: 0.0068\n",
      "Epoch [37/1000], Step [300/448], Loss: 0.0132\n",
      "Epoch [37/1000], Step [310/448], Loss: 0.0190\n",
      "Epoch [37/1000], Step [320/448], Loss: 0.0140\n",
      "Epoch [37/1000], Step [330/448], Loss: 0.0069\n",
      "Epoch [37/1000], Step [340/448], Loss: 0.0168\n",
      "Epoch [37/1000], Step [350/448], Loss: 0.0302\n",
      "Epoch [37/1000], Step [360/448], Loss: 0.0151\n",
      "Epoch [37/1000], Step [370/448], Loss: 0.0173\n",
      "Epoch [37/1000], Step [380/448], Loss: 0.0174\n",
      "Epoch [37/1000], Step [390/448], Loss: 0.0153\n",
      "Epoch [37/1000], Step [400/448], Loss: 0.0086\n",
      "Epoch [37/1000], Step [410/448], Loss: 0.0097\n",
      "Epoch [37/1000], Step [420/448], Loss: 0.0096\n",
      "Epoch [37/1000], Step [430/448], Loss: 0.0166\n",
      "Epoch [37/1000], Step [440/448], Loss: 0.0091\n",
      "Epoch [37/1000], Average Loss: 0.0166\n",
      "Epoch [37/1000], Validation Loss: 0.0197\n",
      "Epoch [38/1000], Step [10/448], Loss: 0.0211\n",
      "Epoch [38/1000], Step [20/448], Loss: 0.0171\n",
      "Epoch [38/1000], Step [30/448], Loss: 0.0108\n",
      "Epoch [38/1000], Step [40/448], Loss: 0.0146\n",
      "Epoch [38/1000], Step [50/448], Loss: 0.0138\n",
      "Epoch [38/1000], Step [60/448], Loss: 0.0169\n",
      "Epoch [38/1000], Step [70/448], Loss: 0.0127\n",
      "Epoch [38/1000], Step [80/448], Loss: 0.0097\n",
      "Epoch [38/1000], Step [90/448], Loss: 0.0197\n",
      "Epoch [38/1000], Step [100/448], Loss: 0.0141\n",
      "Epoch [38/1000], Step [110/448], Loss: 0.0106\n",
      "Epoch [38/1000], Step [120/448], Loss: 0.0139\n",
      "Epoch [38/1000], Step [130/448], Loss: 0.0105\n",
      "Epoch [38/1000], Step [140/448], Loss: 0.0146\n",
      "Epoch [38/1000], Step [150/448], Loss: 0.0204\n",
      "Epoch [38/1000], Step [160/448], Loss: 0.0147\n",
      "Epoch [38/1000], Step [170/448], Loss: 0.0113\n",
      "Epoch [38/1000], Step [180/448], Loss: 0.0111\n",
      "Epoch [38/1000], Step [190/448], Loss: 0.0170\n",
      "Epoch [38/1000], Step [200/448], Loss: 0.0200\n",
      "Epoch [38/1000], Step [210/448], Loss: 0.0122\n",
      "Epoch [38/1000], Step [220/448], Loss: 0.0166\n",
      "Epoch [38/1000], Step [230/448], Loss: 0.0134\n",
      "Epoch [38/1000], Step [240/448], Loss: 0.0113\n",
      "Epoch [38/1000], Step [250/448], Loss: 0.0208\n",
      "Epoch [38/1000], Step [260/448], Loss: 0.0153\n",
      "Epoch [38/1000], Step [270/448], Loss: 0.0164\n",
      "Epoch [38/1000], Step [280/448], Loss: 0.0198\n",
      "Epoch [38/1000], Step [290/448], Loss: 0.0145\n",
      "Epoch [38/1000], Step [300/448], Loss: 0.0277\n",
      "Epoch [38/1000], Step [310/448], Loss: 0.0181\n",
      "Epoch [38/1000], Step [320/448], Loss: 0.0183\n",
      "Epoch [38/1000], Step [330/448], Loss: 0.0130\n",
      "Epoch [38/1000], Step [340/448], Loss: 0.0135\n",
      "Epoch [38/1000], Step [350/448], Loss: 0.0126\n",
      "Epoch [38/1000], Step [360/448], Loss: 0.0117\n",
      "Epoch [38/1000], Step [370/448], Loss: 0.0203\n",
      "Epoch [38/1000], Step [380/448], Loss: 0.0251\n",
      "Epoch [38/1000], Step [390/448], Loss: 0.0129\n",
      "Epoch [38/1000], Step [400/448], Loss: 0.0215\n",
      "Epoch [38/1000], Step [410/448], Loss: 0.0228\n",
      "Epoch [38/1000], Step [420/448], Loss: 0.0075\n",
      "Epoch [38/1000], Step [430/448], Loss: 0.0217\n",
      "Epoch [38/1000], Step [440/448], Loss: 0.0191\n",
      "Epoch [38/1000], Average Loss: 0.0162\n",
      "Epoch [38/1000], Validation Loss: 0.0199\n",
      "Epoch [39/1000], Step [10/448], Loss: 0.0162\n",
      "Epoch [39/1000], Step [20/448], Loss: 0.0113\n",
      "Epoch [39/1000], Step [30/448], Loss: 0.0187\n",
      "Epoch [39/1000], Step [40/448], Loss: 0.0148\n",
      "Epoch [39/1000], Step [50/448], Loss: 0.0157\n",
      "Epoch [39/1000], Step [60/448], Loss: 0.0057\n",
      "Epoch [39/1000], Step [70/448], Loss: 0.0167\n",
      "Epoch [39/1000], Step [80/448], Loss: 0.0239\n",
      "Epoch [39/1000], Step [90/448], Loss: 0.0141\n",
      "Epoch [39/1000], Step [100/448], Loss: 0.0097\n",
      "Epoch [39/1000], Step [110/448], Loss: 0.0138\n",
      "Epoch [39/1000], Step [120/448], Loss: 0.0060\n",
      "Epoch [39/1000], Step [130/448], Loss: 0.0089\n",
      "Epoch [39/1000], Step [140/448], Loss: 0.0154\n",
      "Epoch [39/1000], Step [150/448], Loss: 0.0183\n",
      "Epoch [39/1000], Step [160/448], Loss: 0.0188\n",
      "Epoch [39/1000], Step [170/448], Loss: 0.0143\n",
      "Epoch [39/1000], Step [180/448], Loss: 0.0189\n",
      "Epoch [39/1000], Step [190/448], Loss: 0.0104\n",
      "Epoch [39/1000], Step [200/448], Loss: 0.0358\n",
      "Epoch [39/1000], Step [210/448], Loss: 0.0063\n",
      "Epoch [39/1000], Step [220/448], Loss: 0.0119\n",
      "Epoch [39/1000], Step [230/448], Loss: 0.0162\n",
      "Epoch [39/1000], Step [240/448], Loss: 0.0151\n",
      "Epoch [39/1000], Step [250/448], Loss: 0.0125\n",
      "Epoch [39/1000], Step [260/448], Loss: 0.0107\n",
      "Epoch [39/1000], Step [270/448], Loss: 0.0131\n",
      "Epoch [39/1000], Step [280/448], Loss: 0.0165\n",
      "Epoch [39/1000], Step [290/448], Loss: 0.0208\n",
      "Epoch [39/1000], Step [300/448], Loss: 0.0200\n",
      "Epoch [39/1000], Step [310/448], Loss: 0.0211\n",
      "Epoch [39/1000], Step [320/448], Loss: 0.0136\n",
      "Epoch [39/1000], Step [330/448], Loss: 0.0256\n",
      "Epoch [39/1000], Step [340/448], Loss: 0.0303\n",
      "Epoch [39/1000], Step [350/448], Loss: 0.0183\n",
      "Epoch [39/1000], Step [360/448], Loss: 0.0163\n",
      "Epoch [39/1000], Step [370/448], Loss: 0.0259\n",
      "Epoch [39/1000], Step [380/448], Loss: 0.0275\n",
      "Epoch [39/1000], Step [390/448], Loss: 0.0234\n",
      "Epoch [39/1000], Step [400/448], Loss: 0.0173\n",
      "Epoch [39/1000], Step [410/448], Loss: 0.0120\n",
      "Epoch [39/1000], Step [420/448], Loss: 0.0105\n",
      "Epoch [39/1000], Step [430/448], Loss: 0.0132\n",
      "Epoch [39/1000], Step [440/448], Loss: 0.0118\n",
      "Epoch [39/1000], Average Loss: 0.0162\n",
      "Epoch [39/1000], Validation Loss: 0.0206\n",
      "Epoch [40/1000], Step [10/448], Loss: 0.0222\n",
      "Epoch [40/1000], Step [20/448], Loss: 0.0093\n",
      "Epoch [40/1000], Step [30/448], Loss: 0.0092\n",
      "Epoch [40/1000], Step [40/448], Loss: 0.0153\n",
      "Epoch [40/1000], Step [50/448], Loss: 0.0166\n",
      "Epoch [40/1000], Step [60/448], Loss: 0.0186\n",
      "Epoch [40/1000], Step [70/448], Loss: 0.0161\n",
      "Epoch [40/1000], Step [80/448], Loss: 0.0112\n",
      "Epoch [40/1000], Step [90/448], Loss: 0.0191\n",
      "Epoch [40/1000], Step [100/448], Loss: 0.0180\n",
      "Epoch [40/1000], Step [110/448], Loss: 0.0176\n",
      "Epoch [40/1000], Step [120/448], Loss: 0.0123\n",
      "Epoch [40/1000], Step [130/448], Loss: 0.0062\n",
      "Epoch [40/1000], Step [140/448], Loss: 0.0172\n",
      "Epoch [40/1000], Step [150/448], Loss: 0.0188\n",
      "Epoch [40/1000], Step [160/448], Loss: 0.0158\n",
      "Epoch [40/1000], Step [170/448], Loss: 0.0198\n",
      "Epoch [40/1000], Step [180/448], Loss: 0.0160\n",
      "Epoch [40/1000], Step [190/448], Loss: 0.0122\n",
      "Epoch [40/1000], Step [200/448], Loss: 0.0104\n",
      "Epoch [40/1000], Step [210/448], Loss: 0.0106\n",
      "Epoch [40/1000], Step [220/448], Loss: 0.0167\n",
      "Epoch [40/1000], Step [230/448], Loss: 0.0130\n",
      "Epoch [40/1000], Step [240/448], Loss: 0.0158\n",
      "Epoch [40/1000], Step [250/448], Loss: 0.0104\n",
      "Epoch [40/1000], Step [260/448], Loss: 0.0162\n",
      "Epoch [40/1000], Step [270/448], Loss: 0.0120\n",
      "Epoch [40/1000], Step [280/448], Loss: 0.0145\n",
      "Epoch [40/1000], Step [290/448], Loss: 0.0163\n",
      "Epoch [40/1000], Step [300/448], Loss: 0.0125\n",
      "Epoch [40/1000], Step [310/448], Loss: 0.0129\n",
      "Epoch [40/1000], Step [320/448], Loss: 0.0074\n",
      "Epoch [40/1000], Step [330/448], Loss: 0.0164\n",
      "Epoch [40/1000], Step [340/448], Loss: 0.0134\n",
      "Epoch [40/1000], Step [350/448], Loss: 0.0125\n",
      "Epoch [40/1000], Step [360/448], Loss: 0.0158\n",
      "Epoch [40/1000], Step [370/448], Loss: 0.0128\n",
      "Epoch [40/1000], Step [380/448], Loss: 0.0137\n",
      "Epoch [40/1000], Step [390/448], Loss: 0.0203\n",
      "Epoch [40/1000], Step [400/448], Loss: 0.0234\n",
      "Epoch [40/1000], Step [410/448], Loss: 0.0124\n",
      "Epoch [40/1000], Step [420/448], Loss: 0.0165\n",
      "Epoch [40/1000], Step [430/448], Loss: 0.0140\n",
      "Epoch [40/1000], Step [440/448], Loss: 0.0144\n",
      "Epoch [40/1000], Average Loss: 0.0159\n",
      "Epoch [40/1000], Validation Loss: 0.0199\n",
      "Epoch [41/1000], Step [10/448], Loss: 0.0167\n",
      "Epoch [41/1000], Step [20/448], Loss: 0.0111\n",
      "Epoch [41/1000], Step [30/448], Loss: 0.0166\n",
      "Epoch [41/1000], Step [40/448], Loss: 0.0161\n",
      "Epoch [41/1000], Step [50/448], Loss: 0.0137\n",
      "Epoch [41/1000], Step [60/448], Loss: 0.0187\n",
      "Epoch [41/1000], Step [70/448], Loss: 0.0266\n",
      "Epoch [41/1000], Step [80/448], Loss: 0.0109\n",
      "Epoch [41/1000], Step [90/448], Loss: 0.0108\n",
      "Epoch [41/1000], Step [100/448], Loss: 0.0168\n",
      "Epoch [41/1000], Step [110/448], Loss: 0.0189\n",
      "Epoch [41/1000], Step [120/448], Loss: 0.0132\n",
      "Epoch [41/1000], Step [130/448], Loss: 0.0232\n",
      "Epoch [41/1000], Step [140/448], Loss: 0.0213\n",
      "Epoch [41/1000], Step [150/448], Loss: 0.0155\n",
      "Epoch [41/1000], Step [160/448], Loss: 0.0110\n",
      "Epoch [41/1000], Step [170/448], Loss: 0.0149\n",
      "Epoch [41/1000], Step [180/448], Loss: 0.0125\n",
      "Epoch [41/1000], Step [190/448], Loss: 0.0094\n",
      "Epoch [41/1000], Step [200/448], Loss: 0.0164\n",
      "Epoch [41/1000], Step [210/448], Loss: 0.0183\n",
      "Epoch [41/1000], Step [220/448], Loss: 0.0164\n",
      "Epoch [41/1000], Step [230/448], Loss: 0.0177\n",
      "Epoch [41/1000], Step [240/448], Loss: 0.0207\n",
      "Epoch [41/1000], Step [250/448], Loss: 0.0193\n",
      "Epoch [41/1000], Step [260/448], Loss: 0.0164\n",
      "Epoch [41/1000], Step [270/448], Loss: 0.0069\n",
      "Epoch [41/1000], Step [280/448], Loss: 0.0158\n",
      "Epoch [41/1000], Step [290/448], Loss: 0.0125\n",
      "Epoch [41/1000], Step [300/448], Loss: 0.0191\n",
      "Epoch [41/1000], Step [310/448], Loss: 0.0178\n",
      "Epoch [41/1000], Step [320/448], Loss: 0.0125\n",
      "Epoch [41/1000], Step [330/448], Loss: 0.0172\n",
      "Epoch [41/1000], Step [340/448], Loss: 0.0139\n",
      "Epoch [41/1000], Step [350/448], Loss: 0.0081\n",
      "Epoch [41/1000], Step [360/448], Loss: 0.0095\n",
      "Epoch [41/1000], Step [370/448], Loss: 0.0190\n",
      "Epoch [41/1000], Step [380/448], Loss: 0.0260\n",
      "Epoch [41/1000], Step [390/448], Loss: 0.0207\n",
      "Epoch [41/1000], Step [400/448], Loss: 0.0163\n",
      "Epoch [41/1000], Step [410/448], Loss: 0.0176\n",
      "Epoch [41/1000], Step [420/448], Loss: 0.0243\n",
      "Epoch [41/1000], Step [430/448], Loss: 0.0150\n",
      "Epoch [41/1000], Step [440/448], Loss: 0.0166\n",
      "Epoch [41/1000], Average Loss: 0.0158\n",
      "Epoch [41/1000], Validation Loss: 0.0206\n",
      "Epoch [42/1000], Step [10/448], Loss: 0.0234\n",
      "Epoch [42/1000], Step [20/448], Loss: 0.0163\n",
      "Epoch [42/1000], Step [30/448], Loss: 0.0173\n",
      "Epoch [42/1000], Step [40/448], Loss: 0.0197\n",
      "Epoch [42/1000], Step [50/448], Loss: 0.0232\n",
      "Epoch [42/1000], Step [60/448], Loss: 0.0097\n",
      "Epoch [42/1000], Step [70/448], Loss: 0.0131\n",
      "Epoch [42/1000], Step [80/448], Loss: 0.0124\n",
      "Epoch [42/1000], Step [90/448], Loss: 0.0133\n",
      "Epoch [42/1000], Step [100/448], Loss: 0.0140\n",
      "Epoch [42/1000], Step [110/448], Loss: 0.0171\n",
      "Epoch [42/1000], Step [120/448], Loss: 0.0117\n",
      "Epoch [42/1000], Step [130/448], Loss: 0.0111\n",
      "Epoch [42/1000], Step [140/448], Loss: 0.0225\n",
      "Epoch [42/1000], Step [150/448], Loss: 0.0126\n",
      "Epoch [42/1000], Step [160/448], Loss: 0.0131\n",
      "Epoch [42/1000], Step [170/448], Loss: 0.0135\n",
      "Epoch [42/1000], Step [180/448], Loss: 0.0146\n",
      "Epoch [42/1000], Step [190/448], Loss: 0.0149\n",
      "Epoch [42/1000], Step [200/448], Loss: 0.0091\n",
      "Epoch [42/1000], Step [210/448], Loss: 0.0154\n",
      "Epoch [42/1000], Step [220/448], Loss: 0.0114\n",
      "Epoch [42/1000], Step [230/448], Loss: 0.0151\n",
      "Epoch [42/1000], Step [240/448], Loss: 0.0198\n",
      "Epoch [42/1000], Step [250/448], Loss: 0.0152\n",
      "Epoch [42/1000], Step [260/448], Loss: 0.0113\n",
      "Epoch [42/1000], Step [270/448], Loss: 0.0205\n",
      "Epoch [42/1000], Step [280/448], Loss: 0.0179\n",
      "Epoch [42/1000], Step [290/448], Loss: 0.0172\n",
      "Epoch [42/1000], Step [300/448], Loss: 0.0208\n",
      "Epoch [42/1000], Step [310/448], Loss: 0.0089\n",
      "Epoch [42/1000], Step [320/448], Loss: 0.0131\n",
      "Epoch [42/1000], Step [330/448], Loss: 0.0155\n",
      "Epoch [42/1000], Step [340/448], Loss: 0.0111\n",
      "Epoch [42/1000], Step [350/448], Loss: 0.0221\n",
      "Epoch [42/1000], Step [360/448], Loss: 0.0091\n",
      "Epoch [42/1000], Step [370/448], Loss: 0.0167\n",
      "Epoch [42/1000], Step [380/448], Loss: 0.0165\n",
      "Epoch [42/1000], Step [390/448], Loss: 0.0146\n",
      "Epoch [42/1000], Step [400/448], Loss: 0.0126\n",
      "Epoch [42/1000], Step [410/448], Loss: 0.0116\n",
      "Epoch [42/1000], Step [420/448], Loss: 0.0125\n",
      "Epoch [42/1000], Step [430/448], Loss: 0.0226\n",
      "Epoch [42/1000], Step [440/448], Loss: 0.0090\n",
      "Epoch [42/1000], Average Loss: 0.0156\n",
      "Epoch [42/1000], Validation Loss: 0.0206\n",
      "Epoch [43/1000], Step [10/448], Loss: 0.0234\n",
      "Epoch [43/1000], Step [20/448], Loss: 0.0174\n",
      "Epoch [43/1000], Step [30/448], Loss: 0.0165\n",
      "Epoch [43/1000], Step [40/448], Loss: 0.0163\n",
      "Epoch [43/1000], Step [50/448], Loss: 0.0143\n",
      "Epoch [43/1000], Step [60/448], Loss: 0.0123\n",
      "Epoch [43/1000], Step [70/448], Loss: 0.0206\n",
      "Epoch [43/1000], Step [80/448], Loss: 0.0178\n",
      "Epoch [43/1000], Step [90/448], Loss: 0.0180\n",
      "Epoch [43/1000], Step [100/448], Loss: 0.0131\n",
      "Epoch [43/1000], Step [110/448], Loss: 0.0114\n",
      "Epoch [43/1000], Step [120/448], Loss: 0.0213\n",
      "Epoch [43/1000], Step [130/448], Loss: 0.0048\n",
      "Epoch [43/1000], Step [140/448], Loss: 0.0151\n",
      "Epoch [43/1000], Step [150/448], Loss: 0.0180\n",
      "Epoch [43/1000], Step [160/448], Loss: 0.0163\n",
      "Epoch [43/1000], Step [170/448], Loss: 0.0152\n",
      "Epoch [43/1000], Step [180/448], Loss: 0.0112\n",
      "Epoch [43/1000], Step [190/448], Loss: 0.0163\n",
      "Epoch [43/1000], Step [200/448], Loss: 0.0141\n",
      "Epoch [43/1000], Step [210/448], Loss: 0.0149\n",
      "Epoch [43/1000], Step [220/448], Loss: 0.0190\n",
      "Epoch [43/1000], Step [230/448], Loss: 0.0144\n",
      "Epoch [43/1000], Step [240/448], Loss: 0.0294\n",
      "Epoch [43/1000], Step [250/448], Loss: 0.0191\n",
      "Epoch [43/1000], Step [260/448], Loss: 0.0228\n",
      "Epoch [43/1000], Step [270/448], Loss: 0.0126\n",
      "Epoch [43/1000], Step [280/448], Loss: 0.0132\n",
      "Epoch [43/1000], Step [290/448], Loss: 0.0186\n",
      "Epoch [43/1000], Step [300/448], Loss: 0.0146\n",
      "Epoch [43/1000], Step [310/448], Loss: 0.0200\n",
      "Epoch [43/1000], Step [320/448], Loss: 0.0142\n",
      "Epoch [43/1000], Step [330/448], Loss: 0.0097\n",
      "Epoch [43/1000], Step [340/448], Loss: 0.0170\n",
      "Epoch [43/1000], Step [350/448], Loss: 0.0144\n",
      "Epoch [43/1000], Step [360/448], Loss: 0.0193\n",
      "Epoch [43/1000], Step [370/448], Loss: 0.0128\n",
      "Epoch [43/1000], Step [380/448], Loss: 0.0120\n",
      "Epoch [43/1000], Step [390/448], Loss: 0.0249\n",
      "Epoch [43/1000], Step [400/448], Loss: 0.0171\n",
      "Epoch [43/1000], Step [410/448], Loss: 0.0229\n",
      "Epoch [43/1000], Step [420/448], Loss: 0.0154\n",
      "Epoch [43/1000], Step [430/448], Loss: 0.0166\n",
      "Epoch [43/1000], Step [440/448], Loss: 0.0102\n",
      "Epoch [43/1000], Average Loss: 0.0153\n",
      "Epoch [43/1000], Validation Loss: 0.0199\n",
      "Epoch [44/1000], Step [10/448], Loss: 0.0175\n",
      "Epoch [44/1000], Step [20/448], Loss: 0.0144\n",
      "Epoch [44/1000], Step [30/448], Loss: 0.0173\n",
      "Epoch [44/1000], Step [40/448], Loss: 0.0152\n",
      "Epoch [44/1000], Step [50/448], Loss: 0.0074\n",
      "Epoch [44/1000], Step [60/448], Loss: 0.0116\n",
      "Epoch [44/1000], Step [70/448], Loss: 0.0201\n",
      "Epoch [44/1000], Step [80/448], Loss: 0.0157\n",
      "Epoch [44/1000], Step [90/448], Loss: 0.0209\n",
      "Epoch [44/1000], Step [100/448], Loss: 0.0124\n",
      "Epoch [44/1000], Step [110/448], Loss: 0.0144\n",
      "Epoch [44/1000], Step [120/448], Loss: 0.0105\n",
      "Epoch [44/1000], Step [130/448], Loss: 0.0162\n",
      "Epoch [44/1000], Step [140/448], Loss: 0.0143\n",
      "Epoch [44/1000], Step [150/448], Loss: 0.0155\n",
      "Epoch [44/1000], Step [160/448], Loss: 0.0181\n",
      "Epoch [44/1000], Step [170/448], Loss: 0.0083\n",
      "Epoch [44/1000], Step [180/448], Loss: 0.0183\n",
      "Epoch [44/1000], Step [190/448], Loss: 0.0151\n",
      "Epoch [44/1000], Step [200/448], Loss: 0.0167\n",
      "Epoch [44/1000], Step [210/448], Loss: 0.0072\n",
      "Epoch [44/1000], Step [220/448], Loss: 0.0070\n",
      "Epoch [44/1000], Step [230/448], Loss: 0.0100\n",
      "Epoch [44/1000], Step [240/448], Loss: 0.0201\n",
      "Epoch [44/1000], Step [250/448], Loss: 0.0139\n",
      "Epoch [44/1000], Step [260/448], Loss: 0.0216\n",
      "Epoch [44/1000], Step [270/448], Loss: 0.0202\n",
      "Epoch [44/1000], Step [280/448], Loss: 0.0150\n",
      "Epoch [44/1000], Step [290/448], Loss: 0.0049\n",
      "Epoch [44/1000], Step [300/448], Loss: 0.0169\n",
      "Epoch [44/1000], Step [310/448], Loss: 0.0165\n",
      "Epoch [44/1000], Step [320/448], Loss: 0.0109\n",
      "Epoch [44/1000], Step [330/448], Loss: 0.0225\n",
      "Epoch [44/1000], Step [340/448], Loss: 0.0056\n",
      "Epoch [44/1000], Step [350/448], Loss: 0.0123\n",
      "Epoch [44/1000], Step [360/448], Loss: 0.0211\n",
      "Epoch [44/1000], Step [370/448], Loss: 0.0135\n",
      "Epoch [44/1000], Step [380/448], Loss: 0.0145\n",
      "Epoch [44/1000], Step [390/448], Loss: 0.0101\n",
      "Epoch [44/1000], Step [400/448], Loss: 0.0146\n",
      "Epoch [44/1000], Step [410/448], Loss: 0.0206\n",
      "Epoch [44/1000], Step [420/448], Loss: 0.0175\n",
      "Epoch [44/1000], Step [430/448], Loss: 0.0174\n",
      "Epoch [44/1000], Step [440/448], Loss: 0.0171\n",
      "Epoch [44/1000], Average Loss: 0.0155\n",
      "Epoch [44/1000], Validation Loss: 0.0203\n",
      "Epoch [45/1000], Step [10/448], Loss: 0.0120\n",
      "Epoch [45/1000], Step [20/448], Loss: 0.0111\n",
      "Epoch [45/1000], Step [30/448], Loss: 0.0181\n",
      "Epoch [45/1000], Step [40/448], Loss: 0.0072\n",
      "Epoch [45/1000], Step [50/448], Loss: 0.0082\n",
      "Epoch [45/1000], Step [60/448], Loss: 0.0093\n",
      "Epoch [45/1000], Step [70/448], Loss: 0.0078\n",
      "Epoch [45/1000], Step [80/448], Loss: 0.0187\n",
      "Epoch [45/1000], Step [90/448], Loss: 0.0153\n",
      "Epoch [45/1000], Step [100/448], Loss: 0.0175\n",
      "Epoch [45/1000], Step [110/448], Loss: 0.0167\n",
      "Epoch [45/1000], Step [120/448], Loss: 0.0087\n",
      "Epoch [45/1000], Step [130/448], Loss: 0.0116\n",
      "Epoch [45/1000], Step [140/448], Loss: 0.0131\n",
      "Epoch [45/1000], Step [150/448], Loss: 0.0178\n",
      "Epoch [45/1000], Step [160/448], Loss: 0.0204\n",
      "Epoch [45/1000], Step [170/448], Loss: 0.0232\n",
      "Epoch [45/1000], Step [180/448], Loss: 0.0171\n",
      "Epoch [45/1000], Step [190/448], Loss: 0.0173\n",
      "Epoch [45/1000], Step [200/448], Loss: 0.0183\n",
      "Epoch [45/1000], Step [210/448], Loss: 0.0193\n",
      "Epoch [45/1000], Step [220/448], Loss: 0.0242\n",
      "Epoch [45/1000], Step [230/448], Loss: 0.0153\n",
      "Epoch [45/1000], Step [240/448], Loss: 0.0153\n",
      "Epoch [45/1000], Step [250/448], Loss: 0.0154\n",
      "Epoch [45/1000], Step [260/448], Loss: 0.0084\n",
      "Epoch [45/1000], Step [270/448], Loss: 0.0036\n",
      "Epoch [45/1000], Step [280/448], Loss: 0.0156\n",
      "Epoch [45/1000], Step [290/448], Loss: 0.0154\n",
      "Epoch [45/1000], Step [300/448], Loss: 0.0123\n",
      "Epoch [45/1000], Step [310/448], Loss: 0.0130\n",
      "Epoch [45/1000], Step [320/448], Loss: 0.0192\n",
      "Epoch [45/1000], Step [330/448], Loss: 0.0116\n",
      "Epoch [45/1000], Step [340/448], Loss: 0.0116\n",
      "Epoch [45/1000], Step [350/448], Loss: 0.0169\n",
      "Epoch [45/1000], Step [360/448], Loss: 0.0141\n",
      "Epoch [45/1000], Step [370/448], Loss: 0.0186\n",
      "Epoch [45/1000], Step [380/448], Loss: 0.0185\n",
      "Epoch [45/1000], Step [390/448], Loss: 0.0229\n",
      "Epoch [45/1000], Step [400/448], Loss: 0.0251\n",
      "Epoch [45/1000], Step [410/448], Loss: 0.0116\n",
      "Epoch [45/1000], Step [420/448], Loss: 0.0094\n",
      "Epoch [45/1000], Step [430/448], Loss: 0.0125\n",
      "Epoch [45/1000], Step [440/448], Loss: 0.0083\n",
      "Epoch [45/1000], Average Loss: 0.0147\n",
      "Epoch [45/1000], Validation Loss: 0.0214\n",
      "Epoch [46/1000], Step [10/448], Loss: 0.0094\n",
      "Epoch [46/1000], Step [20/448], Loss: 0.0108\n",
      "Epoch [46/1000], Step [30/448], Loss: 0.0081\n",
      "Epoch [46/1000], Step [40/448], Loss: 0.0203\n",
      "Epoch [46/1000], Step [50/448], Loss: 0.0194\n",
      "Epoch [46/1000], Step [60/448], Loss: 0.0167\n",
      "Epoch [46/1000], Step [70/448], Loss: 0.0134\n",
      "Epoch [46/1000], Step [80/448], Loss: 0.0230\n",
      "Epoch [46/1000], Step [90/448], Loss: 0.0192\n",
      "Epoch [46/1000], Step [100/448], Loss: 0.0167\n",
      "Epoch [46/1000], Step [110/448], Loss: 0.0123\n",
      "Epoch [46/1000], Step [120/448], Loss: 0.0118\n",
      "Epoch [46/1000], Step [130/448], Loss: 0.0254\n",
      "Epoch [46/1000], Step [140/448], Loss: 0.0043\n",
      "Epoch [46/1000], Step [150/448], Loss: 0.0166\n",
      "Epoch [46/1000], Step [160/448], Loss: 0.0232\n",
      "Epoch [46/1000], Step [170/448], Loss: 0.0207\n",
      "Epoch [46/1000], Step [180/448], Loss: 0.0110\n",
      "Epoch [46/1000], Step [190/448], Loss: 0.0141\n",
      "Epoch [46/1000], Step [200/448], Loss: 0.0139\n",
      "Epoch [46/1000], Step [210/448], Loss: 0.0203\n",
      "Epoch [46/1000], Step [220/448], Loss: 0.0131\n",
      "Epoch [46/1000], Step [230/448], Loss: 0.0102\n",
      "Epoch [46/1000], Step [240/448], Loss: 0.0106\n",
      "Epoch [46/1000], Step [250/448], Loss: 0.0144\n",
      "Epoch [46/1000], Step [260/448], Loss: 0.0222\n",
      "Epoch [46/1000], Step [270/448], Loss: 0.0155\n",
      "Epoch [46/1000], Step [280/448], Loss: 0.0055\n",
      "Epoch [46/1000], Step [290/448], Loss: 0.0187\n",
      "Epoch [46/1000], Step [300/448], Loss: 0.0199\n",
      "Epoch [46/1000], Step [310/448], Loss: 0.0144\n",
      "Epoch [46/1000], Step [320/448], Loss: 0.0095\n",
      "Epoch [46/1000], Step [330/448], Loss: 0.0089\n",
      "Epoch [46/1000], Step [340/448], Loss: 0.0115\n",
      "Epoch [46/1000], Step [350/448], Loss: 0.0129\n",
      "Epoch [46/1000], Step [360/448], Loss: 0.0161\n",
      "Epoch [46/1000], Step [370/448], Loss: 0.0148\n",
      "Epoch [46/1000], Step [380/448], Loss: 0.0153\n",
      "Epoch [46/1000], Step [390/448], Loss: 0.0147\n",
      "Epoch [46/1000], Step [400/448], Loss: 0.0185\n",
      "Epoch [46/1000], Step [410/448], Loss: 0.0100\n",
      "Epoch [46/1000], Step [420/448], Loss: 0.0161\n",
      "Epoch [46/1000], Step [430/448], Loss: 0.0233\n",
      "Epoch [46/1000], Step [440/448], Loss: 0.0125\n",
      "Epoch [46/1000], Average Loss: 0.0152\n",
      "Epoch [46/1000], Validation Loss: 0.0202\n",
      "Epoch [47/1000], Step [10/448], Loss: 0.0184\n",
      "Epoch [47/1000], Step [20/448], Loss: 0.0099\n",
      "Epoch [47/1000], Step [30/448], Loss: 0.0053\n",
      "Epoch [47/1000], Step [40/448], Loss: 0.0153\n",
      "Epoch [47/1000], Step [50/448], Loss: 0.0180\n",
      "Epoch [47/1000], Step [60/448], Loss: 0.0118\n",
      "Epoch [47/1000], Step [70/448], Loss: 0.0149\n",
      "Epoch [47/1000], Step [80/448], Loss: 0.0092\n",
      "Epoch [47/1000], Step [90/448], Loss: 0.0118\n",
      "Epoch [47/1000], Step [100/448], Loss: 0.0168\n",
      "Epoch [47/1000], Step [110/448], Loss: 0.0109\n",
      "Epoch [47/1000], Step [120/448], Loss: 0.0113\n",
      "Epoch [47/1000], Step [130/448], Loss: 0.0198\n",
      "Epoch [47/1000], Step [140/448], Loss: 0.0147\n",
      "Epoch [47/1000], Step [150/448], Loss: 0.0174\n",
      "Epoch [47/1000], Step [160/448], Loss: 0.0104\n",
      "Epoch [47/1000], Step [170/448], Loss: 0.0075\n",
      "Epoch [47/1000], Step [180/448], Loss: 0.0262\n",
      "Epoch [47/1000], Step [190/448], Loss: 0.0161\n",
      "Epoch [47/1000], Step [200/448], Loss: 0.0113\n",
      "Epoch [47/1000], Step [210/448], Loss: 0.0071\n",
      "Epoch [47/1000], Step [220/448], Loss: 0.0175\n",
      "Epoch [47/1000], Step [230/448], Loss: 0.0175\n",
      "Epoch [47/1000], Step [240/448], Loss: 0.0187\n",
      "Epoch [47/1000], Step [250/448], Loss: 0.0180\n",
      "Epoch [47/1000], Step [260/448], Loss: 0.0151\n",
      "Epoch [47/1000], Step [270/448], Loss: 0.0101\n",
      "Epoch [47/1000], Step [280/448], Loss: 0.0228\n",
      "Epoch [47/1000], Step [290/448], Loss: 0.0236\n",
      "Epoch [47/1000], Step [300/448], Loss: 0.0135\n",
      "Epoch [47/1000], Step [310/448], Loss: 0.0159\n",
      "Epoch [47/1000], Step [320/448], Loss: 0.0159\n",
      "Epoch [47/1000], Step [330/448], Loss: 0.0183\n",
      "Epoch [47/1000], Step [340/448], Loss: 0.0113\n",
      "Epoch [47/1000], Step [350/448], Loss: 0.0152\n",
      "Epoch [47/1000], Step [360/448], Loss: 0.0128\n",
      "Epoch [47/1000], Step [370/448], Loss: 0.0157\n",
      "Epoch [47/1000], Step [380/448], Loss: 0.0134\n",
      "Epoch [47/1000], Step [390/448], Loss: 0.0204\n",
      "Epoch [47/1000], Step [400/448], Loss: 0.0092\n",
      "Epoch [47/1000], Step [410/448], Loss: 0.0193\n",
      "Epoch [47/1000], Step [420/448], Loss: 0.0127\n",
      "Epoch [47/1000], Step [430/448], Loss: 0.0199\n",
      "Epoch [47/1000], Step [440/448], Loss: 0.0161\n",
      "Epoch [47/1000], Average Loss: 0.0148\n",
      "Epoch [47/1000], Validation Loss: 0.0200\n",
      "Epoch [48/1000], Step [10/448], Loss: 0.0275\n",
      "Epoch [48/1000], Step [20/448], Loss: 0.0123\n",
      "Epoch [48/1000], Step [30/448], Loss: 0.0103\n",
      "Epoch [48/1000], Step [40/448], Loss: 0.0146\n",
      "Epoch [48/1000], Step [50/448], Loss: 0.0126\n",
      "Epoch [48/1000], Step [60/448], Loss: 0.0094\n",
      "Epoch [48/1000], Step [70/448], Loss: 0.0205\n",
      "Epoch [48/1000], Step [80/448], Loss: 0.0150\n",
      "Epoch [48/1000], Step [90/448], Loss: 0.0132\n",
      "Epoch [48/1000], Step [100/448], Loss: 0.0207\n",
      "Epoch [48/1000], Step [110/448], Loss: 0.0138\n",
      "Epoch [48/1000], Step [120/448], Loss: 0.0167\n",
      "Epoch [48/1000], Step [130/448], Loss: 0.0079\n",
      "Epoch [48/1000], Step [140/448], Loss: 0.0137\n",
      "Epoch [48/1000], Step [150/448], Loss: 0.0220\n",
      "Epoch [48/1000], Step [160/448], Loss: 0.0158\n",
      "Epoch [48/1000], Step [170/448], Loss: 0.0105\n",
      "Epoch [48/1000], Step [180/448], Loss: 0.0115\n",
      "Epoch [48/1000], Step [190/448], Loss: 0.0128\n",
      "Epoch [48/1000], Step [200/448], Loss: 0.0154\n",
      "Epoch [48/1000], Step [210/448], Loss: 0.0249\n",
      "Epoch [48/1000], Step [220/448], Loss: 0.0177\n",
      "Epoch [48/1000], Step [230/448], Loss: 0.0198\n",
      "Epoch [48/1000], Step [240/448], Loss: 0.0134\n",
      "Epoch [48/1000], Step [250/448], Loss: 0.0086\n",
      "Epoch [48/1000], Step [260/448], Loss: 0.0166\n",
      "Epoch [48/1000], Step [270/448], Loss: 0.0229\n",
      "Epoch [48/1000], Step [280/448], Loss: 0.0159\n",
      "Epoch [48/1000], Step [290/448], Loss: 0.0162\n",
      "Epoch [48/1000], Step [300/448], Loss: 0.0190\n",
      "Epoch [48/1000], Step [310/448], Loss: 0.0173\n",
      "Epoch [48/1000], Step [320/448], Loss: 0.0114\n",
      "Epoch [48/1000], Step [330/448], Loss: 0.0117\n",
      "Epoch [48/1000], Step [340/448], Loss: 0.0257\n",
      "Epoch [48/1000], Step [350/448], Loss: 0.0237\n",
      "Epoch [48/1000], Step [360/448], Loss: 0.0219\n",
      "Epoch [48/1000], Step [370/448], Loss: 0.0195\n",
      "Epoch [48/1000], Step [380/448], Loss: 0.0282\n",
      "Epoch [48/1000], Step [390/448], Loss: 0.0195\n",
      "Epoch [48/1000], Step [400/448], Loss: 0.0140\n",
      "Epoch [48/1000], Step [410/448], Loss: 0.0248\n",
      "Epoch [48/1000], Step [420/448], Loss: 0.0069\n",
      "Epoch [48/1000], Step [430/448], Loss: 0.0237\n",
      "Epoch [48/1000], Step [440/448], Loss: 0.0166\n",
      "Epoch [48/1000], Average Loss: 0.0148\n",
      "Epoch [48/1000], Validation Loss: 0.0199\n",
      "Epoch [49/1000], Step [10/448], Loss: 0.0154\n",
      "Epoch [49/1000], Step [20/448], Loss: 0.0196\n",
      "Epoch [49/1000], Step [30/448], Loss: 0.0154\n",
      "Epoch [49/1000], Step [40/448], Loss: 0.0115\n",
      "Epoch [49/1000], Step [50/448], Loss: 0.0130\n",
      "Epoch [49/1000], Step [60/448], Loss: 0.0194\n",
      "Epoch [49/1000], Step [70/448], Loss: 0.0109\n",
      "Epoch [49/1000], Step [80/448], Loss: 0.0145\n",
      "Epoch [49/1000], Step [90/448], Loss: 0.0090\n",
      "Epoch [49/1000], Step [100/448], Loss: 0.0144\n",
      "Epoch [49/1000], Step [110/448], Loss: 0.0100\n",
      "Epoch [49/1000], Step [120/448], Loss: 0.0178\n",
      "Epoch [49/1000], Step [130/448], Loss: 0.0113\n",
      "Epoch [49/1000], Step [140/448], Loss: 0.0130\n",
      "Epoch [49/1000], Step [150/448], Loss: 0.0154\n",
      "Epoch [49/1000], Step [160/448], Loss: 0.0113\n",
      "Epoch [49/1000], Step [170/448], Loss: 0.0123\n",
      "Epoch [49/1000], Step [180/448], Loss: 0.0105\n",
      "Epoch [49/1000], Step [190/448], Loss: 0.0071\n",
      "Epoch [49/1000], Step [200/448], Loss: 0.0148\n",
      "Epoch [49/1000], Step [210/448], Loss: 0.0183\n",
      "Epoch [49/1000], Step [220/448], Loss: 0.0115\n",
      "Epoch [49/1000], Step [230/448], Loss: 0.0228\n",
      "Epoch [49/1000], Step [240/448], Loss: 0.0083\n",
      "Epoch [49/1000], Step [250/448], Loss: 0.0195\n",
      "Epoch [49/1000], Step [260/448], Loss: 0.0219\n",
      "Epoch [49/1000], Step [270/448], Loss: 0.0149\n",
      "Epoch [49/1000], Step [280/448], Loss: 0.0141\n",
      "Epoch [49/1000], Step [290/448], Loss: 0.0227\n",
      "Epoch [49/1000], Step [300/448], Loss: 0.0167\n",
      "Epoch [49/1000], Step [310/448], Loss: 0.0092\n",
      "Epoch [49/1000], Step [320/448], Loss: 0.0188\n",
      "Epoch [49/1000], Step [330/448], Loss: 0.0113\n",
      "Epoch [49/1000], Step [340/448], Loss: 0.0128\n",
      "Epoch [49/1000], Step [350/448], Loss: 0.0139\n",
      "Epoch [49/1000], Step [360/448], Loss: 0.0126\n",
      "Epoch [49/1000], Step [370/448], Loss: 0.0127\n",
      "Epoch [49/1000], Step [380/448], Loss: 0.0193\n",
      "Epoch [49/1000], Step [390/448], Loss: 0.0105\n",
      "Epoch [49/1000], Step [400/448], Loss: 0.0188\n",
      "Epoch [49/1000], Step [410/448], Loss: 0.0204\n",
      "Epoch [49/1000], Step [420/448], Loss: 0.0119\n",
      "Epoch [49/1000], Step [430/448], Loss: 0.0149\n",
      "Epoch [49/1000], Step [440/448], Loss: 0.0089\n",
      "Epoch [49/1000], Average Loss: 0.0147\n",
      "Epoch [49/1000], Validation Loss: 0.0192\n",
      "Epoch [50/1000], Step [10/448], Loss: 0.0212\n",
      "Epoch [50/1000], Step [20/448], Loss: 0.0131\n",
      "Epoch [50/1000], Step [30/448], Loss: 0.0093\n",
      "Epoch [50/1000], Step [40/448], Loss: 0.0143\n",
      "Epoch [50/1000], Step [50/448], Loss: 0.0062\n",
      "Epoch [50/1000], Step [60/448], Loss: 0.0131\n",
      "Epoch [50/1000], Step [70/448], Loss: 0.0117\n",
      "Epoch [50/1000], Step [80/448], Loss: 0.0218\n",
      "Epoch [50/1000], Step [90/448], Loss: 0.0189\n",
      "Epoch [50/1000], Step [100/448], Loss: 0.0119\n",
      "Epoch [50/1000], Step [110/448], Loss: 0.0061\n",
      "Epoch [50/1000], Step [120/448], Loss: 0.0143\n",
      "Epoch [50/1000], Step [130/448], Loss: 0.0126\n",
      "Epoch [50/1000], Step [140/448], Loss: 0.0223\n",
      "Epoch [50/1000], Step [150/448], Loss: 0.0137\n",
      "Epoch [50/1000], Step [160/448], Loss: 0.0113\n",
      "Epoch [50/1000], Step [170/448], Loss: 0.0133\n",
      "Epoch [50/1000], Step [180/448], Loss: 0.0239\n",
      "Epoch [50/1000], Step [190/448], Loss: 0.0088\n",
      "Epoch [50/1000], Step [200/448], Loss: 0.0194\n",
      "Epoch [50/1000], Step [210/448], Loss: 0.0264\n",
      "Epoch [50/1000], Step [220/448], Loss: 0.0113\n",
      "Epoch [50/1000], Step [230/448], Loss: 0.0155\n",
      "Epoch [50/1000], Step [240/448], Loss: 0.0118\n",
      "Epoch [50/1000], Step [250/448], Loss: 0.0076\n",
      "Epoch [50/1000], Step [260/448], Loss: 0.0077\n",
      "Epoch [50/1000], Step [270/448], Loss: 0.0332\n",
      "Epoch [50/1000], Step [280/448], Loss: 0.0102\n",
      "Epoch [50/1000], Step [290/448], Loss: 0.0087\n",
      "Epoch [50/1000], Step [300/448], Loss: 0.0156\n",
      "Epoch [50/1000], Step [310/448], Loss: 0.0130\n",
      "Epoch [50/1000], Step [320/448], Loss: 0.0158\n",
      "Epoch [50/1000], Step [330/448], Loss: 0.0098\n",
      "Epoch [50/1000], Step [340/448], Loss: 0.0141\n",
      "Epoch [50/1000], Step [350/448], Loss: 0.0148\n",
      "Epoch [50/1000], Step [360/448], Loss: 0.0092\n",
      "Epoch [50/1000], Step [370/448], Loss: 0.0106\n",
      "Epoch [50/1000], Step [380/448], Loss: 0.0142\n",
      "Epoch [50/1000], Step [390/448], Loss: 0.0143\n",
      "Epoch [50/1000], Step [400/448], Loss: 0.0113\n",
      "Epoch [50/1000], Step [410/448], Loss: 0.0073\n",
      "Epoch [50/1000], Step [420/448], Loss: 0.0115\n",
      "Epoch [50/1000], Step [430/448], Loss: 0.0103\n",
      "Epoch [50/1000], Step [440/448], Loss: 0.0094\n",
      "Epoch [50/1000], Average Loss: 0.0145\n",
      "Epoch [50/1000], Validation Loss: 0.0208\n",
      "Epoch [51/1000], Step [10/448], Loss: 0.0179\n",
      "Epoch [51/1000], Step [20/448], Loss: 0.0159\n",
      "Epoch [51/1000], Step [30/448], Loss: 0.0162\n",
      "Epoch [51/1000], Step [40/448], Loss: 0.0126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m loss = criterion(output, trg)\n\u001b[32m     36\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     40\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/newenv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import configs.common as cc\n",
    "import torch.nn.functional as F\n",
    "import processing\n",
    "import configs.paths as paths\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "dataset_path = '/home/s203861/midi-classical-music/np_data'\n",
    "loader = processing.DatasetLoader(dataset_path)\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cc.config.values.learning_rate)\n",
    "\n",
    "# Logging setup\n",
    "log_data = []\n",
    "log_file_path = f'training_log_classifier.json'\n",
    "\n",
    "# Training loop\n",
    "num_epochs = cc.config.values.epochs\n",
    "print('Training started!')\n",
    "log_data.append({'timestamp': str(datetime.now()), 'message': 'Training started!'})\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg, meta) in enumerate(train_dataloader):\n",
    "        output = model(src).to('cuda')\n",
    "        trg = get_all_targets(meta).to('cuda')\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % cc.config.values.eval_interval == 0:\n",
    "            msg = f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}'\n",
    "            print(msg)\n",
    "            log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    msg = f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}'\n",
    "    print(msg)\n",
    "    log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg, meta in test_dataloader:\n",
    "            output = model(src).to('cuda')\n",
    "            trg = get_all_targets(meta).to('cuda')\n",
    "            val_loss += criterion(output, trg).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    msg = f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}'\n",
    "    print(msg)\n",
    "    log_data.append({'timestamp': str(datetime.now()), 'message': msg})\n",
    "\n",
    "    if (epoch + 1) % cc.config.values.save_interval == 0:\n",
    "        save_model(model, avg_val_loss)\n",
    "        with open(log_file_path, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "log_data.append({'timestamp': str(datetime.now()), 'message': 'Training complete!'})\n",
    "\n",
    "save_model(model, avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s203861/newenv/lib/python3.11/site-packages/pretty_midi/instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/opt/cuda/cuda-12.6/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/s203861/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/s203861/.cache/torch_extensions/py311_cu126/slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "/home/s203861/newenv/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s203861/xlstm/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n",
      "/home/s203861/xlstm/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/opt/cuda/cuda-12.6/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/s203861/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/opt/cuda/cuda-12.6/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/s203861/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/opt/cuda/cuda-12.6/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/s203861/.cache/torch_extensions/py311_cu126 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import configs.common as cc\n",
    "import torch.nn.functional as F\n",
    "import processing\n",
    "import configs.paths as paths\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import models.classifier\n",
    "\n",
    "model = models.classifier.Classifier()\n",
    "model.load_state_dict(torch.load(f'/home/s203861/Deep-Learning-Based-Sequence-Models-for-Music-Generation/pretrained/classifier/loss_0.02_time_2025-07-03-23-15-14.pth'))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 63, 123,  12, 140,  63,  61, 140, 140], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/home/s203861/midi-classical-music/np_data'\n",
    "loader = processing.DatasetLoader(dataset_path)\n",
    "train_dataloader, test_dataloader = loader.get_dataloaders()\n",
    "for batch_idx, (src, trg, meta) in enumerate(train_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 67, 127,  16, 144,  67,  65, 144, 144], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(src)\n",
    "output[:,4:].argmax(-1) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 67, 100,  16,  60,  67,  65, 144,  31], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s203861/newenv/lib/python3.11/site-packages/pretty_midi/instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import processing\n",
    "import os\n",
    "import configs.paths as paths\n",
    "# thing = processing.SequenceDataset(\"/home/s203861/midi-classical-music/np_data/data/Alkan\")\n",
    "dataset_path = paths.config.paths.np_dataset\n",
    "loader = processing.DatasetLoader(f'{dataset_path}/Alkan')\n",
    "# train_dataloader, test_dataloader = loader.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = []\n",
    "for root, _, files in os.walk('/home/s203861/midi-classical-music/np_data/data/Alkan'):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "len(file_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
